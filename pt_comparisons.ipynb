{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pt_comparisons.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhM/Q2+WggoYhM/0WTg5Cn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emerenan/xlsx_validation/blob/main/pt_comparisons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcY8Vu07O-5d",
        "outputId": "daf04f59-2c87-40ed-dc32-c9337ee57350"
      },
      "source": [
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 241 kB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ejFsg2BPPWP"
      },
      "source": [
        "TowerDB Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4vRO__rPR31"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', \n",
        "                             status_col='', kind='tw', dates=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col):\n",
        "        #print(df.head(1))\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "        t_col = type_dates.lower()\n",
        "        if format == 1:\n",
        "            type_date = \"%d/%m/%Y\"\n",
        "        else:\n",
        "            type_date = \"%d-%m-%Y\"\n",
        "        for column in lower_str(columns):\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new).fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,encoding='latin1').fillna('')\n",
        "        cols_old = lower_str(list(df_OLD.columns))\n",
        "        df_OLD.columns = cols_old\n",
        "        df_OLD = fit_df(df_OLD, index_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        for i in dates:\n",
        "            df_NEW[i] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) \\\n",
        "                         and not isinstance(date_obj, str) else date_obj for date_obj in df_NEW[i]]\n",
        "            df_NEW[i] = df_NEW[i].astype(str)\n",
        "            \n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW[index_col] = df_NEW[index_col].astype(str)\n",
        "        df_NEW['sites'] = df_NEW[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        #df_NEW = df_NEW[cols_old]\n",
        "\n",
        "        #df_NEW = fit_df(df_NEW, index_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "bill_cols = [\"Site Code\",\\\n",
        "             \"Macro Region\",\\\n",
        "             \"Categorization by Transmission Sys\",\\\n",
        "             \"Categorization by Site Type\",\\\n",
        "             \"Core site type\",\\\n",
        "             \"Transmission Hub Site (YES/NO)\",\\\n",
        "             \"Transmission Hub Site (inc. with/without Shelters)\",\\\n",
        "             \"Transmission Site (inc. with/without Shelters)\",\\\n",
        "             \"Room configuration\",\\\n",
        "             \"Climate Control (YES/NO)\",\\\n",
        "             \"Power Supply \",\\\n",
        "             \"Strategic Site (YES/NO)\",\\\n",
        "             \"Critical Site (YES/NO)\",\\\n",
        "             \"Is the Site a WIP site\",\\\n",
        "             \"\tNOS shared site (Yes/No)\",\\\n",
        "             \"\tMEO shared site (Yes/No)\",\\\n",
        "             \"BTS Sites (Yes/No)\",\\\n",
        "             \"Sites_As_Metered_Estimated\",\\\n",
        "             \"Strategic_Site_Bucket\",\\\n",
        "             \"Critical_Site_Beyond_10\",\\\n",
        "             \"First_Active_Sharing_Deployment_Type \",\\\n",
        "             \"First_Active_Sharing_Start_Date\",\\\n",
        "             \"First_Active_Sharing_End_Date \",\\\n",
        "             \"Subsequent_Sharing_Arrangement\",\\\n",
        "             \"Legacy_Site_Agreement_Terminated(Yes/NO)\",\\\n",
        "             \"Decommissioned Sites(True/false)\"]\n",
        "\n",
        "#'Billing Trigger Date',\\\n",
        "path = '/content/Vantage Towers_PT TowerDB Jun21_20210720.xlsx'\n",
        "msa = '/content/TowerDB_Portugal_20210731.csv'\n",
        "sheet = 'Final Delivery'\n",
        "skiprows = 6\n",
        "skipcolumns = 1\n",
        "to_save = '/content/PT_TW'\n",
        "old = 'TowerDB_Portugal_20210731.csv'\n",
        "new = \"TowerDB_Portugal_Jun21_20210720.xlsx\"\n",
        "dates_tw = ['infrastructure ready (existing)/ to be ready (new)', 'infrastructure to be dismantled by', \n",
        "            'date when vodafone active equipment is removed', 'infrastructure to be shared by']\n",
        "#(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='', kind='tw', sheetname='', skipr=0, skipc=0)\n",
        "find_diffs_between_files(msa, path, 'Site Code', bill_cols, to_save, old, new,type_file='mix',status_col='Status',kind='tw', dates=dates_tw, sheetname=sheet, skipr=6, skipc=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSpeQcvCPXgr"
      },
      "source": [
        "UIS Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFsOijWFPcVU"
      },
      "source": [
        "uis_new = '/content/UserInput_Portugal_20210831.xlsx'\n",
        "uis_old = '/content/UserInput_Portugal_20210731.xlsx'\n",
        "sheet = 'SiteLevel'\n",
        "uis_index = 'Site_ID (Numeric)'\n",
        "to_uis = '/content/PT_UIS_'\n",
        "old_uis = 'UIS_20210731.xlsx'\n",
        "new_uis = 'UIS_20210831.xlsx'\n",
        "bill = []\n",
        "find_diffs_between_files(uis_old, uis_new, uis_index, bill, to_uis, old_uis,new_uis,'excel',status_col='',\\\n",
        "                         kind='',kind_col='', sheetname=sheet, skipr=2, skipc=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt0nG-7VnNGm"
      },
      "source": [
        "TA Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UShI8sSunQAX"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col]\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old,  engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new).fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "# Lendo o Ficheiro de input\n",
        "path='/content/Vantage Towers_PT TowerDB Jun21_20210720.xlsx'\n",
        "#test = '/content/teste_dates.xlsx'\n",
        "skiprows_ta = 13\n",
        "skipcol = 1\n",
        "\n",
        "path_old_ta = '/content/TA_Input_Portugal_20210731.csv'\n",
        "sheet_ta = 'Tenant Agreements'\n",
        "\n",
        "ta_save = '/content/PT_TA'\n",
        "old_ta = 'TA_Input_Portugal_20210630.csv'\n",
        "new_ta = \"TA_Input_Portugal_20210731.csv\"\n",
        "ta_cols = ['VF Site code', 'TENANT', 'Tenant classification','Authorisation Date', 'Fee Starting Date',\"Fee Expiring Date\",\"Sharing In Scope\"]\n",
        "\n",
        "\"\"\"path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0)\"\"\"\n",
        "find_diffs_between_files(path_old_ta, path, 'VF Site code', ta_cols, ta_save, old_ta, new_ta,'mix', '', 'ta', 'TENANT', \n",
        "                         sheetname=sheet_ta, skipr=13, skipc=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}