{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hu_compariosons.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOk+a2n4PcQW+LXT5E3KOt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emerenan/xlsx_validation/blob/main/hu_compariosons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6uVWnIQw2Z3"
      },
      "source": [
        "!pip install unidecode\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "from unidecode import unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O42TDEf8u5BX"
      },
      "source": [
        "Towerdb Comparisons\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzFEo9z7urbO"
      },
      "source": [
        "def find_diffs_between_files_1(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', dates_cols=[], status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col] \n",
        "            #df.columns = lower_str(list(df.columns))\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "        t_col = type_dates.lower()\n",
        "        if format == 1:\n",
        "            type_date = \"%d/%m/%Y\"\n",
        "        else:\n",
        "            type_date = \"%d-%m-%Y\"\n",
        "        for column in lower_str(columns):\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,engine='python', encoding='latin1').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD[bill_cols]\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, engine='python', encoding='latin1').fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW[bill_cols]\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD) header=0, names=cols_old,\n",
        "        df_OLD = pd.read_csv(path_OLD, engine='python').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols, 1, 'normal')\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "hu_bill_cols = [\"Active / not active\",\\\n",
        "    \"Confirmed Skylon scope\",\\\n",
        "    \"Code\",\\\n",
        "    \"Categorization by Transmission Sys\",\\\n",
        "    \"Additional information \\x96 room configuration (including container information)\",\\\n",
        "    \"Active Sharing Arrangements involving the Operator\",\\\n",
        "    \"Critical site\",\\\n",
        "    \"Strategic site\",\\\n",
        "    \"Telenor tenant\",\\\n",
        "    \"Telekom tenant\",\\\n",
        "    \"Power supply for tenants\",\\\n",
        "    \"Energy category\",\\\n",
        "    'Ready For Active Installation (\"RFAI\") date',\\\n",
        "    \"Wip_Site\",\\\n",
        "    \"Bts_Site\",\\\n",
        "    \"Strategic_Site_Bucket\",\\\n",
        "    \"Critical_Site_Beyond_10\",\\\n",
        "    \"Subsequent_Sharing_Arrangement\",\\\n",
        "    \"First_Active_Sharing_Start_Date\",\\\n",
        "    \"First_Active_Sharing_End_Date\",\\\n",
        "    \"Active Sharing Deployment Types\",\\\n",
        "    \"Sites_As_Metered_Estimated\",\\\n",
        "    \"Date of equipment removal\"]\n",
        "\n",
        "path_tw = '/content/TowerDB_Hungary_20210831.csv'\n",
        "path_msa = '/content/TowerDB_Hungary_20210731 (1).csv' \n",
        "tw_save = '/content/TW_HU'\n",
        "new_name = 'TowerDB_Hungary_20210831.csv'\n",
        "old_name = 'TowerDB_Hungary_20210731.csv'  \n",
        "tw_dates = ['ready for active installation (\"rfai\") date',\"First_Active_Sharing_Start_Date\",\\\n",
        "            \"First_Active_Sharing_End_Date\", \"Date of equipment removal\"]\n",
        "find_diffs_between_files_1(path_msa, path_tw, 'Code',hu_bill_cols, tw_save, old_name, new_name, type_file='csv',\\\n",
        "                           status_col='active / not active',kind='tw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jYL0HhCulQ2"
      },
      "source": [
        "TA Comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIFuowSC3k7y"
      },
      "source": [
        "Verificar duplicatas do TA para evitar error no cross values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJcnT9tEufkf",
        "outputId": "e742962d-b319-4bad-ad28-36e813c637af"
      },
      "source": [
        "import natsort\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name, new_name, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col):\n",
        "        col = lower_str(list(df.columns))\n",
        "        df.columns = col\n",
        "        #df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        #df[index_col] = df[index_col].astype(str)\n",
        "        #df = df.iloc[natsort.index_humansorted(df[index_col])]\n",
        "        for i in ['tenant (name/id)', 'mno classification']:\n",
        "            df[i] = [str(j).lower() for j in  df[i]]\n",
        "\n",
        "        df = df.sort_values(by=[index_col, 'tenant (name/id)'])\n",
        "        df['sites'] = df[index_col].astype(str)+df['tenant (name/id)'].astype(str) + df['mno classification'].astype(str)\n",
        "        df = df.sort_values(by=['sites'])\n",
        "        \"\"\"lista=[]\n",
        "        dico = {}\n",
        "        for id in df[index_col]:\n",
        "            nome = str(id)\n",
        "            if nome not in dico.keys():\n",
        "                dico[nome] = 1\n",
        "                n = nome+f'_{dico[nome]}'\n",
        "                lista.append(n)\n",
        "            else:\n",
        "                dico[nome] += 1\n",
        "                n = nome+f'_{dico[nome]}'\n",
        "                lista.append(n)\n",
        "        print(lista)\n",
        "        df['sites'] = lista\"\"\"\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    #status_col = status_col.lower()\n",
        "    \n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,index_col)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin1').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,index_col)\n",
        "\n",
        "    else:\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,index_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col)\n",
        "\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    new_copy['sites_code'] = [re.sub('_[0-9]$', '', str(x)) for x in new_copy['sites']]\n",
        "    new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "    \n",
        "    \"\"\"n = 4\n",
        "    lista = []\n",
        "    for site in new_copy['sites']:\n",
        "        chunks = [site[i:i+n] for i in range(0, len(site), n)]\n",
        "        lista.append(chunks[0])\n",
        "    new_copy['sites_code'] = lista\n",
        "    new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\"\"\"\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    ws.delete_cols(2)\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "pathta = '/content/TA_Input_Hungary_20210831.csv'\n",
        "pathold = '/content/TA_Input_Hungary_20210731.csv'\n",
        "ta_save = '/content/HU_TA'\n",
        "ta_on = '20210731.csv'\n",
        "ta_nn = '20210831.csv'\n",
        "ta_bill = ['SiteID', 'MNO classification','Base fee (@ signing) in HUF', 'Starting date', 'Expiring date (raw data)']\n",
        "#path_OLD, path_NEW, index_col, bill_cols, path_save, old_name, new_name, type_file='mix',kind='tw'\n",
        "find_diffs_between_files(pathold, pathta, 'SiteID', ta_bill, ta_save,ta_on,ta_nn, 'csv', kind='ta')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['1077digi távközlési és szolgáltató kft.mno 3', '1253digi távközlési és szolgáltató kft.mno 3', '1781digi távközlési és szolgáltató kft.mno 3', '1866digi távközlési és szolgáltató kft.mno 3', '2036digi távközlési és szolgáltató kft.mno 3', '2362digi távközlési és szolgáltató kft.mno 3', '3094digi távközlési és szolgáltató kft.mno 3', '4089wavecom informatikai kft.otmo', '4095wavecom informatikai kft.otmo', '4821wavecom informatikai kft.otmo', '4906wavecom informatikai kft.otmo', '5048digi távközlési és szolgáltató kft.mno 3', '5870digi távközlési és szolgáltató kft.mno 3', '6009digi távközlési és szolgáltató kft.mno 3']\n",
            "Dropped Rows: ['4089wavecom informatikai kftotmo', '4095wavecom informatikai kftotmo', '4821wavecom informatikai kftotmo', '4906wavecom informatikai kftotmo']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PicU7TjzucuM"
      },
      "source": [
        "LC Comparions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMUyIEeeuDLO"
      },
      "source": [
        "import natsort\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name, new_name, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col):\n",
        "        col = lower_str(list(df.columns))\n",
        "        df.columns = col\n",
        "        #df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        #df[index_col] = df[index_col].astype(str)\n",
        "        #df['sites'] = df[index_col]\n",
        "        #df = df.iloc[natsort.index_humansorted(df[index_col])]\n",
        "        df = df.sort_values(by=[index_col, 'counterpart'])\n",
        "        lista=[]\n",
        "        dico = {}\n",
        "        for id in df[index_col]:\n",
        "            nome = str(id)\n",
        "            if nome not in dico.keys():\n",
        "                dico[nome] = 1\n",
        "                n = nome+f'_{dico[nome]}'\n",
        "                lista.append(n)\n",
        "            else:\n",
        "                dico[nome] += 1\n",
        "                n = nome+f'_{dico[nome]}'\n",
        "                lista.append(n)\n",
        "        print(lista)\n",
        "        df['sites'] = lista\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    #status_col = status_col.lower()\n",
        "    \n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,index_col)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin1').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,index_col)\n",
        "\n",
        "    else:\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,index_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col)\n",
        "\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "    \n",
        "    n = 4\n",
        "    lista = []\n",
        "    for site in new_copy['sites']:\n",
        "        chunks = [site[i:i+n] for i in range(0, len(site), n)]\n",
        "        lista.append(chunks[0])\n",
        "    new_copy['sites_code'] = lista\n",
        "    new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "pathlc = '/content/LC_Input_Hungary_20210831.csv'\n",
        "patholdlc = '/content/LC_Input_Hungary_20210731.csv'\n",
        "lc_save = '/content/HU_LC'\n",
        "lc_on = '20210731.csv'\n",
        "lc_nn = '20210831.csv'\n",
        "lc_bill = ['Code', 'Current annual lease fees in HUF','Starting date', 'Expiring date (raw data)']\n",
        "#path_OLD, path_NEW, index_col, bill_cols, path_save, old_name, new_name, type_file='mix',kind='tw'\n",
        "find_diffs_between_files(patholdlc, pathlc, 'Code', lc_bill, lc_save,lc_on,lc_nn, 'csv', kind='ta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yOTx8Ert_hU"
      },
      "source": [
        "UIS Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7QBB87Mt5dg"
      },
      "source": [
        "def find_diffs_between_files_uis(path_OLD, path_NEW, index_col, uis_cols, \\\n",
        "                             path_save, old_name, new_name, type_file='mix', sheetname='', skipr=0, skipc=0):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, uis_cols):\n",
        "\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        #df = df[uis_cols]\n",
        "        #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    #uis_cols = lower_str(uis_cols)\n",
        "    #index_col = index_col.lower()\n",
        "    # type_file = type_file.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD, sheet_name = sheetname,  skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = df_OLD.drop([0,0])\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,  skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = df_NEW.drop([0,0]) \n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "    else:\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "    print(sharedCols)\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0,thresh=len(new_copy.columns[1:]))\n",
        "    new_copy = new_copy.dropna(axis=1)\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - [{old_name}] vs [{new_name}].xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "col_uis = ['Site_ID (Alphanumeric, Alphabetical or Numeric)',\n",
        "       'Site Categorization as Phase 1 or Phase 2 Site',\n",
        "       'BTS site applicable charge (Annual)',\n",
        "       'Commercials for sites beyond 10% cap of critical sites (Annual)',\n",
        "       'Total charges to be applied for subsequent active sharing arrangement',\n",
        "       'Fixed component of energy bill', 'Variable component of energy bill',\n",
        "       'Energy consumption (for metered site)',\n",
        "       'Agreed unit price (for metered sites)',\n",
        "       'Operator’s active energy charge through estimated model',\n",
        "       'Capex incurred by TowerCo under clause 29 to be reimbursed by Operator',\n",
        "       'Energy reduction measures - Final value to be charged',\n",
        "       'Legacy site upgrades Or BBU capacity upgrade - Final value to be charged',\n",
        "       'BTS site - Site service order withdrawal - Final value to be charged',\n",
        "       'Energy infrastructure enhancements - Final value to be charged',\n",
        "       'One-off power related costs - Final value to be charged',\n",
        "       'Space blocking - Unutilized sites\\nIf unutilized then select Yes ',\n",
        "       'Decommissioning charges - Equipment removal cost - Final value to be charged',\n",
        "       'Site exit fee - Site ID part of delta',\n",
        "       'Site exit fee - Remaining term of site IDs in delta',\n",
        "       'Site power availability - Site IDs where service credits are applicable',\n",
        "       'Site power availability - Total hours applicable ',\n",
        "       'Site power availability - Unavailable time',\n",
        "       'Site power availability - Count of repeat incidents (if applicable)',\n",
        "       'Site cooling - Final value associated with Service Credit applicable',\n",
        "       'Site access - Final value associated with Service Credit applicable',\n",
        "       'Incident resolution - Final value associated with Service Credit applicable',\n",
        "       'BTS sites - Deployment time - Delay',\n",
        "       'Site modification Completion time - Delay',\n",
        "       'Delay In RSCR site modification completion Time',\n",
        "       'Delay In RSCR BTS site completion time',\n",
        "       'Additional Customer Discount percentage applicable (Number only)',\n",
        "       'Delay in Site Modification Projects', 'Delay in BTS Projects',\n",
        "       'Excess of Upgrade Capital Expenditure over Threshold']\n",
        "       \n",
        "pathuis_n = '/content/UserInput_Hungary_20210831.xlsx'\n",
        "pathuis_o = '/content/UserInput_Hungary_20210630.xlsx'\n",
        "sheetuis='SiteLevel'\n",
        "skiprows=2\n",
        "to_save = '/content/HU_UIS'\n",
        "\n",
        "old_n = 'UIS - 20210630_true_up.xlsx'\n",
        "new_n = 'UIS - 20210831.xlsx'\n",
        "\n",
        "find_diffs_between_files_uis(pathuis_o, pathuis_n, 'Site_ID (Alphanumeric or Numeric)', col_uis, \\\n",
        "                        to_save, old_n, new_n, type_file='excel', sheetname=sheetuis, skipr=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}