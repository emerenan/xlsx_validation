{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ie_comparisons.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWMnOJghmGBuyFLqK5Ce2D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emerenan/xlsx_validation/blob/main/ie_comparisons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeuZ5x7D00Qs"
      },
      "source": [
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQZy8EpD1BSt"
      },
      "source": [
        "TowerDB Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVOeXWBQ0-v2"
      },
      "source": [
        "def find_diffs_between_files_tw(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, old_name, new_name, type_file='mix',kind='tw',dates=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        #df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "    def date_parser(df, columns, format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "                \n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    else:\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW[dates] = df_NEW[dates].fillna('')\n",
        "        #date_parser(df_NEW, dates, 'normal', \"%d/%m/%Y\")\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "        status_1 = f'{status_col}_current'\n",
        "        status_2 = f'{status_col}_before'\n",
        "        new_copy = new_copy.set_index('sites')\n",
        "        new_copy = new_copy[[status_2, status_1]+ new_copy.columns[:-2].tolist()]\n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - [{old_name}] vs [{new_name}].xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                        end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "tw_bills = ['Site ID ', 'Strategic','Critical','Site Type (See structure Type)','Core Site Type',\\\n",
        "            'Transmission System','Macro Site - Transmission Hub Site',\\\n",
        "            'Macro Site - Transmission Hub Site with/without Shelters',\\\n",
        "            'Transmission Sites','Room Configuration','Power Supply','Air Conditioning',\\\n",
        "            'Site Status','Strategic_Site_Bucket','CriticalSite_Beyond_10',\\\n",
        "            'Sites_As_Metered_Estimated','Meter_Sharing_Site','Bts_Sites',\\\n",
        "            'Phase1_Site_Or_Consent_Site','Transfer_Date_Of_Consent_Site',\\\n",
        "            'WIP_Sites','Active_Sharing_Arrangement','Subsequent_Sharing_Arrangement',\\\n",
        "            'First_Active_Sharing_Deployment_Type','First_Active_Sharing_Start_Date',\\\n",
        "            'First_Active_Sharing_End_Date']\n",
        "\n",
        "pathtw='/content/TowerDB_Ireland_20210630 v1.xlsx'\n",
        "pathold = '/content/TowerDB_Ireland_20210731.csv'\n",
        "sheet='TowerDB_Input_Ireland_20210630'\n",
        "skiprows=0\n",
        "skipcols=0\n",
        "to_save = '/content/IE_TW'\n",
        "old_n = 'TowerDB_Ireland_20210630 v1.xlsx'\n",
        "new_n = \"TowerDB_Ireland_20210731.csv\"\n",
        "dates_tw = ['Infrastructure Ready Date']\n",
        "find_diffs_between_files_tw(pathold, pathtw, 'Site ID ', tw_bills,'Site Status', \\\n",
        "                            to_save, old_n, new_n, 'mix',kind='tw',dates=dates_tw, sheetname=sheet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdfSMdNU43cr"
      },
      "source": [
        "UIS Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9eauodc54YT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4ea2de-6933-4928-a950-2ebea36ac18c"
      },
      "source": [
        "def find_diffs_between_files_uis(path_OLD, path_NEW, index_col, uis_cols, \\\n",
        "                             path_save, old_name, new_name, type_file='mix', sheetname='', skipr=0, skipc=0):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, uis_cols):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        df = df[uis_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    uis_cols = lower_str(uis_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD, sheet_name = sheetname, header=0, names=uis_cols, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, header=0, names=uis_cols, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "    else:\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - [{old_name}] vs [{new_name}].xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "col_uis = [\"Site_ID (Alphanumeric)\",\"BTS site applicable charge (Annual)\",\\\n",
        " \"Commercials for sites beyond 10% cap of critical sites\",\\\n",
        " \"Total charges to be applied for subsequent active sharing arrangement\",\\\n",
        " \"Fixed component of energy bill\",\"Variable component of energy bill\",\\\n",
        " \"Energy consumption (for metered site)\",\"Agreed unit price (for metered sites)\",\\\n",
        " \"Operator’s active energy charge through estimated model\",\\\n",
        " \"Agreed Passive Energy Charge for Outdoor Sites with DC Power\",\\\n",
        " \"Energy reduction measures - Final value to be charged\",\\\n",
        " \"Legacy site upgrades Or BBU capacity upgrade - Final value to be charged\",\\\n",
        " \"BTS site - Site service order withdrawal - Final value to be charged\",\\\n",
        " \"Energy infrastructure enhancements - Final value to be charged\",\\\n",
        " \"One-off power related Capex costs - Final value to be charged\",\\\n",
        " \"Space blocking - Unutilized sites\\nIf unutilized then select Yes \",\\\n",
        " \"Decommissioning charges - Equipment removal cost - Final value to be charged\",\\\n",
        " \"Site exit fee - Site ID part of delta\",\"Site exit fee - Remaining term of site IDs in delta\",\\\n",
        " \"Site power availability - Site IDs where service credits are applicable\",\\\n",
        " \"Site power availability - Site IDs for Critical sites Categorization\",\\\n",
        " \"Site power availability - Total hours applicable \",\"Site power availability - Unavailable time\",\\\n",
        " \"Site power availability - Count of repeat incidents (if applicable)\",\\\n",
        " \"Site cooling - Final value associated with Service Credit applicable\",\\\n",
        " \"Site access - Final value associated with Service Credit applicable\",\\\n",
        " \"Incident resolution - Final value associated with Service Credit applicable\",\\\n",
        " \"BTS sites - Deployment time - Delay\",\"Site modification Completion time - Delay\",\\\n",
        " \"Delay In RSCR site modification completion Time\",\"Delay In RSCR BTS site completion time\",\\\n",
        " \"Delay in Site Modification Projects\",\"Delay in BTS Projects\",\"Excess of Upgrade Capital Expenditure over Threshold\"]\n",
        "pathuis_n = '/content/UserInput_Ireland_July 21_06.07.2021.xlsx'\n",
        "pathuis_o = '/content/UserInput_Ireland_20210630_07.07.21.xlsx'\n",
        "sheetuis='SiteLevel'\n",
        "skiprows=2\n",
        "to_save = '/content/IE_UIS'\n",
        "old_n = 'UIS - Old'\n",
        "new_n = \"UIS - Current\"\n",
        "\n",
        "find_diffs_between_files_uis(pathuis_o, pathuis_n, 'Site_ID (Alphanumeric)', col_uis, \\\n",
        "                        to_save, old_n, new_n, type_file='excel', sheetname=sheetuis, skipr=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['WXBYL']\n",
            "Dropped Rows: ['DN370']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMlHiSK0_Udp"
      },
      "source": [
        "TA Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSUiH35PEUAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b49d8d-2588-48f5-a297-d6f8995c9d8a"
      },
      "source": [
        "def find_diffs_between_files_ta(path_OLD, path_NEW, old_name, new_name, index_col, bill_cols, path_save, type_file='mix', am_cols = [],\\\n",
        "                             cols_int=[], cols_date=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "     \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "    \n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                #df[column] = df[column].replace(0, \"\")\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "                #return df[column]\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "                \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        #return df\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    am_cols = lower_str(am_cols)\n",
        "    cols_int = lower_str(cols_int)\n",
        "    cols_date = lower_str(cols_date)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        #Ajustado só para CZ_Julho, tirei o encoding\n",
        "        df_NEW = pd.read_csv(path_NEW).fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    else:\n",
        "        ta_col_ord = [\"Site ID \",\"Site Name \",\"Address \",\"Site Provider\",\"Site type\",\\\n",
        "              \"Tenant/Customer \",\"Customer Type \",\" Initial Rent  \",\" Current Rent  \",\" REC Uplift Charge \",\\\n",
        "              \"Current Power Charge\",\"Renegotiations \",\"Lease Start Date \",\"Lease End date \",\"Right to Renew \",\\\n",
        "              \"End of Renewal Period \",\"Payment Terms \",\"Payment Frequency \",\"Rent Review \",\"Indexation Driver\",\\\n",
        "              \"Fixed Increase \",\"CPI \",\"Percentage\",\"Increase frequency\",\"Increase Date\",\"VAT\",\"Rate \",\"Termination date \"]\n",
        "        ta_col_ord = lower_str(ta_col_ord)\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='ISO-8859-1')\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD = df_OLD.sort_values(by=index_col)\n",
        "        \n",
        "        lista = []\n",
        "        df_OLD['sites'] = df_OLD[index_col]+df_OLD['tenant/customer ']\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD[k].astype(str)+df_OLD['count of contrbct '].astype(str)+df_OLD[\"count of key\"].astype(str)+df_OLD[\"counterpart\"].astype(str)\n",
        "        df_OLD = df_OLD.fillna('')\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "        \"\"\"Read New File\"\"\"\n",
        "        \n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW[ta_col_ord]\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW = df_NEW.sort_values(by=index_col)\n",
        "        #df_NEW[\"uplift for sublet \"] = [re.sub('^€', '', str(x)) for x in df_NEW[\"uplift for sublet \"]]\n",
        "        #df_NEW[\"uplift for sublet \"] = df_NEW[\"uplift for sublet \"].replace('-', '')\n",
        "        lista = []\n",
        "        df_NEW['sites'] = df_NEW[index_col]+df_NEW['tenant/customer ']\n",
        "\n",
        "        #print([i for i in list(df_NEW['sites']) if list(df_NEW['sites']).count(i)>1])\n",
        "        #df_NEW = df_NEW.replace(['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' '], '')\n",
        "        \n",
        "        \"\"\"if len(am_cols)>0:\n",
        "            for col in am_cols:\n",
        "                df_NEW[col] = df_NEW[col].fillna(0)\n",
        "                df_NEW[col] = df_NEW[col].astype(int).apply(lambda x: f'{x:,}')\"\"\"\n",
        "\n",
        "        \"\"\"if len(cols_int)>0:\n",
        "            for col in cols_int:\n",
        "                df_NEW[col] = df_NEW[col].fillna(0)\n",
        "                df_NEW[col] = df_NEW[col].astype(int)\"\"\"\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "\n",
        "        replace_values(df_NEW, bill_cols)\n",
        "        date_parser(df_NEW, cols_date, 'mix', \"%d/%m/%Y\")\n",
        "\n",
        "        df_NEW = df_NEW.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "        #print(df_NEW.info())\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        \n",
        "        try:\n",
        "            df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        except:\n",
        "            print('falha')\n",
        "        #print(list(df_NEW.columns)==(df_OLD.columns))\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    n = 5\n",
        "    lista = []\n",
        "    for site in new_copy['sites']:\n",
        "        chunks = [site[i:i+n] for i in range(0, len(site), n)]\n",
        "        lista.append(chunks[0])\n",
        "    new_copy['sites_code'] = lista\n",
        "    new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} {old_name} vs {new_name}.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                             end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "\n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    ws.delete_cols(2)\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "\n",
        "\n",
        "ta_col = [\"Site ID \",\"Tenant/Customer \",\"Customer Type \",\\\n",
        "                \"Lease Start Date \",\"Lease End date \"]\n",
        "path_ta_input = '/content/TowerDB_Ireland_20210630 v1.xlsx'\n",
        "ta_old = '/content/TA_Input_Ireland_20210731.csv'\n",
        "ta_save = '/content/IE_TA'\n",
        "ta_tab = 'TA_Input_Ireland_20210630'\n",
        "dates_ta = [\"Lease Start Date \",\"Lease End date \", \"Increase Date\"]\n",
        "interger_ta = ['Increase frequency']\n",
        "amount_ta = [\" Initial Rent  \", \" Current Rent  \"]\n",
        "old_n_ta = 'old_in_month(csv)'\n",
        "new_n_ta = 'new_file_in_month(xlsx)'\n",
        "\"\"\"find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, \\\n",
        "                         status_col='', path_save=ta_save, type_file='mix',kind='ta',\\\n",
        "                         kind_col='tenant agreement id - new',k='service type',am_cols = amount_ta,\\\n",
        "                         cols_int=interger, cols_date=dates, sheetname=sheet)\"\"\"\n",
        "find_diffs_between_files_ta(ta_old, path_ta_input,old_n_ta,new_n_ta,  'Site ID ', ta_col, path_save=ta_save,type_file='mix',am_cols=amount_ta,\\\n",
        "                           cols_int=interger_ta, cols_date=dates_ta, sheetname=ta_tab)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['CE030Imagine', 'MN028 Imagine', 'MO032Eir Mobile', 'WD012Imagine', 'WXBYLImagine']\n",
            "Dropped Rows: []\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2X8dqji_XRg"
      },
      "source": [
        "LC Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FmfU6lw_jUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3ce178-3e3c-4111-8e29-a835b83d0faf"
      },
      "source": [
        "def find_diffs_between_files_lc(path_OLD, path_NEW, old_name, new_name, index_col, bill_cols, path_save, type_file='mix', am_cols = [],\\\n",
        "                                cols_int=[], cols_date=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "     \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "    \n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                #df[column] = df[column].replace(0, \"\")\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "                #return df[column]\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "                \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    am_cols = lower_str(am_cols)\n",
        "    cols_int = lower_str(cols_int)\n",
        "    cols_date = lower_str(cols_date)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        #Ajustado só para CZ_Julho, tirei o encoding\n",
        "        df_NEW = pd.read_csv(path_NEW).fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    else:\n",
        "        col_order_lc = [\"Site ID \",\"Site Name \",\"Address \",\"Site Provider\",\"Contract Type\",\"Vendor\",\" Current Rent  \",\\\n",
        "          \" Sublet/Uplift \",\" Total Rent \",\"Misc Fees \",\"Sublet Clause \",\"Conditions \",\"Uplift for Sublet \",\\\n",
        "          \"Lease Start Date \",\"Lease End date \",\"Right to Renew \",\"End of Renewal Period \",\"Payment Terms \",\\\n",
        "          \"Payment Frequency\",\"Rent Review \",\"Fixed Increase \",\"Indexation Driver\",\"CPI/Increase Type\",\\\n",
        "          \"Percentage\",\"Increase Frequency\",\"Increase date\",\"VAT Subject\",\"VAT\",\"Termination date \",\"Observations\"]\n",
        "        col_order_lc = lower_str(col_order_lc)\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='ISO-8859-1')\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD = df_OLD.sort_values(by=index_col)\n",
        "        \n",
        "        lista = []\n",
        "        df_OLD['sites'] = df_OLD[index_col].copy()\n",
        "        for i in range(len(df_OLD[index_col])):\n",
        "            if str(df_OLD.iloc[i][index_col]) in lista:           \n",
        "                df_OLD.loc[i, 'sites'] = str(df_OLD.iloc[i][index_col])+\"_\"+str(lista.count(str(df_OLD.iloc[i][index_col])))\n",
        "            else:  \n",
        "                df_OLD.loc[i, 'sites']= str(df_OLD.iloc[i][index_col])+\"_0\"\n",
        "\n",
        "            lista.append(str(df_OLD.iloc[i][index_col]))   \n",
        "\n",
        "        print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD[k].astype(str)+df_OLD['count of contrbct '].astype(str)+df_OLD[\"count of key\"].astype(str)+df_OLD[\"counterpart\"].astype(str)\n",
        "        df_OLD = df_OLD.fillna('')\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "        \"\"\"Read New File\"\"\"\n",
        "        \n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW[col_order_lc]\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW = df_NEW.sort_values(by=index_col)\n",
        "        df_NEW[\"uplift for sublet \"] = [re.sub('^€', '', str(x)) for x in df_NEW[\"uplift for sublet \"]]\n",
        "        df_NEW[\"uplift for sublet \"] = df_NEW[\"uplift for sublet \"].replace('-', '')\n",
        "        lista = []\n",
        "        df_NEW['sites'] = df_NEW[index_col].copy()\n",
        "        for i in range(len(df_NEW[index_col])):\n",
        "            if str(df_NEW.iloc[i][index_col]) in lista:           \n",
        "                df_NEW.loc[i, 'sites'] = str(df_NEW.iloc[i][index_col])+\"_\"+str(lista.count(str(df_NEW.iloc[i][index_col])))\n",
        "            else:   \n",
        "                df_NEW.loc[i, 'sites'] = str(df_NEW.iloc[i][index_col])+\"_0\"\n",
        "            lista.append(str(df_NEW.iloc[i][index_col]))   \n",
        "         #print([i for i in list(df_NEW['sites']) if list(df_NEW['sites']).count(i)>1])\n",
        "        df_NEW = replace_values(df_NEW, am_cols, 0)\n",
        "        if len(am_cols)>0:\n",
        "            for col in am_cols:\n",
        "                #df[col] = df[col].fillna(0)\n",
        "                df_NEW[col] = df_NEW[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "\n",
        "        if len(cols_int)>0:\n",
        "            for col in cols_int:\n",
        "                df_NEW[col] = df_NEW[col].fillna(0)\n",
        "                df_NEW[col] = df_NEW[col].astype(int)\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "\n",
        "        df_NEW = replace_values(df_NEW, bill_cols)\n",
        "        date_parser(df_NEW, cols_date, 'mix', \"%d/%m/%Y\")\n",
        "\n",
        "        df_NEW = df_NEW.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "        #print(df_NEW.info())\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        \n",
        "        try:\n",
        "            df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        except:\n",
        "            print('falha')\n",
        "        #print(list(df_NEW.columns)==(df_OLD.columns))\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "    n = 5\n",
        "    lista = []\n",
        "    for site in new_copy['sites']:\n",
        "        chunks = [site[i:i+n] for i in range(0, len(site), n)]\n",
        "        lista.append(chunks[0])\n",
        "    print(lista)\n",
        "    new_copy['sites_code'] = lista\n",
        "    new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} {old_name} vs {new_name}.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                             end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "\n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    ws.delete_cols(2)\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "lc_ord = [\"Site ID \",\"Site Name \",\"Address \",\"Site Provider\",\"Contract Type\",\"Vendor\",\" Current Rent  \",\\\n",
        "          \" Sublet/Uplift \",\" Total Rent \",\"Misc Fees \",\"Sublet Clause \",\"Conditions \",\"Uplift for Sublet \",\\\n",
        "          \"Lease Start Date \",\"Lease End date \",\"Right to Renew \",\"End of Renewal Period \",\"Payment Terms \",\\\n",
        "          \"Payment Frequency\",\"Rent Review \",\"Fixed Increase \",\"Indexation Driver\",\"CPI/Increase Type\",\\\n",
        "          \"Percentage\",\"Increase Frequency\",\"Increase date\",\"VAT Subject\",\"VAT\",\"Termination date \",\"Observations\"]\n",
        "\n",
        "lc_col = ['Site ID ',' Total Rent ', 'Lease Start Date ', 'Lease End date ']\n",
        "path_lc_input = '/content/TowerDB_Ireland_20210630 v1.xlsx'\n",
        "lc_old = '/content/LC_Input_Ireland_20210731.csv'\n",
        "lc_save = '/content/IE_LC'\n",
        "lc_tab = 'LC_Input_Ireland_20210630'\n",
        "dates_lc = [\"Lease Start Date \",\"Lease End date \", 'increase date']\n",
        "interger_lc = [\" Sublet/Uplift \"]\n",
        "amount_lc = [\" Total Rent \"]\n",
        "old_n = 'old_in_month(csv)'\n",
        "new_n = 'new_file_in_month(xlsx)'\n",
        "\"\"\"find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, \\\n",
        "                         status_col='', path_save=ta_save, type_file='mix',kind='ta',\\\n",
        "                         kind_col='tenant agreement id - new',k='service type',am_cols = amount_ta,\\\n",
        "                         cols_int=interger, cols_date=dates, sheetname=sheet)\"\"\"\n",
        "find_diffs_between_files_lc(lc_old, path_lc_input,old_n,new_n, 'Site ID ', lc_col, path_save=lc_save,type_file='mix',am_cols=[],\\\n",
        "                           cols_int=[], cols_date=dates_lc, sheetname=lc_tab)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "['CE006', 'CE007', 'CE009', 'CE010', 'CE014', 'CE015', 'CE016', 'CE018', 'CE020', 'CE023', 'CE024', 'CE030', 'CE031', 'CE032', 'CE032', 'CE032', 'CE039', 'CE040', 'CE044', 'CE046', 'CE050', 'CE054', 'CE057', 'CE058', 'CE066', 'CE068', 'CE070', 'CE071', 'CE076', 'CE085', 'CE092', 'CEDDC', 'CK003', 'CK004', 'CK005', 'CK013', 'CK019', 'CK022', 'CK027', 'CK030', 'CK032', 'CK035', 'CK039', 'CK041', 'CK044', 'CK048', 'CK052', 'CK053', 'CK053', 'CK053', 'CK055', 'CK056', 'CK056', 'CK059', 'CK066', 'CK071', 'CK073', 'CK079', 'CK083', 'CK084', 'CK087', 'CK094', 'CK099', 'CK100', 'CK111', 'CK113', 'CK143', 'CK144', 'CK147', 'CK156', 'CK166', 'CK173', 'CK176', 'CK192', 'CK198', 'CK199', 'CK204', 'CK207', 'CK211', 'CK213', 'CK216', 'CK219', 'CK221', 'CK224', 'CK229', 'CK235', 'CK238', 'CK242', 'CK243', 'CK245', 'CK247', 'CK253', 'CK259', 'CK272', 'CK273', 'CK276', 'CK294', 'CK298', 'CK299', 'CK302', 'CK304', 'CK307', 'CK311', 'CK313', 'CK320', 'CK323', 'CK324', 'CK327', 'CK329', 'CK330', 'CK332', 'CK334', 'CK335', 'CK340', 'CK342', 'CK344', 'CK347', 'CK349', 'CK365', 'CK371', 'CK376', 'CK377', 'CK379', 'CK380', 'CK384', 'CK385', 'CK392', 'CK400', 'CK405', 'CK446', 'CK449', 'CK556', 'CK604', 'CK614', 'CK634', 'CK640', 'CK650', 'CK665', 'CK684', 'CK693', 'CKBFO', 'CKCHM', 'CKCKA', 'CKCOH', 'CKCUH', 'CKCVE', 'CKCXR', 'CKGHS', 'CKICA', 'CKLQY', 'CKMSE', 'CKMTG', 'CKRIS', 'CKWHP', 'CN004', 'CN008', 'CN009', 'CN010', 'CN011', 'CN012', 'CN017', 'CN020', 'CN021', 'CN022', 'CN023', 'CN024', 'CN026', 'CN047', 'CN047', 'CN049', 'CN050', 'CN051', 'CN055', 'CN059', 'CN060', 'CN061', 'CN063', 'CN066', 'CN069', 'CN509', 'CN559', 'CN562', 'CN563', 'CN563', 'CNGFR', 'CNSGA', 'CW002', 'CW010', 'CW014', 'CW018', 'CW035', 'CW037', 'CW047', 'CW051', 'CW052', 'CW055', 'CW056', 'CWLKN', 'DL012', 'DL017', 'DL020', 'DL023', 'DL025', 'DL027', 'DL031', 'DL036', 'DL068', 'DL069', 'DL078', 'DL088', 'DL095', 'DL098', 'DL100', 'DL101', 'DL103', 'DL108', 'DL109', 'DL112', 'DL119', 'DL121', 'DL129', 'DL130', 'DL131', 'DL2LD', 'DLSOR', 'DN011', 'DN036', 'DN040', 'DN047', 'DN050', 'DN058', 'DN067', 'DN073', 'DN073', 'DN083', 'DN089', 'DN090', 'DN094', 'DN099', 'DN101', 'DN103', 'DN104', 'DN105', 'DN109', 'DN111', 'DN113', 'DN115', 'DN116', 'DN117', 'DN120', 'DN121', 'DN124', 'DN126', 'DN129', 'DN141', 'DN145', 'DN146', 'DN148', 'DN152', 'DN155', 'DN165', 'DN167', 'DN168', 'DN173', 'DN176', 'DN179', 'DN180', 'DN189', 'DN192', 'DN198', 'DN199', 'DN1AR', 'DN1FN', 'DN200', 'DN201', 'DN204', 'DN211', 'DN218', 'DN235', 'DN236', 'DN247', 'DN250', 'DN253', 'DN256', 'DN257', 'DN258', 'DN261', 'DN265', 'DN270', 'DN273', 'DN274', 'DN278', 'DN279', 'DN280', 'DN281', 'DN282', 'DN287', 'DN287', 'DN288', 'DN292', 'DN293', 'DN295', 'DN300', 'DN301', 'DN302', 'DN303', 'DN304', 'DN305', 'DN307', 'DN310', 'DN311', 'DN313', 'DN314', 'DN316', 'DN317', 'DN320', 'DN322', 'DN323', 'DN324', 'DN327', 'DN332', 'DN335', 'DN338', 'DN340', 'DN341', 'DN343', 'DN353', 'DN354', 'DN355', 'DN360', 'DN365', 'DN366', 'DN370', 'DN371', 'DN373', 'DN378', 'DN379', 'DN380', 'DN382', 'DN385', 'DN386', 'DN390', 'DN392', 'DN394', 'DN395', 'DN400', 'DN406', 'DN407', 'DN413', 'DN416', 'DN417', 'DN420', 'DN421', 'DN422', 'DN423', 'DN435', 'DN435', 'DN436', 'DN447', 'DN449', 'DN451', 'DN453', 'DN459', 'DN461', 'DN468', 'DN475', 'DN477', 'DN478', 'DN479', 'DN480', 'DN481', 'DN488', 'DN491', 'DN492', 'DN493', 'DN495', 'DN499', 'DN501', 'DN504', 'DN506', 'DN508', 'DN509', 'DN510', 'DN511', 'DN513', 'DN514', 'DN515', 'DN518', 'DN519', 'DN522', 'DN524', 'DN531', 'DN532', 'DN535', 'DN536', 'DN537', 'DN540', 'DN542', 'DN544', 'DN546', 'DN555', 'DN565', 'DN569', 'DN570', 'DN572', 'DN573', 'DN575', 'DN576', 'DN577', 'DN578', 'DN580', 'DN584', 'DN585', 'DN588', 'DN589', 'DN590', 'DN591', 'DN599', 'DN600', 'DN614', 'DN618', 'DN620', 'DN623', 'DN627', 'DN635', 'DN636', 'DN642', 'DN647', 'DN648', 'DN649', 'DN650', 'DN654', 'DN656', 'DN657', 'DN658', 'DN659', 'DN661', 'DN663', 'DN664', 'DN665', 'DN676', 'DN677', 'DN680', 'DN684', 'DN686', 'DN687', 'DN707', 'DN709', 'DN710', 'DN710', 'DN710', 'DN712', 'DN714', 'DN716', 'DN719', 'DN730', 'DN735', 'DN739', 'DN746', 'DN748', 'DN759', 'DN760', 'DN763', 'DN765', 'DN769', 'DN771', 'DN776', 'DN782', 'DN786', 'DN791', 'DN792', 'DN793', 'DN794', 'DN795', 'DN801', 'DN812', 'DN822', 'DN823', 'DN826', 'DN832', 'DN836', 'DN837', 'DN842', 'DN843', 'DN851', 'DN852', 'DN861', 'DN862', 'DN864', 'DN866', 'DN869', 'DN875', 'DN888', 'DN901', 'DN903', 'DN911', 'DN913', 'DN924', 'DN925', 'DN932', 'DN938', 'DN947', 'DN949', 'DN951', 'DN952', 'DNABC', 'DNABS', 'DNBDE', 'DNBE1', 'DNBS1', 'DNBW1', 'DNCF1', 'DNCI1', 'DNCP1', 'DNDOC', 'DNDP1', 'DNDSD', 'DNFL1', 'DNGS2', 'DNGS3', 'DNHB1', 'DNIGB', 'DNITE', 'DNJWT', 'DNMHD', 'DNNGE', 'DNNSB', 'DNRD1', 'DNRD2', 'DNRD3', 'DNSBS', 'DNSD1', 'DNSDX', 'DNSL1', 'DNSS2', 'DNTPH', 'DNTS1', 'DNTWR', 'DNTY1', 'DNVIC', 'DNWR1', 'DNWSE', 'DX014', 'DX053', 'DX066', 'DX074', 'DX084', 'DX109', 'DX128', 'DX144', 'DX146', 'DX148', 'DX166', 'DX169', 'DX194', 'DX218', 'DX219', 'DX224', 'DX230', 'DX237', 'DX252', 'DX259', 'DX263', 'DX264', 'DX282', 'GY008', 'GY011', 'GY014', 'GY015', 'GY019', 'GY021', 'GY023', 'GY024', 'GY025', 'GY030', 'GY032', 'GY034', 'GY036', 'GY037', 'GY044', 'GY045', 'GY046', 'GY047', 'GY048', 'GY049', 'GY054', 'GY055', 'GY066', 'GY075', 'GY080', 'GY082', 'GY090', 'GY092', 'GY099', 'GY100', 'GY101', 'GY101', 'GY102', 'GY104', 'GY105', 'GY111', 'GY113', 'GY115', 'GY116', 'GY120', 'GY124', 'GY125', 'GY128', 'GY132', 'GY133', 'GY135', 'GY138', 'GY144', 'GY152', 'GY156', 'GY157', 'GY159', 'GY160', 'GY161', 'GY163', 'GY164', 'GY165', 'GY167', 'GY168', 'GY170', 'GY171', 'GY171', 'GY172', 'GY173', 'GY176', 'GY179', 'GY179', 'GY190', 'GY191', 'GY192', 'GY198', 'GY198', 'GY204', 'GY205', 'GY211', 'GY213', 'GY215', 'GY223', 'GY556', 'GY564', 'GY568', 'GY570', 'GYADC', 'GYBEH', 'GYINV', 'GYTBY', 'GYTEP', 'KE006', 'KE007', 'KE008', 'KE011', 'KE012', 'KE013', 'KE014', 'KE015', 'KE017', 'KE018', 'KE020', 'KE022', 'KE025', 'KE026', 'KE027', 'KE029', 'KE030', 'KE046', 'KE048', 'KE057', 'KE058', 'KE059', 'KE070', 'KE072', 'KE079', 'KE080', 'KE082', 'KE085', 'KE092', 'KE093', 'KE094', 'KE098', 'KE100', 'KE113', 'KE118', 'KE124', 'KE178', 'KE364', 'KEA3J', 'KEA4H', 'KEA5C', 'KEAMN', 'KECUP', 'KECYE', 'KEG4K', 'KEMHE', 'KEMHS', 'KK002', 'KK003', 'KK004', 'KK006', 'KK010', 'KK012', 'KK014', 'KK021', 'KK022', 'KK027', 'KK040', 'KK044', 'KK047', 'KK050', 'KK051', 'KK051', 'KK060', 'KK062', 'KK066', 'KK074', 'KK075', 'KK076', 'KK081', 'KK089', 'KK092', 'KK100', 'KK112', 'KKGCW', 'KY012', 'KY013', 'KY016', 'KY017', 'KY019', 'KY023', 'KY024', 'KY026', 'KY029', 'KY030', 'KY032', 'KY034', 'KY039', 'KY047', 'KY048', 'KY049', 'KY052', 'KY053', 'KY055', 'KY056', 'KY058', 'KY074', 'KY090', 'KY106', 'KY108', 'KY113', 'KY118', 'KY123', 'KY124', 'KY127', 'KY128', 'KY129', 'KY130', 'KY131', 'KY133', 'KY134', 'KY136', 'KY142', 'KY153', 'KY155', 'KY161', 'KY562', 'KYAVN', 'KYCYT', 'KYEHL', 'KYMSH', 'LD003', 'LD006', 'LD007', 'LD008', 'LD010', 'LD013', 'LD016', 'LD017', 'LD025', 'LD02A', 'LD042', 'LD043', 'LD045', 'LD047', 'LD052', 'LD055', 'LD056', 'LD505', 'LH002', 'LH004', 'LH010', 'LH011', 'LH012', 'LH021', 'LH034', 'LH035', 'LH047', 'LH052', 'LH057', 'LH058', 'LH063', 'LH072', 'LH075', 'LH076', 'LH563', 'LHALT', 'LK006', 'LK007', 'LK008', 'LK010', 'LK011', 'LK015', 'LK016', 'LK018', 'LK019', 'LK022', 'LK024', 'LK031', 'LK034', 'LK035', 'LK036', 'LK046', 'LK050', 'LK052', 'LK053', 'LK054', 'LK060', 'LK064', 'LK065', 'LK067', 'LK068', 'LK071', 'LK073', 'LK074', 'LK076', 'LK077', 'LK078', 'LK079', 'LK080', 'LK082', 'LK083', 'LK089', 'LK090', 'LK092', 'LK094', 'LK097', 'LK098', 'LK099', 'LK100', 'LK104', 'LK108', 'LK116', 'LK117', 'LK119', 'LK127', 'LK131', 'LK132', 'LK135', 'LK560', 'LKC3C', 'LM002', 'LM006', 'LM011', 'LM013', 'LM014', 'LM016', 'LM017', 'LM018', 'LM019', 'LM025', 'LM027', 'LM028', 'LM043', 'LM554', 'LMO5A', 'LMWHE', 'LS002', 'LS008', 'LS017', 'LS018', 'LS020', 'LS022', 'LS040', 'LS041', 'LS049', 'LS054', 'LSAXR', 'LSB1C', 'LSB2A', 'LSCLT', 'LSGFW', 'LSKIE', 'LSMRE', 'LSRGB', 'MH004', 'MH009', 'MH010', 'MH012', 'MH013', 'MH015', 'MH017', 'MH019', 'MH020', 'MH021', 'MH022', 'MH024', 'MH025', 'MH031', 'MH033', 'MH035', 'MH036', 'MH038', 'MH042', 'MH051', 'MH052', 'MH055', 'MH057', 'MH060', 'MH063', 'MH066', 'MH069', 'MH070', 'MH072', 'MH074', 'MH078', 'MH083', 'MH085', 'MH086', 'MH096', 'MH097', 'MH098', 'MH108', 'MH109', 'MH111', 'MH117', 'MH121', 'MH124', 'MH127', 'MHABE', 'MHCRS', 'MN003', 'MN004', 'MN007', 'MN008', 'MN009', 'MN010', 'MN022', 'MN025', 'MN026', 'MN027', 'MN027', 'MN028', 'MN035', 'MN037', 'MN040', 'MN041', 'MN043', 'MN045', 'MN054', 'MN058', 'MN060', 'MN063', 'MN065', 'MN066', 'MN067', 'MN072', 'MN558', 'MNCVG', 'MO004', 'MO008', 'MO009', 'MO009', 'MO018', 'MO019', 'MO021', 'MO022', 'MO027', 'MO032', 'MO034', 'MO035', 'MO037', 'MO038', 'MO043', 'MO045', 'MO048', 'MO061', 'MO071', 'MO072', 'MO074', 'MO075', 'MO082', 'MO084', 'MO085', 'MO086', 'MO088', 'MO093', 'MO097', 'MO099', 'MO567', 'MOKAP', 'MOTCH', 'OY008', 'OY009', 'OY016', 'OY017', 'OY029', 'OY030', 'OY031', 'OY038', 'OY039', 'OY052', 'OY059', 'OY068', 'OY070', 'OYAAN', 'OYB5C', 'OYNTN', 'RN005', 'RN010', 'RN011', 'RN014', 'RN016', 'RN019', 'RN038', 'RN039', 'RN041', 'RN047', 'RN048', 'RN053', 'RN054', 'RN056', 'RN057', 'RN058', 'RN060', 'RN063', 'RNARG', 'RNK8A', 'RNK9S', 'RNSCE', 'RNSLM', 'SO008', 'SO009', 'SO010', 'SO012', 'SO016', 'SO017', 'SO018', 'SO033', 'SO03A', 'SO042', 'SO071', 'SO073', 'SOK11', 'TY001', 'TY002', 'TY003', 'TY004', 'TY009', 'TY011', 'TY013', 'TY014', 'TY018', 'TY020', 'TY024', 'TY026', 'TY028', 'TY029', 'TY029', 'TY031', 'TY032', 'TY036', 'TY037', 'TY038', 'TY044', 'TY046', 'TY052', 'TY060', 'TY063', 'TY065', 'TY066', 'TY067', 'TY072', 'TY077', 'TY078', 'TY079', 'TY086', 'TY087', 'TY093', 'TY093', 'TY095', 'TY097', 'TY099', 'TY117', 'TY555', 'TYB7K', 'TYB8T', 'TYBHB', 'WD006', 'WD012', 'WD017', 'WD019', 'WD024', 'WD045', 'WD050', 'WD051', 'WD061', 'WD063', 'WD067', 'WD074', 'WD075', 'WD076', 'WD080', 'WD081', 'WD085', 'WD086', 'WD095', 'WD096', 'WD097', 'WD101', 'WD104', 'WD106', 'WD109', 'WD110', 'WD116', 'WD119', 'WD124', 'WD603', 'WD606', 'WD610', 'WD612', 'WDCDG', 'WDCHN', 'WDHRN', 'WH004', 'WH007', 'WH009', 'WH011', 'WH012', 'WH015', 'WH016', 'WH018', 'WH022', 'WH027', 'WH029', 'WH032', 'WH038', 'WH043', 'WH050', 'WH055', 'WH056', 'WHGEE', 'WHLIN', 'WW005', 'WW007', 'WW008', 'WW010', 'WW011', 'WW012', 'WW015', 'WW020', 'WW022', 'WW023', 'WW024', 'WW025', 'WW026', 'WW027', 'WW029', 'WW032', 'WW033', 'WW034', 'WW036', 'WW042', 'WW043', 'WW044', 'WW046', 'WW047', 'WW052', 'WW053', 'WW055', 'WW056', 'WW069', 'WW074', 'WW076', 'WW077', 'WW080', 'WW088', 'WWBRR', 'WWDEL', 'WWSFD', 'WX004', 'WX008', 'WX008', 'WX012', 'WX013', 'WX014', 'WX017', 'WX018', 'WX020', 'WX021', 'WX022', 'WX023', 'WX028', 'WX032', 'WX033', 'WX036', 'WX037', 'WX042', 'WX043', 'WX049', 'WX051', 'WX054', 'WX055', 'WX056', 'WX058', 'WX061', 'WX066', 'WX068', 'WX070', 'WX072', 'WX074', 'WX086', 'WX171', 'WXAKH', 'WXBYL', 'WXSBY']\n",
            "\n",
            "New Rows:     []\n",
            "Dropped Rows: []\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0onKWT9F938"
      },
      "source": [
        "df_NEW = pd.read_excel('/content/TowerDB_Ireland_20210630 v1.xlsx',sheet_name = 'ta_Input_Ireland_20210630')\n",
        "df_NEW.columns"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}