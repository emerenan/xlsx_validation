{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hu_validations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWBXT7jlhal9y4U/GcVtwM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sPiDTWZxtiO"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "\n",
        "def csv_files(path):\n",
        "    \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "    import csv\n",
        "    f = open(path, encoding='windows-1252', errors='ignore')\n",
        "    data = []\n",
        "    for row in csv.reader(f, delimiter=','):\n",
        "        data.append(row)\n",
        "    col = [*data[0]]\n",
        "    data.pop(0)\n",
        "    df = pd.DataFrame(data, columns=col)\n",
        "    return df, col\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index):\n",
        "    \"\"\"\n",
        "    Params:\\n\n",
        "    path: parth of file in the computer.\\n\n",
        "    n_skiprows: Number of rows to delete in the original file,.\\n\n",
        "    columns_to_convert: Columns to convert the data general type. \\n\n",
        "    n_skipcolumn: Columns to skip in the original file. \\n\n",
        "    endrow = pass 0 to read everything, 1 to count entire\n",
        "    columns_order: List of columns names in specific order to pass in the engine.\\n\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    \"\"\"cont = 0\n",
        "    for i in df.iloc[:,site_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[:cont, :]\n",
        "    #df.columns = columns_order\"\"\"\n",
        "    \n",
        "    # convert intery columns to integer \n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].fillna(0)\n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].astype('int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def check_df(df, error_msg):\n",
        "    if df == None or df.empty:\n",
        "        return f'{error_msg}'\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def count_duplicates(lista):\n",
        "    count_dict = {}\n",
        "    for entry in lista:\n",
        "        if entry in count_dict.keys():\n",
        "            count_dict[entry] += 1\n",
        "        else:\n",
        "            count_dict[entry] = 1\n",
        "    \n",
        "    duplicates = {}\n",
        "    for k, v in count_dict.items():\n",
        "        if v > 1:\n",
        "            duplicates[k] = v\n",
        "    return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "def defining_df(df, column_range, number_col):\n",
        "    cont = 0\n",
        "    for i in df.iloc[:,number_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[0:cont, :]\n",
        "    return df\n",
        "\n",
        "#old version\n",
        "def check_columns(table, output_columns):\n",
        "    \"\"\"\n",
        "    Check the total of number of missing columns and the missing columns in passed table.\\n\n",
        "\n",
        "    Params:\\n\n",
        "    table: contain the columns to be check.\\n\n",
        "    output_columns: columns structure at the final file. \n",
        "\n",
        "    Returns:\\n\n",
        "    Number of missing columns and a list that contains the name os missing columns.\n",
        "    \"\"\"\n",
        "    total_received = len(table.columns)\n",
        "    number_missing_columns = 0\n",
        "    missing_columns = []\n",
        "    #Counting of missing columns       \n",
        "    #if country in contries:      \n",
        "    for columns in output_columns:\n",
        "        if columns.lower() not in [labels.lower() for labels in table]:\n",
        "            number_missing_columns +=1\n",
        "            missing_columns.append(columns)\n",
        "    \n",
        "    return total_received, number_missing_columns, missing_columns\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    \"\"\"\n",
        "    for i in bill_cols:\n",
        "        if i not in twdb_col:\n",
        "            col_miss.append(i)\"\"\"\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", 0, '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "      \n",
        "def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "    t_col = type_dates.lower()\n",
        "    if format == 1:\n",
        "        type_date = \"%d/%m/%Y\"\n",
        "    else:\n",
        "        type_date = \"%d-%m-%Y\"\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Refactorar esse codigo para receber todas as colunas num dic\n",
        "# Sendo as keys=columns e values= picklist for each column\n",
        "def check_date_columns(df, df_index,status_col,columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "\n",
        "def check_amounts(df_check, df_index, status_col, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    filtered = df_check[[df_index, status_col]]\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        df_new = pd.merge(df_new, filtered, how='left', left_on='sites', right_on=tw_index)\n",
        "        df_new = df_new[['sites', status_col]]\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "        \n",
        "def check_picklist(df,df_index,df_status, df_cols, picklist_dict):\n",
        "\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        columns = [i.lower() for i in picklist_dict.keys()]\n",
        "        for column in set(columns): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            #print(value)\n",
        "            if not value in set(picklist_dict[column]) or pd.isnull(value):\n",
        "                #print(set(picklist_dict[column]))\n",
        "\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "                else:\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "\n",
        "    #df = df_errors.dropna()   \n",
        "    df_errors = df_errors.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    if len(df_errors)>0:\n",
        "        df = df[[df_index, df_status]]\n",
        "        df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "        df_errors = df_errors.reset_index()\n",
        "    else:\n",
        "        print('\\nNo one Picklist Error Founded!\\n')\n",
        "    return df_errors\n",
        "\n",
        "def check_picklist_3(df,df_index,df_status,picklist_dict):\n",
        "    import json\n",
        "    log = {}\n",
        "\n",
        "    #picklist_dict={'Categorization by Transmission Sys':['Long-Term Mobile Sites','Macro','Non-Enterprise DAS','Repeater Sites','Public DAS Sites','Transmission']}\n",
        "    for column in picklist_dict:\n",
        "        df_aux=df.copy()\n",
        "        #print(type(picklist_dict[column]))\n",
        "        new=df_aux[column].isin(picklist_dict[column])\n",
        "        #print(new)\n",
        "        indexes=df.index[new == False].tolist()\n",
        "        if column not in log:log[column]=[]\n",
        "        log[column]=log[column]+indexes\n",
        "    newDict = {}\n",
        "    #print(log)\n",
        "    #print(json.dumps(picklist_dict,indent=2))\n",
        "    for key,value in log.items():\n",
        "      for val in value:\n",
        "          ID=df.iloc[val][df_index]\n",
        "          if ID in newDict:\n",
        "              v=f'Incorret picklist value: {df.iloc[val][key]}'\n",
        "              if df.iloc[val][key]!=df.iloc[val][key] or pd.isnull(df.iloc[val][key]) or  pd.isna(df.iloc[val][key]) or df.iloc[val][key]=='' or df.iloc[val][key]=='nan':\n",
        "                   v='Blank Value'\n",
        "              newDict[ID][key]=v\n",
        "          else:\n",
        "              v=f'Incorret picklist value: {df.iloc[val][key]}'\n",
        "              if df.iloc[val][key]!=df.iloc[val][key] or pd.isnull(df.iloc[val][key]) or  pd.isna(df.iloc[val][key]) or df.iloc[val][key]=='' or df.iloc[val][key]=='nan':\n",
        "                   v='Blank Value'\n",
        "              newDict[ID] = {'status': df.iloc[val][df_status],key:v}\n",
        "    #print(newDict)\n",
        "    logs = pd.DataFrame.from_dict(newDict, orient='index')\n",
        "    logs.index.name='Site'\n",
        "    logs = logs.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    return logs\n",
        "\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [i for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uis = [i for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index] if str(i) not in uip_list]\n",
        "    bts_out_uis = pd.DataFrame(bts_out_uis, columns=['Bts_Sites_Out_UIS_File'])\n",
        "    bts_out_uis = pd.merge(bts_out_uis, filtered, how='left', left_on=['Bts_Sites_Out_UIS_File'], right_on=tw_index)\n",
        "    bts_out_uis = bts_out_uis[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "    if not (new_sites.empty or bts_out_uis.empty or df_site_bts.empty):\n",
        "        return new_sites, bts_out_uis, df_site_bts[[tw_index,status_col, bts_col, bill_col]]\n",
        "        \n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, status_col, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    filtered = df_tw[[tw_index, status_col]]\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    if not df.empty:\n",
        "        df = pd.merge(df, filtered, how='left', left_on=['New Sites'], right_on=tw_index)\n",
        "        df = df[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "        return df\n",
        "    else: \n",
        "        print('\\nNo errors founded!\\n')\n",
        "\n",
        "def check_wip(df_tw,tw_index,tw_status, tw_wip, tw_bts, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[tw_wip]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[tw_wip]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index,tw_status,tw_wip, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    if not(len(wip_out_tw_list)==0 or tw_wip_site_bts_flagged.empty):\n",
        "        return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "\n",
        "def check_decommissioned(df,df_index,status_col, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[df_index,status_col, decom_col, doer_col]]\n",
        "    else:\n",
        "        print('\\nNo errors Founded!\\n')\n",
        "    \n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    df_tw = df_tw[df_tw[status_col]==status][[tw_index, status_col, date_col]] \n",
        "    #print(df_tw)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[df_tw[date_col].astype('datetime64[ns]') < current_date]\n",
        "            return filtered\n",
        "        else:\n",
        "            #bill_dates = pd.to_datetime(df_tw[date_col], errors==coerrce)\n",
        "            filtered = df_tw[df_tw[date_col]=='']\n",
        "            return filtered\n",
        "    else:\n",
        "        print('\\nNo errors founded!\\n')\n",
        "\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col,bill_col, status_col, dt_format):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date, format=dt_format)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        df_tw[bill_col] = pd.to_datetime(df_tw[bill_col],errors='coerce', format=dt_format)\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)&\\\n",
        "                         (df_tw[bill_col].astype('datetime64[ns]') < current_date)]\n",
        "        return filtered[[tw_index, status_col, bill_col, doer_col]]  \n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "                                                              \n",
        "def check_mom_bts(df_tw, tw_index,status_col, tw_col, df_msa, msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index,status_col, tw_col]]    \n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "\n",
        "def check_lc_ta_dates(df,tw_index,status_col, start_date,end_date):\n",
        "    filtered = df[df[start_date] > df[end_date]]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index,status_col, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "    t1 = ['pt', 'de', 'cz', 'ie', 'es', 'ro', 'hu']\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    if country.lower() in t1:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIS File!'])\n",
        "        in_service_uip_sites = pd.merge(in_service_uip_sites, filtered, how='left', left_on='Site In service out of UIS File!',\\\n",
        "                                        right_on=tw_index)\n",
        "        in_service_uip_sites = in_service_uip_sites[['Site In service out of UIS File!', status_tw_col]]\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        if decom_col != \"\":\n",
        "            tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "            decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "            decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "        \n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "            bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIPS File', status_tw_col]]\n",
        "            if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "                return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "        else:\n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "            bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "            if not (in_service_uip_sites.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "                return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIS File!'])\n",
        "        in_service_uip_sites = pd.merge(in_service_uip_sites, filtered, how='left', left_on='Site In service out of UIS File!',\\\n",
        "                                        right_on=tw_index)\n",
        "        in_service_uip_sites = in_service_uip_sites[['Site In service out of UIS File!', status_tw_col]]\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "        decomiss_sites_in_uip = []\n",
        "        if set(tw_decomiss).intersection(uip_sites):\n",
        "            decomiss_sites_in_uip = [i for i in tw_decomiss if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(decomiss_sites_in_uip, columns=['Decomissioned Site in UIS File'])\n",
        "        decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip = [i for i in df_uip['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Yes'][tw_index]\n",
        "        critical = []\n",
        "        if set(uip).intersection(bts_tw_critical):\n",
        "            critical = [i for i in bts_tw_critical if i not in uip]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% out in UIS File'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "        if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "            return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "        #lista = []\n",
        "\n",
        "        df.fillna(\"\", inplace=True)\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df    \n",
        "    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    \n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names).fillna('')\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names ).fillna('')\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols).fillna('')\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    if not df_commercial_diffs.empty:\n",
        "        return df_commercial_diffs\n",
        "    else:\n",
        "        print('\\nNo errors founded!')\n",
        "\n",
        "def check_diffs_v2(path_current, path_last, cols_order, type_file='Excel', sheet='Commercial'):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def highlight_diff(data, color='yellow'):\n",
        "        attr = 'background-color: {}'.format(color)\n",
        "        other = data.xs('Current', axis='columns', level=-1)\n",
        "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                            index=data.index, columns=data.columns)\n",
        "    type_file = type_file.lower()\n",
        "    if type_file =='excel':\n",
        "        _actual = pd.read_excel(path_current,sheet_name=sheet).fillna('')\n",
        "\n",
        "        _before = pd.read_excel(path_last,sheet_name=sheet).fillna('')\n",
        "\n",
        "        df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "        df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "        #df_final.style.apply(highlight_diff, axis=None)\n",
        "        if not df_final.empty:\n",
        "            return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "        else:\n",
        "            print('\\nNo differences Founded!\\n')\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() \n",
        "\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, old_name, new_name, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        df.columns = lower_str(list(df.columns))\n",
        "        df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    \n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD,engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW,engine='python').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = fit_df(path_OLD,bill_cols)\n",
        "        df_NEW = fit_df(path_NEW,bill_cols)\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "        status_1 = f'{status_col}_current'\n",
        "        status_2 = f'{status_col}_before'\n",
        "        new_copy = new_copy.set_index('sites')\n",
        "        new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "def find_diffs_between_files_1(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', dates_cols=[], status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col] \n",
        "            #df.columns = lower_str(list(df.columns))\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "        t_col = type_dates.lower()\n",
        "        if format == 1:\n",
        "            type_date = \"%d/%m/%Y\"\n",
        "        else:\n",
        "            type_date = \"%d-%m-%Y\"\n",
        "        for column in lower_str(columns):\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,engine='python', encoding='latin1').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD[bill_cols]\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, engine='python', encoding='latin1').fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW[bill_cols]\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD) header=0, names=cols_old,\n",
        "        df_OLD = pd.read_csv(path_OLD, engine='python').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols, 1, 'normal')\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfU2Gd5lQ0mp"
      },
      "source": [
        "Looking for differences betwwnn older and newest files in towerdb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DyyTHFEX2L6"
      },
      "source": [
        "Tem colunas com mesmo nome e está gerando erros na hora de correr o script por toda a planilha."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB5ABfXGWQCn"
      },
      "source": [
        "hu_bill_cols = [\"Active / not active\",\\\n",
        "    \"Confirmed Skylon scope\",\\\n",
        "    \"Code\",\\\n",
        "    \"Categorization by Transmission Sys\",\\\n",
        "    \"Additional information \\x96 room configuration (including container information)\",\\\n",
        "    \"Active Sharing Arrangements involving the Operator\",\\\n",
        "    \"Critical site\",\\\n",
        "    \"Strategic site\",\\\n",
        "    \"Telenor tenant\",\\\n",
        "    \"Telekom tenant\",\\\n",
        "    \"Power supply for tenants\",\\\n",
        "    \"Energy category\",\\\n",
        "    'Ready For Active Installation (\"RFAI\") date',\\\n",
        "    \"Wip_Site\",\\\n",
        "    \"Bts_Site\",\\\n",
        "    \"Strategic_Site_Bucket\",\\\n",
        "    \"Critical_Site_Beyond_10\",\\\n",
        "    \"Subsequent_Sharing_Arrangement\",\\\n",
        "    \"First_Active_Sharing_Start_Date\",\\\n",
        "    \"First_Active_Sharing_End_Date\",\\\n",
        "    \"Active Sharing Deployment Types\",\\\n",
        "    \"Sites_As_Metered_Estimated\",\\\n",
        "    \"Date of equipment removal\"]\n",
        "\n",
        "path_tw = '/content/TowerDB_Hungary_20210731.csv'\n",
        "path_msa = '/content/TowerDB_Hungary_20210630.csv'   \n",
        "tw_save = '/content/TW_HU'\n",
        "new_name = 'TowerDB_Hungary_20210731.csv'\n",
        "old_name = 'TowerDB_Hungary_20210630.csv'  \n",
        "tw_dates = ['ready for active installation (\"rfai\") date',\"First_Active_Sharing_Start_Date\",\\\n",
        "            \"First_Active_Sharing_End_Date\", \"Date of equipment removal\"]\n",
        "find_diffs_between_files_1(path_msa, path_tw, 'Code',hu_bill_cols, tw_save, old_name, new_name, type_file='csv',status_col='active / not active',kind='tw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjIt6YFKQ2HV"
      },
      "source": [
        "Doing all checks in Towerdb File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aamb1an5yEhc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "6cc02fd5-9fb6-49a0-fe4b-d5c8694549b7"
      },
      "source": [
        "path_msa = '/content/TowerDB_Hungary_20210630.csv'\n",
        "msa = pd.read_csv(path_msa, encoding='latin')\n",
        "\n",
        "msa_cols = lower_str(list(msa.columns))\n",
        "msa.columns = msa_cols\n",
        "\n",
        "path_tw = '/content/TowerDB_Hungary_20210731.csv'\n",
        "towerdb = pd.read_csv(path_tw, encoding='latin').fillna('')\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "\n",
        "\"\"\"Check Columns Received\"\"\"             \n",
        "df_cols = check_columns_received(towerdb, msa_cols)\n",
        "df_cols\n",
        "#No columns missing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column(s) Missing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vertical infrastructure \\n(ex: torre, suportes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>site type\\n(ex. rtt; gbt)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>technology_vod\\n(ex: umts,gsm,lte)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fibre/microwave\\n(fo/mw)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>power supply\\n(ex: ac power, dc power)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Column(s) Missing\n",
              "0  vertical infrastructure \\n(ex: torre, suportes...\n",
              "1                          site type\\n(ex. rtt; gbt)\n",
              "2                 technology_vod\\n(ex: umts,gsm,lte)\n",
              "3                           fibre/microwave\\n(fo/mw)\n",
              "4             power supply\\n(ex: ac power, dc power)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRY_-bVE0AZ0"
      },
      "source": [
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = 'code'\n",
        "tw_doer = 'date of equipment removal'\n",
        "tw_status = 'active / not active'\n",
        "tw_bts = 'bts_site'\n",
        "tw_bill = 'ready for active installation (\"rfai\") date'\n",
        "tw_wip = 'wip_site'\n",
        "#tw_decom = 'Decommissioned sites'\n",
        "tw_critical = 'critical site'\n",
        "\n",
        "\n",
        "msa_index ='code'\n",
        "msa_doer = 'date of equipment removal'\n",
        "msa_status = 'active / not active'\n",
        "msa_bts = 'bts_site'\n",
        "msa_bill = 'ready for active installation (\"rfai\") date'\n",
        "msa_wip = 'wip_site'\n",
        "#msa_decom = 'Decommissioned sites'\n",
        "msa_critical = 'critical site'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuTDCDSVAP9P"
      },
      "source": [
        "First Check - Dates Formats (dd/mm/YYYY)\n",
        "\n",
        "Columns: Date of equipment removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzPJTGcSANf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "4b73333f-156d-4352-f19f-55a66a00503a"
      },
      "source": [
        "\"\"\"You need to convert all values in cols for string format to check\"\"\"\n",
        "\"\"\"CSV FIles doesn't need convert, only fill na values\"\"\"\n",
        "# Columns to functions\n",
        "dates_doer = [tw_index, tw_doer]\n",
        "dates_bill = [tw_index, tw_bill]\n",
        "#Columns to parser\n",
        "doer=[tw_doer]\n",
        "bill=[tw_bill]\n",
        "\n",
        "\n",
        "#date_parser(towerdb, bill, \"%d/%m/%Y\", 'no')\n",
        "#date_parser(towerdb, doer, \"%d/%m/%Y\", 'mix')\n",
        "\n",
        "actives_1 = towerdb[towerdb[tw_status]=='In Service']\n",
        "no_actives_1 = towerdb[~(towerdb[tw_status]=='In Service')]\n",
        "\n",
        "#Checking columns for errors\n",
        "actives_dates_errors = check_date_columns(actives_1, tw_index,tw_status, dates_bill, 2) \n",
        "# Actives sites with blank billing trigger date\n",
        "no_actives_dates_errors = check_date_columns(no_actives_1, tw_index, tw_status, dates_doer, 2) \n",
        "no_actives_dates_errors"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5d9cd7660528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\"\"\"CSV FIles doesn't need convert, only fill na values\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Columns to functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdates_doer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtw_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw_doer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdates_bill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtw_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw_bill\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Columns to parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tw_index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "fRkWMoSNduTM",
        "outputId": "a79d1144-5d07-4d2e-d091-d7751d595a02"
      },
      "source": [
        "#Checking columns for errors\n",
        "actives_dates_errors\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>active / not active</th>\n",
              "      <th>ready for active installation (\"rfai\") date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1004</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1005</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1007</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017</th>\n",
              "      <td>RGU0349</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018</th>\n",
              "      <td>RGU0352</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019</th>\n",
              "      <td>RGU0453</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>RU0094</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021</th>\n",
              "      <td>RU0138</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2022 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         code active / not active ready for active installation (\"rfai\") date\n",
              "0        1001          In Service                                 Blank Value\n",
              "1        1002          In Service                                 Blank Value\n",
              "2        1004          In Service                                 Blank Value\n",
              "3        1005          In Service                                 Blank Value\n",
              "4        1007          In Service                                 Blank Value\n",
              "...       ...                 ...                                         ...\n",
              "2017  RGU0349          In Service                                 Blank Value\n",
              "2018  RGU0352          In Service                                 Blank Value\n",
              "2019  RGU0453          In Service                                 Blank Value\n",
              "2020   RU0094          In Service                                 Blank Value\n",
              "2021   RU0138          In Service                                 Blank Value\n",
              "\n",
              "[2022 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAhMeodaA7Sj"
      },
      "source": [
        "Second Check - TW Amount value General (xxx.xx)\n",
        "\n",
        "Column(s): ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBa0a7lQBBUC"
      },
      "source": [
        "amount_cols = [tw_index, \"\"\"???\"\"\"]\n",
        "df_amount_errors = check_amounts(actives, tw_index, amount_cols, 1)\n",
        "#No one error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFmokgkfB6pr"
      },
      "source": [
        "Thirth - Check Picklist values All sites\n",
        "\n",
        "Do this check in all sites\n",
        "\n",
        "Check the picklist for each case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzn0nNJGLbsr"
      },
      "source": [
        "Check - Remove \"N/A\", \"0\" or \"-\" values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "826YA4wtGsyR"
      },
      "source": [
        "bill_cols = ['confirmed skylon scope','code','categorization by transmission sys', 'categorization by site type',\\\n",
        "             'additional information  room configuration (including container information)', 'type of air cooling', \\\n",
        "             'active sharing arrangements involving the operator', 'critical site','strategic site','telenor tenant','telekom tenant',\\\n",
        "             'power supply for tenants','energy category', 'ready for active installation (\"rfai\") date', 'wip_site','bts_site','strategic_site_bucket',\\\n",
        "             'critical_site_beyond_10','subsequent_sharing_arrangement','first_active_sharing_start_date','first_active_sharing_end_date',\\\n",
        "             'active sharing deployment types','sites_as_metered_estimated']\n",
        "towerdb = replace_values(towerdb, bill_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4y5qNGSLf6z"
      },
      "source": [
        "Thirth - Check Picklist values All sites\n",
        "\n",
        "Do this check in all sites\n",
        "\n",
        "Check the picklist for each case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbwAq3ClCDqx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "dbbb7526-fe6b-46ab-8539-fdd7a093a38b"
      },
      "source": [
        "picklist_tw_general = {\n",
        "    'categorization by transmission sys': ['Macro', 'Macro+DAS', 'Public DAS','Transmission']\n",
        "}\n",
        "pick_col_general = ['code', 'categorization by transmission sys']\n",
        "\n",
        "df_general_pick = check_picklist(towerdb, tw_index,tw_status, pick_col_general, picklist_tw_general)\n",
        "df_general_pick"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No one Picklist Error Founded!\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:272: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsMno4c2OdqL"
      },
      "source": [
        "Check Picklist and dates formats for In service sites\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CyoPjLhIv88"
      },
      "source": [
        "picklis_dict = {\n",
        "    'categorization by transmission sys': ['Macro', 'Macro+DAS', 'Public DAS','Transmission'],\n",
        "    'categorization by site type': ['DAS passive','GBT','RTT', 'RTT+DAS passive'],\n",
        "    'type of air cooling' : ['Yes', 'No'],\n",
        "    'critical site': ['Yes', 'No'],\n",
        "    'strategic site': ['Yes', 'No'],\n",
        "    'power supply for tenants': ['AC', 'DC', 'no power'],\n",
        "    'bts_site': ['Yes', 'No'],\n",
        "    'strategic site': ['Yes', 'No'],\n",
        "    'critical site': ['Yes', 'No'],\n",
        "    'strategic_site_bucket': ['Non Strategic'],\n",
        "    'critical_site_beyond_10':['Within 10%','Non Critical'],\n",
        "    'wip_site': ['Yes', 'No'],\n",
        "    'sites_as_metered_estimated': ['Estimated Model','Metered Model']\n",
        "}\n",
        "\n",
        "picklist_cols = ['code','categorization by transmission sys', 'categorization by site type', 'type of air cooling',\\\n",
        "                 'critical site','strategic site','power supply for tenants', 'wip_site','bts_site',\\\n",
        "                 'strategic_site_bucket','critical_site_beyond_10','sites_as_metered_estimated','strategic site','critical site']\n",
        "\n",
        "actives_2 = towerdb[towerdb[tw_status]=='In Service']\n",
        "df_in_service_picklist = check_picklist_3(actives_2,tw_index,tw_status, picklis_dict)\n",
        " #= check_picklist(actives, tw_index,tw_status, picklist_cols, picklis_dict)\n",
        "df_in_service_picklist\n",
        "#muitos errors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAwey2bGcDER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "6d73313f-e349-465e-a5cb-acfeaa23e941"
      },
      "source": [
        "def check_date_columns(df, df_index,status_col,columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            #df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "\n",
        "actives_1 = towerdb[towerdb[tw_status]=='In Service']\n",
        "date_cols = ['code','infrastructure ready (existing)/ to be ready (new)', 'ready for active installation (\"rfai\") date', 'first_active_sharing_start_date','first_active_sharing_end_date']\n",
        "start_dates = ['infrastructure ready (existing)/ to be ready (new)', 'ready for active installation (\"rfai\") date', 'first_active_sharing_start_date','first_active_sharing_end_date']\n",
        "\n",
        "#date_parser(actives, start_dates, \"%d/%m/%Y\", 'no')\n",
        "df_in_service_dates = check_date_columns(actives_1, tw_index,tw_status,date_cols, 2)\n",
        "df_in_service_dates\n",
        "#Muitos Campos em branco"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>ready for active installation (\"rfai\") date</th>\n",
              "      <th>first_active_sharing_start_date</th>\n",
              "      <th>first_active_sharing_end_date</th>\n",
              "      <th>active / not active</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1004</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1005</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1007</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2043</th>\n",
              "      <td>RGU0352</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2044</th>\n",
              "      <td>RGU0453</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2045</th>\n",
              "      <td>RU0094</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2046</th>\n",
              "      <td>RU0138</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2047</th>\n",
              "      <td>S0113</td>\n",
              "      <td>Ok</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2048 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         code  ... active / not active\n",
              "0        1001  ...          In Service\n",
              "1        1002  ...          In Service\n",
              "2        1004  ...          In Service\n",
              "3        1005  ...          In Service\n",
              "4        1007  ...          In Service\n",
              "...       ...  ...                 ...\n",
              "2043  RGU0352  ...          In Service\n",
              "2044  RGU0453  ...          In Service\n",
              "2045   RU0094  ...          In Service\n",
              "2046   RU0138  ...          In Service\n",
              "2047    S0113  ...          In Service\n",
              "\n",
              "[2048 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TNxDZU4fp0J"
      },
      "source": [
        "Fifth Check BTS Flagged(Billing Trigger and Commercial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pd0kaUofmxX"
      },
      "source": [
        "status = 'Yes'\n",
        "df_bts_flagged = check_tw_bill_doer(actives, tw_index,tw_bill, tw_bts, status, 'bill')\n",
        "df_bts_flagged\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AEXpWOvfzLo"
      },
      "source": [
        "Check MoM Sites (BTS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-gggsUqgB6-"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "path_uip = '/content/UserInput_Hungary_20210731.xlsx'\n",
        "uip_names = ['Site_ID','BTS site applicable charge (Annual)',\\\n",
        "             'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip = pd.read_excel(path_uip ,sheet_name='SiteLevel',usecols=[0,1,2],skiprows=2)\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [str(i) for i in msa[msa_index]]\n",
        "tw_sites = [str(i) for i in towerdb[tw_index]]\n",
        "uip_sites = [str(i) for i in uip['Site_ID']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfestas6f6ES"
      },
      "source": [
        "df_mom_bts = check_mom_bts(actives, tw_index, tw_bts, msa, msa_index, msa_bts)\n",
        "df_mom_bts\n",
        "# No one error df_mom_bts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U5IfkxQgsXF"
      },
      "source": [
        "Check UIP sites with towerDB\n",
        "Some countries doesn't have decommissioned column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye6Vdrpyg7tJ"
      },
      "source": [
        "in_service_uip_sites, bts_sites_out_uip, critical = check_uip_tw(towerdb,tw_index, tw_status, '', tw_bts,tw_critical,\\\n",
        "                                                                 uip, uip_sites, 'hu')\n",
        "#no errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFe-T0eRkaLU"
      },
      "source": [
        "Commercial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XNsmrtTkbe8"
      },
      "source": [
        "path_before = '/content/UserInput_Hungary_20210630.xlsx'\n",
        "names = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', 'Input_Value',\\\n",
        "        'Description/Instruction', 'Frequency of Update']\n",
        "merge_cols = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', \\\n",
        "              'Description/Instruction', 'Frequency of Update']\n",
        "cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Data_Type','Input_Value_actual',\\\n",
        "                'Input_Value_before','Equal Values','Description/Instruction','Frequency of Update']\n",
        "col_replaced = ['Input_Value']\n",
        "df_commercial_diffs = check_commercial(path_uip, path_before,col_replaced, names, merge_cols, cols_ordered)\n",
        "#df_commercial_diffs = check_commercial(path_uip, path_before, replace_values, names, merge_cols, cols_ordered)\n",
        "df_commercial_diffs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzYvxx5wnuh8"
      },
      "source": [
        "Excel Log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYdE0UzCnq5h"
      },
      "source": [
        "df_list = [df_in_service_dates, df_in_service_picklist]\n",
        "sheetnames = ['In services dates errrors', 'In servive picklist errors']\n",
        "\n",
        "path = '/content/HU_towerdb_errors.xlsx'\n",
        "general_log_erros(df_list, sheetnames, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l3yQMVO_oGt"
      },
      "source": [
        "towerdb.loc[towerdb[tw_index]=='1001'][['ready for active installation (\"rfai\") date', 'first_active_sharing_start_date','first_active_sharing_end_date']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymaR9uoioRBC"
      },
      "source": [
        "TA Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rmiiDZxoLr0"
      },
      "source": [
        "pathta = '/content/TA_Input_Hungary_20210731.csv'\n",
        "\n",
        "ta = pd.read_csv(pathta,  encoding='latin')\n",
        "\n",
        "ta_leases = ['Base fee (@ signing) in HUF']\n",
        "ta = replace_values(ta,ta_leases, '')\n",
        "\n",
        "ta_cols = ['SiteID', 'Base fee (@ signing) in HUF']  \n",
        "df_ta_amount = check_amounts(ta, 'SiteID', ta_cols)\n",
        "print(df_ta_amount)\n",
        "# No errors\n",
        "\n",
        "df_ta_dates = check_lc_ta_dates(ta,'SiteID', 'Starting date', 'Expiring date (raw data)')\n",
        "#No erros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FAYrnmuRFOD"
      },
      "source": [
        "Looking for differences between newest and oldest TA file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLCpvBdkRyut"
      },
      "source": [
        "pathta = '/content/TA_Input_Hungary_20210731.csv'\n",
        "pathold = '/content/TA_Input_Hungary_20210630.csv'\n",
        "ta_save = '/content/HU_TA'\n",
        "ta_bill = ['SiteID', 'MNO classification','Base fee (@ signing) in HUF', 'Starting date', 'Expiring date (raw data)']\n",
        "find_diffs_between_files(pathold, pathta, 'SiteID', ta_bill,'', ta_save,'csv', 'ta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcStV100sgVb"
      },
      "source": [
        "LC Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vM0_R8z3ZGg"
      },
      "source": [
        "ta = pd.read_csv(pathta,  encoding='latin')\n",
        "ta.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uiUvmkQsyar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ce51c4-d4fd-431d-d351-6ae886876093"
      },
      "source": [
        " #separador virgulas\n",
        "pathlc = '/content/LC_Input_Hungary_20210731.csv'\n",
        "\n",
        "lc = pd.read_csv(pathlc,  encoding='latin')\n",
        "leases = ['Current annual lease fees in HUF']\n",
        "lc = replace_values(lc,leases, '')\n",
        "\n",
        "lc_cols = ['Code', 'Current annual lease fees in HUF']  \n",
        "\n",
        "df_lc_amount = check_amounts(lc, 'Code', lc_cols)\n",
        "# No errors\n",
        "\n",
        "df_lc_dates = check_lc_ta_dates(lc,'Code', 'Starting date', 'Expiring date (raw data)')\n",
        "#No erros"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:217: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxilQu2gBK8l"
      },
      "source": [
        "lc.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nIGLPwSAuFP",
        "outputId": "ad3a19ba-242a-484e-cf4b-18e01645a776"
      },
      "source": [
        "pathlc = '/content/LC_Input_Hungary_20210731.csv'\n",
        "patholdlc = '/content/LC_Input_Hungary_20210630.csv'\n",
        "lc_save = '/content/HU_LC'\n",
        "lc_bill = ['Code', 'Current annual lease fees in HUF','Starting date', 'Expiring date (raw data)']\n",
        "find_diffs_between_files(patholdlc, pathlc, 'Code', lc_bill,'', lc_save,'csv', 'ta')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['5028', 'S0113']\n",
            "Dropped Rows: []\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjqV8mwnuN1L",
        "outputId": "be52476c-1b5e-4212-84c2-ee8a452df280"
      },
      "source": [
        "p = re.compile(r'^\\d+(\\.\\d*[1-9])?$')\n",
        "v = '10200,01'\n",
        "if not str(v).__contains__(','):\n",
        "#if p.match(v) == None:\n",
        "    print(\"Erro\")\n",
        "else:\n",
        "    print(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10200,01\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}