{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hu_validations.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOStxjvbR64B+Znmc/eikFg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sPiDTWZxtiO"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook\n",
        "\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index):\n",
        "    \"\"\"\n",
        "    Params:\\n\n",
        "    path: parth of file in the computer.\\n\n",
        "    n_skiprows: Number of rows to delete in the original file,.\\n\n",
        "    columns_to_convert: Columns to convert the data general type. \\n\n",
        "    n_skipcolumn: Columns to skip in the original file. \\n\n",
        "    endrow = pass 0 to read everything, 1 to count entire\n",
        "    columns_order: List of columns names in specific order to pass in the engine.\\n\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    \"\"\"cont = 0\n",
        "    for i in df.iloc[:,site_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[:cont, :]\n",
        "    #df.columns = columns_order\"\"\"\n",
        "    \n",
        "    # convert intery columns to integer \n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].fillna(0)\n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].astype('int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def check_df(df, error_msg):\n",
        "    if df == None or df.empty:\n",
        "        return f'{error_msg}'\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def count_duplicates(lista):\n",
        "    count_dict = {}\n",
        "    for entry in lista:\n",
        "        if entry in count_dict.keys():\n",
        "            count_dict[entry] += 1\n",
        "        else:\n",
        "            count_dict[entry] = 1\n",
        "    \n",
        "    duplicates = {}\n",
        "    for k, v in count_dict.items():\n",
        "        if v > 1:\n",
        "            duplicates[k] = v\n",
        "    return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "def defining_df(df, column_range, number_col):\n",
        "    cont = 0\n",
        "    for i in df.iloc[:,number_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[0:cont, :]\n",
        "    return df\n",
        "\n",
        "#old version\n",
        "def check_columns(table, output_columns):\n",
        "    \"\"\"\n",
        "    Check the total of number of missing columns and the missing columns in passed table.\\n\n",
        "\n",
        "    Params:\\n\n",
        "    table: contain the columns to be check.\\n\n",
        "    output_columns: columns structure at the final file. \n",
        "\n",
        "    Returns:\\n\n",
        "    Number of missing columns and a list that contains the name os missing columns.\n",
        "    \"\"\"\n",
        "    \"\"\"\"\n",
        "    countries = ['DE':{'towerDB':[],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':[]}\\,\n",
        "                 'HU',\n",
        "                 'IE',\n",
        "                 'RO',\n",
        "                 'PT',\n",
        "                 'ES',\n",
        "                 'CZ': {'towerDB':[\"Code (Duplicate)\",\"Site Status\",\"VF - In scope / out of scope (Generalised scoping)\",\"Site in Skylon scope Actual (From Site List Sheet )\",\"Legacy Site Code(Duplicate)\",\"TIMS Site Code\",\"Legacy Site Code\",\"Site Name\",\"Macro Region\",\"Province\",\"Municipality\",\"Inhabitants\",\"Address\",\"Ground Register\",\"Altitude\",\"Latitude\",\"Longitude\",\"Categorization by Inhabitants\",\"Categorization by Transmission Sys\",\"Categorization by Site Type\",\"Categorization by Transmission Sys (subcluster)\",\"Other internal Categorization 1 (Identify ACQ Sites)\",\"Other internal Categorization 2 Energy provider (Eon/ LL)\",\"DAS+Macro\",\"DAS (Yes/ No)\",\"DAS Ownership (Complete/ Partial/ 3rd Party)\",\"Active/ Passive DAS\",\"# of remote units/ radiating points\",\"Type of Structure\",\"Distance highest antenna to ground level\",\"GBT Tower height\",\"POD ID\",\"Energy Consumption LTM (kwh)\",\"Annual Energy cost LTM (Euros)\",\"Infrastructure ready (existing)/ to be ready (new)\",\"Infrastructure to be dismantled by\",\"Radio equipments to be deactivated by\",\"Infrastructure to be shared by\",\"Technology VOD\",\"Fibre / Microwave\",\"Vertical passive structure owner\",\"Room configuration (detailed)\",\"Shelter passive structure ownership\",\"Type of Air Conditioning\",\"Number of cabinets (Full Capacity)\",\"Number of Antenna (Full Capacity)\",\"Number of MW (Full Capacity)\",\"Counterpart\",\"# of Lease Contracts\",\"Current annual lease fees \",\"Current other fees (Maintenance)\",\"Current other fees\",\"(Average) residual duration - Lease contract\",\"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\"# of Tenants Agreements\",\"Current Total Annual Hosting Fees\",\"Tenant (name/ID) MNO1 (Česká telekomunikační infrastruktura a.s.)\",\"Annual Fee per Tenant MNO1\",\"Annual Energy Fee MNO1\",\"Annual Maintenance Fee MNO1\",\"Other Services Fee MNO1\",\"Residual duration MNO1 (Years)\",\"Tenant (name/ID) MNO2 (T-Mobile Czech Republic a.s.)\",\"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\"Annual Maintenance Fee MNO2\",\"Other Services Fee MNO2\",\"Residual duration MNO2 (days)\",\"Tenant (name/ID) MNO3\",\"Annual Fee per Tenant MNO3\",\"Annual Energy Fee MNO3\",\"Annual Maintenance Fee MNO3\",\"Other Services Fee MNO3\",\"Residual duration MNO3\",\"# of OTMOs\",\"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\"Annual Maintenance Fee OTMOs\",\"Other Services Fee OTMOs\",\"Average residual duration (days)\",\"Check\",\"Strategic Macro Sites\",\"Critical Sites\",\"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\"Transmission Sites\",\"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\"VF-CZ Demerger phase\",\"EVO Location [FAR Site ID] \",\"Billing Trigger date \",\\\n",
        "                 \"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\"Wip_Site\",\"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', 'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', 'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index','Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', 'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', 'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month','Updated Item', 'Comment', 'Counterpart_extra_1', 'Counterpart_extra_2', 'x','SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count', 'not_def_1']}\\,\n",
        "                 }\n",
        "                 'GR']\n",
        "                 \"\"\"\n",
        "    total_received = len(table.columns)\n",
        "    number_missing_columns = 0\n",
        "    missing_columns = []\n",
        "    #Counting of missing columns       \n",
        "    #if country in contries:      \n",
        "    for columns in output_columns:\n",
        "        if columns.lower() not in [labels.lower() for labels in table]:\n",
        "            number_missing_columns +=1\n",
        "            missing_columns.append(columns)\n",
        "    \n",
        "    return total_received, number_missing_columns, missing_columns\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    \"\"\"\n",
        "    for i in bill_cols:\n",
        "        if i not in twdb_col:\n",
        "            col_miss.append(i)\"\"\"\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", 0, '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "      \n",
        "def date_parser(df, columns, format, type_dates):\n",
        "    t_col = type_dates.lower()\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Refactorar esse codigo para receber todas as colunas num dic\n",
        "# Sendo as keys=columns e values= picklist for each column\n",
        "def check_date_columns(df, df_index, columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                if date_format.match(df_dates.loc[de_site,de_column]) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "                    else:\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            return df_de\n",
        "        else: \n",
        "            print('No one columns with incorrect date format!')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                if date_format.match(df_dates.loc[de_site,de_column]) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "                    else:\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            return df_de\n",
        "        else: \n",
        "            print('No one columns with incorrect date format!')\n",
        "\n",
        "def check_amounts(df_check, df_index, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "        \n",
        "def check_picklist(df,df_index,df_cols, picklist_dict):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        df:\n",
        "        df_index: \n",
        "        picklist_dict:\n",
        "        nultiIndex: More than one column to check\n",
        "    \"\"\"\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in picklist_dict.keys(): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            col_values = [i.lower() for i in picklist_dict[column]]\n",
        "            if not value.lower() in col_values or pd.isnull(value):\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "                else:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    #df = df_errors.dropna()   \n",
        "    df = df_errors.dropna(how='all', axis=1) \n",
        "    \n",
        "    return df\n",
        "\n",
        "def check_picklist_3(df,df_index, picklist_dict):\n",
        "    log = {}\n",
        "    for column in picklist_dict:\n",
        "        df_aux = df.copy()\n",
        "        new = df_aux[column].isin(picklist_dict[column])\n",
        "        #new = df_aux[column].apply(lambda x: x in picklist_dict[column])\n",
        "        # Aceita somento os valores que não estão na picklist\n",
        "        indexes = df.index[new == False].tolist()\n",
        "        #if len(indexes)>0:\n",
        "        if column not in log:log[column]=[]\n",
        "        log[column]=log[column]+indexes\n",
        "    #print(log.keys())\n",
        "\n",
        "    newDict ={}\n",
        "    df1 = pd.DataFrame()\n",
        "    for key,value in log.items():\n",
        "        for val in value:  \n",
        "            ID=df.iloc[val][df_index]\n",
        "            if ID in newDict:\n",
        "                newDict[ID].append(key)\n",
        "            else:\n",
        "                newDict[ID] = [key]\n",
        "        \n",
        "    logs = pd.DataFrame.from_dict(newDict, orient='index')\n",
        "    return logs\n",
        "\n",
        "#check on air foi trocado pelo check_picklist\n",
        "def check_on_air_sites(df, df_index, df_cols):\n",
        "\n",
        "    df_check = df[df_cols]\n",
        "    df_check['sites'] = df_check[df_index]\n",
        "    df_check = df_check.set_index('sites')\n",
        "    #df_check = replace_values(df_check, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in df_cols: \n",
        "            if df_check.loc[site,df_index] not in df_errors.index:\n",
        "                df_errors.loc[site,df_index] = df_check.loc[site,df_index]\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "            else:\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    df = df_errors[df_errors.iloc[:,1:]]\n",
        "    df = df.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    #df = df_errors.dropna(how='all', axis=1)   \n",
        "    return df\n",
        "\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [str(i) for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uip = [str(i) for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index].sort_values() if i not in uip_list]\n",
        "    bts_out_uip = pd.DataFrame(bts_out_uip, columns=['Bts_Sites_Out_UIP_File'])\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "\n",
        "    return new_sites, bts_out_uip, df_site_bts[[tw_index, bts_col, bill_col]]\n",
        "    \"\"\"Restruturar o script em CZ, DE, PT\"\"\"\n",
        "\n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    return df\n",
        "\n",
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[wip_tw]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, wip_tw, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "    \"\"\"Reestrurar os script em PT, DE, CZ\"\"\"\n",
        "    # Falta os outros países\n",
        "\n",
        "def check_decommissioned(df,df_index, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    return filtered[[df_index, decom_col, doer_col]]\n",
        "    \"\"\"Ajustar para CZ, DE, PT\"\"\"\n",
        "    \n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]') < current_date)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "        else:\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]').empty)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "    \"\"\"Ajustar para CZ, DE, PT, IE\"\"\"\n",
        "\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col, status_col):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)]\n",
        "        return filtered[[tw_index, status_col, doer_col]]  \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "                                                              \n",
        "def check_mom_bts(df_tw, tw_index, tw_col, df_msa,msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    return filtered[[tw_index, tw_col]]         \n",
        "\n",
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "\n",
        "        filtered = df[df[start_date] > df[end_date]]\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "    t1 = ['pt', 'de', 'cz', 'ie', 'es', 'ro', 'hu']\n",
        "    if country.lower() in t1:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        if decom_col != \"\":\n",
        "            tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "            decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "\n",
        "        \n",
        "            return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "        else:\n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "            return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "        decomiss_sites_in_uip = []\n",
        "        if set(tw_decomiss).intersection(uip_sites):\n",
        "            decomiss_sites_in_uip = [i for i in tw_decomiss if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(decomiss_sites_in_uip, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip = [i for i in df_uip['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Yes'][tw_index]\n",
        "        critical = []\n",
        "        if set(uip).intersection(bts_tw_critical):\n",
        "            critical = [i for i in bts_tw_critical if i not in uip]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% out in UIP File'])\n",
        "\n",
        "        return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    return df_commercial_diffs\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQnfRF34Yc7y"
      },
      "source": [
        "path_msa = '/content/TowerDB_Hungary_20210630.csv'\n",
        "msa = pd.read_csv(path_msa, encoding='latin')\n",
        "\n",
        "\"\"\"\n",
        "msa_cols = lower_str(list(msa.columns))\n",
        "msa.columns = msa_cols\n",
        "\n",
        "path_tw = '/content/TowerDB_Hungary_20210731.csv'\n",
        "towerdb = pd.read_csv(path_tw, encoding='latin')\"\"\"\n",
        "list(msa.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aamb1an5yEhc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "ba5786df-6fed-47ac-af29-9697a3417000"
      },
      "source": [
        "path_msa = '/content/TowerDB_Hungary_20210630.csv'\n",
        "msa = pd.read_csv(path_msa, encoding='latin')\n",
        "\n",
        "msa_cols = lower_str(list(msa.columns))\n",
        "msa.columns = msa_cols\n",
        "\n",
        "path_tw = '/content/TowerDB_Hungary_20210731.csv'\n",
        "towerdb = pd.read_csv(path_tw, encoding='latin')\n",
        "towerdb.columns = msa_cols\n",
        "\n",
        "\"\"\"Check Columns Received\"\"\"             \n",
        "df_cols = check_columns_received(towerdb, msa_cols)\n",
        "df_cols\n",
        "#No columns missing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column(s) Missing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Column(s) Missing]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRY_-bVE0AZ0"
      },
      "source": [
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = 'code'\n",
        "tw_doer = 'date of equipment removal'\n",
        "tw_status = 'active / not active'\n",
        "tw_bts = 'bts_site'\n",
        "tw_bill = 'ready for active installation (\"rfai\") date'\n",
        "tw_wip = 'wip_site'\n",
        "#tw_decom = 'Decommissioned sites'\n",
        "tw_critical = 'critical site'\n",
        "\n",
        "\n",
        "msa_index ='code'\n",
        "msa_doer = 'date of equipment removal'\n",
        "msa_status = 'active / not active'\n",
        "msa_bts = 'bts_site'\n",
        "msa_bill = 'ready for active installation (\"rfai\") date'\n",
        "msa_wip = 'wip_site'\n",
        "#msa_decom = 'Decommissioned sites'\n",
        "msa_critical = 'critical site'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuTDCDSVAP9P"
      },
      "source": [
        "First Check - Dates Formats (dd/mm/YYYY)\n",
        "\n",
        "Columns: Date of equipment removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzPJTGcSANf4",
        "outputId": "61c88714-2376-4833-e7b4-24514e09339b"
      },
      "source": [
        "\"\"\"You need to convert all values in cols for string format to check\"\"\"\n",
        "# Columns to functions\n",
        "dates_doer = [tw_index, tw_doer]\n",
        "#Columns to parser\n",
        "doer=[tw_doer]\n",
        "\n",
        "#actives_1 = towerdb[towerdb[tw_status]=='In Service']\n",
        "no_actives_1 = towerdb[~(towerdb[tw_status]=='In Service')]\n",
        "\n",
        "#date_parser(towerdb, bill, \"%d/%m/%Y\", 'no')\n",
        "date_parser(no_actives_1, doer, \"%d/%m/%Y\", '')\n",
        "\n",
        "\n",
        "#Checking columns for errors\n",
        "# actives_dates_errors = check_date_columns(actives_1, tw_index, dates_bill, 2) \n",
        "# Actives sites with blank billing trigger date\n",
        "no_actives_dates_errors = check_date_columns(no_actives_1, tw_index, dates_doer, 2) \n",
        "no_actives_dates_errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No one columns with incorrect date format!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAhMeodaA7Sj"
      },
      "source": [
        "Second Check - TW Amount value General (xxx.xx)\n",
        "\n",
        "Column(s): ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBa0a7lQBBUC"
      },
      "source": [
        "amount_cols = [tw_index, \"\"\"???\"\"\"]\n",
        "df_amount_errors = check_amounts(actives, tw_index, amount_cols, 1)\n",
        "#No one error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFmokgkfB6pr"
      },
      "source": [
        "Thirth - Check Picklist values All sites\n",
        "\n",
        "Do this check in all sites\n",
        "\n",
        "Check the picklist for each case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzn0nNJGLbsr"
      },
      "source": [
        "Check - Remove \"N/A\", \"0\" or \"-\" values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "826YA4wtGsyR"
      },
      "source": [
        "bill_cols = ['confirmed skylon scope','code','categorization by transmission sys', 'categorization by site type',\\\n",
        "             'additional information  room configuration (including container information)', 'type of air cooling', \\\n",
        "             'active sharing arrangements involving the operator', 'critical site','strategic site','telenor tenant','telekom tenant',\\\n",
        "             'power supply for tenants','energy category', 'ready for active installation (\"rfai\") date', 'wip_site','bts_site','strategic_site_bucket',\\\n",
        "             'critical_site_beyond_10','subsequent_sharing_arrangement','first_active_sharing_start_date','first_active_sharing_end_date',\\\n",
        "             'active sharing deployment types','sites_as_metered_estimated']\n",
        "towerdb = replace_values(towerdb, bill_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3iVO8qahdTT"
      },
      "source": [
        "towerdb[[tw_bill]].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4y5qNGSLf6z"
      },
      "source": [
        "Thirth - Check Picklist values All sites\n",
        "\n",
        "Do this check in all sites\n",
        "\n",
        "Check the picklist for each case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbwAq3ClCDqx"
      },
      "source": [
        "picklist_tw_general = {\n",
        "    'categorization by transmission sys': ['Macro', 'Macro+DAS', 'Public DAS','Transmission']\n",
        "}\n",
        "pick_col_general = ['code', 'categorization by transmission sys']\n",
        "\n",
        "df_general_pick = check_picklist(towerdb, tw_index, pick_col_general, picklist_tw_general)\n",
        "df_general_pick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsMno4c2OdqL"
      },
      "source": [
        "Check Picklist and dates formats for In service sites\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CyoPjLhIv88"
      },
      "source": [
        "picklis_dict = {\n",
        "    'categorization by transmission sys': ['Macro', 'Macro+DAS', 'Public DAS','Transmission'],\n",
        "    'categorization by site type': ['DAS passive','GBT','RTT', 'RTT+DAS passive'],\n",
        "    'type of air cooling' : ['Yes', 'No'],\n",
        "    'critical site': ['Yes', 'No'],\n",
        "    'strategic site': ['Yes', 'No'],\n",
        "    'power supply for tenants': ['AC', 'DC', 'no power'],\n",
        "    'bts_site': ['Yes', 'No'],\n",
        "    'strategic site': ['Yes', 'No'],\n",
        "    'critical site': ['Yes', 'No'],\n",
        "    'strategic_site_bucket': ['Non Strategic'],\n",
        "    'critical_site_beyond_10':['Within 10%','Non Critical'],\n",
        "    'wip_site': ['Yes', 'No'],\n",
        "    'sites_as_metered_estimated': ['Estimated Model','Metered Model']\n",
        "}\n",
        "\n",
        "picklist_cols = ['code','categorization by transmission sys', 'categorization by site type', 'type of air cooling',\\\n",
        "                 'critical site','strategic site','power supply for tenants', 'wip_site','bts_site',\\\n",
        "                 'strategic_site_bucket','critical_site_beyond_10','sites_as_metered_estimated','strategic site','critical site']\n",
        "\n",
        "date_cols = ['code','infrastructure ready (existing)/ to be ready (new)', 'ready for active installation (\"rfai\") date', 'first_active_sharing_start_date','first_active_sharing_end_date']\n",
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoExMdcibr9v"
      },
      "source": [
        "actives = replace_values(actives,picklist_cols, '')\n",
        "df_in_service_picklist = check_picklist(actives, tw_index, picklist_cols, picklis_dict)\n",
        "df_in_service_picklist\n",
        "#muitos errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAwey2bGcDER"
      },
      "source": [
        "start_dates = ['infrastructure ready (existing)/ to be ready (new)', 'ready for active installation (\"rfai\") date', 'first_active_sharing_start_date','first_active_sharing_end_date']\n",
        "\n",
        "#date_parser(actives, start_dates, \"%d/%m/%Y\", 'no')\n",
        "#actives['First_Active_Sharing_End_Date'] = actives['First_Active_Sharing_End_Date'].fillna('')\n",
        "df_in_service_dates = check_date_columns(actives, tw_index, date_cols, 2)\n",
        "df_in_service_dates\n",
        "#Muitos Campos em branco"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TNxDZU4fp0J"
      },
      "source": [
        "Fifth Check BTS Flagged(Billing Trigger and Commercial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pd0kaUofmxX"
      },
      "source": [
        "status = 'Yes'\n",
        "df_bts_flagged = check_tw_bill_doer(actives, tw_index,tw_bill, tw_bts, status, 'bill')\n",
        "df_bts_flagged\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AEXpWOvfzLo"
      },
      "source": [
        "Check MoM Sites (BTS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-gggsUqgB6-"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "path_uip = '/content/UserInput_Hungary_20210731.xlsx'\n",
        "uip_names = ['Site_ID','BTS site applicable charge (Annual)',\\\n",
        "             'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip = pd.read_excel(path_uip ,sheet_name='SiteLevel',usecols=[0,1,2],skiprows=2)\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [str(i) for i in msa[msa_index]]\n",
        "tw_sites = [str(i) for i in towerdb[tw_index]]\n",
        "uip_sites = [str(i) for i in uip['Site_ID']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfestas6f6ES"
      },
      "source": [
        "df_mom_bts = check_mom_bts(actives, tw_index, tw_bts, msa, msa_index, msa_bts)\n",
        "df_mom_bts\n",
        "# No one error df_mom_bts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U5IfkxQgsXF"
      },
      "source": [
        "Check UIP sites with towerDB\n",
        "Some countries doesn't have decommissioned column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye6Vdrpyg7tJ"
      },
      "source": [
        "in_service_uip_sites, bts_sites_out_uip, critical = check_uip_tw(towerdb,tw_index, tw_status, '', tw_bts,tw_critical,\\\n",
        "                                                                 uip, uip_sites, 'hu')\n",
        "#no errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFe-T0eRkaLU"
      },
      "source": [
        "Commercial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "5XNsmrtTkbe8",
        "outputId": "e13a434a-89c6-4a84-fa1a-1bae56a4d3c5"
      },
      "source": [
        "path_before = '/content/UserInput_Hungary_20210630.xlsx'\n",
        "names = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', 'Input_Value',\\\n",
        "        'Description/Instruction', 'Frequency of Update']\n",
        "merge_cols = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', \\\n",
        "              'Description/Instruction', 'Frequency of Update']\n",
        "cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Data_Type','Input_Value_actual',\\\n",
        "                'Input_Value_before','Equal Values','Description/Instruction','Frequency of Update']\n",
        "col_replaced = ['Input_Value']\n",
        "df_commercial_diffs = check_commercial(path_uip, path_before,col_replaced, names, merge_cols, cols_ordered)\n",
        "#df_commercial_diffs = check_commercial(path_uip, path_before, replace_values, names, merge_cols, cols_ordered)\n",
        "df_commercial_diffs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Charge_Type</th>\n",
              "      <th>Sub_Charge_Type</th>\n",
              "      <th>Param1</th>\n",
              "      <th>Param2</th>\n",
              "      <th>Data_Type</th>\n",
              "      <th>Input_Value_actual</th>\n",
              "      <th>Input_Value_before</th>\n",
              "      <th>Equal Values</th>\n",
              "      <th>Description/Instruction</th>\n",
              "      <th>Frequency of Update</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>MNO &amp; BARTER TOP-UP ARRANGEMENTS</td>\n",
              "      <td>MNO TOP-UP ARRANGEMENTS</td>\n",
              "      <td>TOP-UP THRESHOLD</td>\n",
              "      <td>0</td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>1699500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>- Annual value (in HUF) as per the MSA\\n- To b...</td>\n",
              "      <td>Only in case of change in MSA/Side letter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>MNO &amp; BARTER TOP-UP ARRANGEMENTS</td>\n",
              "      <td>BARTER TENANCY ARRANGEMENTS</td>\n",
              "      <td>TOP-UP FEE</td>\n",
              "      <td>DIGI</td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>1699500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>- Annual value (in HUF) as per the MSA\\n- To b...</td>\n",
              "      <td>Only in case of change in MSA/Side letter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>MNO &amp; BARTER TOP-UP ARRANGEMENTS</td>\n",
              "      <td>BARTER TENANCY ARRANGEMENTS</td>\n",
              "      <td>TOP-UP FEE</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>412000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>- Annual value (in HUF) as per the MSA\\n- To b...</td>\n",
              "      <td>Only in case of change in MSA/Side letter</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Charge_Type  ...                        Frequency of Update\n",
              "9   MNO & BARTER TOP-UP ARRANGEMENTS  ...  Only in case of change in MSA/Side letter\n",
              "47  MNO & BARTER TOP-UP ARRANGEMENTS  ...  Only in case of change in MSA/Side letter\n",
              "48  MNO & BARTER TOP-UP ARRANGEMENTS  ...  Only in case of change in MSA/Side letter\n",
              "\n",
              "[3 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzYvxx5wnuh8"
      },
      "source": [
        "Excel Log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYdE0UzCnq5h"
      },
      "source": [
        "df_list = [df_in_service_dates, df_in_service_picklist]\n",
        "sheetnames = ['In services dates errrors', 'In servive picklist errors']\n",
        "\n",
        "path = '/content/towerdb_hu_errors.xlsx'\n",
        "general_log_erros(df_list, sheetnames, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymaR9uoioRBC"
      },
      "source": [
        "TA Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFsgljd3ofSq",
        "outputId": "dba90eee-9f34-4e41-a61d-8c5d997a6ce7"
      },
      "source": [
        "\n",
        "ta.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['SiteID', 'SiteID.1', 'In/out of scope', 'Tenant Agreement ID',\n",
              "       'Agreement relates to DAS site?', 'Tenant (name/ID)', 'Classification',\n",
              "       'MNO classification', 'Base fee (@ signing) in HUF',\n",
              "       'Additional fee (@ signing) in HUF',\n",
              "       'Annual Fee per Tenant (@ signing) in HUF',\n",
              "       'Annual Energy Fee  (@ signing)', 'Annual Maintenance Fee  (@ signing)',\n",
              "       'Other Services Fee  (@ signing)', 'Base fee in HUF',\n",
              "       'Additional fee in HUF', 'Current Annual Fee per Tenant in HUF',\n",
              "       'Current Annual Energy Fee in HUF', 'Starting date',\n",
              "       'Expiring date (raw data)', 'Expiring date (in practice)',\n",
              "       'Renewal Option', 'Expiring date after renewal', 'Terms of payments',\n",
              "       'Payment type', 'Index', 'Percentage', 'Indexation driver',\n",
              "       'VAT Suject', 'Percentage (VAT)', 'Termination Date',\n",
              "       'Residual duration'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlcwHRwmqfzY"
      },
      "source": [
        "#ta[ta['SiteID']=='5816']['Base fee (@ signing) in HUF']\n",
        "ta[['Base fee (@ signing) in HUF']].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rmiiDZxoLr0",
        "outputId": "3c7e4a97-e285-4de0-e3fb-92b1650fdb81"
      },
      "source": [
        "pathta = '/content/TA_Input_Hungary_20210731.csv'\n",
        "\n",
        "ta = pd.read_csv(pathta,  encoding='latin')\n",
        "\n",
        "ta_leases = ['Base fee (@ signing) in HUF']\n",
        "ta = replace_values(ta,ta_leases, '')\n",
        "\n",
        "ta_cols = ['SiteID', 'Base fee (@ signing) in HUF']  \n",
        "df_ta_amount = check_amounts(ta, 'SiteID', ta_cols)\n",
        "print(df_ta_amount)\n",
        "# No errors\n",
        "\n",
        "df_ta_dates = check_lc_ta_dates(ta,'SiteID', 'Starting date', 'Expiring date (raw data)')\n",
        "#No erros"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:215: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "     SiteID                 Base fee (@ signing) in HUF\n",
            "1075   1075  Incorret Format to separate decimal values\n",
            "1077   1077  Incorret Format to separate decimal values\n",
            "1107   1107  Incorret Format to separate decimal values\n",
            "1137   1137  Incorret Format to separate decimal values\n",
            "1139   1139  Incorret Format to separate decimal values\n",
            "...     ...                                         ...\n",
            "4770   4770  Incorret Format to separate decimal values\n",
            "5429   5429  Incorret Format to separate decimal values\n",
            "5513   5513  Incorret Format to separate decimal values\n",
            "5516   5516  Incorret Format to separate decimal values\n",
            "5816   5816  Incorret Format to separate decimal values\n",
            "\n",
            "[635 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcStV100sgVb"
      },
      "source": [
        "LC Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vM0_R8z3ZGg"
      },
      "source": [
        "lc[['Current annual lease fees in HUF']].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uiUvmkQsyar"
      },
      "source": [
        " #separador virgulas\n",
        "pathlc = '/content/LC_Input_Hungary_20210731.csv'\n",
        "\n",
        "lc = pd.read_csv(pathlc,  encoding='latin')\n",
        "leases = ['Current annual lease fees in HUF']\n",
        "lc = replace_values(lc,leases, '')\n",
        "\n",
        "lc_cols = ['Code', 'Current annual lease fees in HUF']  \n",
        "\n",
        "df_lc_amount = check_amounts(lc, 'Code', lc_cols)\n",
        "# No errors\n",
        "\n",
        "df_lc_dates = check_lc_ta_dates(lc,'Code', 'Starting date', 'Expiring date (raw data)')\n",
        "#No erros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjqV8mwnuN1L",
        "outputId": "be52476c-1b5e-4212-84c2-ee8a452df280"
      },
      "source": [
        "p = re.compile(r'^\\d+(\\.\\d*[1-9])?$')\n",
        "v = '10200,01'\n",
        "if not str(v).__contains__(','):\n",
        "#if p.match(v) == None:\n",
        "    print(\"Erro\")\n",
        "else:\n",
        "    print(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10200,01\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}