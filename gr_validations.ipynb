{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gr_validations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBYY845rXYQWADcHZ8bbFh"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WElA0gjZWaaE"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, status_col, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    filtered = df_tw[[tw_index, status_col]]\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    df = pd.merge(df, filtered, how='left', left_on=['New Sites'], right_on=tw_index)\n",
        "    df = df[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "    return df\n",
        "\n",
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[wip_tw]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, wip_tw, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "    \"\"\"Reestrurar os script em PT, DE, CZ\"\"\"\n",
        "    # Falta os outros países\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() \n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nisnh2VBTgXh"
      },
      "source": [
        "## Reading Excel File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kv1nEVyTi2R"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "def new_format(df, bts_col):\n",
        "    new_format = []\n",
        "    for i in df[bts_col]:\n",
        "        if i == 'BTS':\n",
        "            new_format.append('Yes')\n",
        "        elif i == 'Legacy':\n",
        "            new_format.append('No')\n",
        "        else:\n",
        "            new_format.append('')\n",
        "    return new_format\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index, date_cols):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df.columns = lower_str(list(df.columns))\n",
        "    \n",
        "    #Create empty columns\n",
        "    for i in range(1, 19):\n",
        "        df[f'column{i}'] = [np.nan for i in range(df.shape[0])]\n",
        "    \n",
        "    # Parsing Dates\n",
        "    for date_col in date_cols:\n",
        "        lista = []\n",
        "        df[date_col] = df[date_col].replace([' '], '')\n",
        "        df[date_col] = df[date_col].fillna('')\n",
        "        for i in df[date_col]:\n",
        "            dates_format = re.match(r'\\d{2}\\/\\d{2}\\/\\d{4}', str(i)) or re.match(r'\\d{2}\\-\\d{2}\\-\\d{4}', str(i))\n",
        "            not_date_format = not re.match(r'\\d{2}\\/\\d{2}\\/\\d{4}', str(i)) or not re.match(r'\\d{2}\\-\\d{2}\\-\\d{4}', str(i))\n",
        "            if i !='' and not_date_format:\n",
        "                #print(towerdb[towerdb[date_col]==i][['identification - site key', date_col]])\n",
        "                lista.append(f'{i:%d/%m/%Y}')\n",
        "            \n",
        "            elif dates_format:\n",
        "                lista.append(i)\n",
        "            else:\n",
        "                lista.append('')\n",
        "        df[date_col] = lista\n",
        "\n",
        "    return df\n",
        "\n",
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = \"identification - site key\"\n",
        "tw_doer_vf = 'msa - date of equipment removal (vf)'\n",
        "tw_doer_wh = 'msa - date of equipment removal (wh)'\n",
        "tw_status = 'other - status'\n",
        "tw_bts = 'flag indicating bts site'\n",
        "tw_bill = 'msa - billing trigger date'\n",
        "\"\"\"tw_wip isn't have in GR so, billing trigger date should be in future\"\"\"\n",
        "tw_decom = 'msa - date of decommissioning'\n",
        "tw_amount = 'lease contract - current annual lease fee'\n",
        "tw_critical = 'msa - critical site in excess of the 15.5% cap'\n",
        "\n",
        "msa_index = 'identification - site key'\n",
        "msa_bts = 'flag indicating bts site'\n",
        "msa_doer_vf = 'msa - date of equipment removal (vf)'\n",
        "msa_doer_wh = 'msa - date of equipment removal (wh)'\n",
        "msa_status = 'Other - Status'\n",
        "msa_bill = 'msa - billing trigger date'\n",
        "#tw_wip isn't have in GR so, billing trigger date should be in future\n",
        "msa_decom = 'msa - date of decommissioning'\n",
        "msa_amount = 'lease contract - current annual lease fee'\n",
        "msa_critical = 'msa - critical site in excess of the 15.5% cap'\n",
        "\n",
        "guideline = [\"towerdb owner\", \"identification - site key\", \"identification - code\", \"identification - fl\",\\\n",
        "             \"position - site name\", \"identification - other mno fl\", \"position - macro region\", \\\n",
        "             \"position - region\", \"position - municipality\", \"position - address\", \"position - latitude\", \\\n",
        "             \"position - longitude\", \"category - categorization by transmission sys (subcluster)\", \\\n",
        "             \"category - categorization by site type\", \"column1\", \"category - categorisation by inhabitants\", \\\n",
        "             \"other - ownership\", \"other - status\", \"column2\", \"column3\", \"column4\", \"column5\", \"column6\", \\\n",
        "             \"column7\", \"column8\", \"column9\", \"column10\", \"lease contract - current annual lease fee\", \\\n",
        "             \"lease contract - lease contract comment\", \"column11\", \"column12\", \"column13\", \"column14\", \\\n",
        "             \"tenants - anchor\", \"tenants - vf - ran sharing\", \"tenants - vf - passive\", \\\n",
        "             \"tenants - wind - ran sharing\", \"tenants - wind - passive\", \"tenants - cosmote - passive\", \\\n",
        "             \"tenants - ran sharing tenants\", \"tenants - passive tenants\", \"tenants - non-mno tenants\", \\\n",
        "             \"tenants - non-mno tenant name\", \"tenants - total tenants\", \"column15\", \\\n",
        "             \"ee - no. of 24hr generators\", \"ee - no. of standby generators\", \"kpi - sites\", \"kpi - tenants\", \\\n",
        "             \"msa - bts/replacement\", \"msa - bts commitment site\", \"column16\", \\\n",
        "             \"infrastructure type - technology\", \"infrastructure type - fibre / microwave\", \\\n",
        "             \"infrastructure type - floor space\", \"infrastructure type - tower height\", \"column17\",\\\n",
        "             \"notified-97 (gr use only)\", \"changes (gr use only)\", \"msa - billing trigger stop date\", \\\n",
        "             \"other - indoor vs outdoor\", \"skylon - category\", \"msa - standard configuration\", \\\n",
        "             \"msa - construction type\", \"msa - hub sites total mw diameter\", \"bp - consolidated classification\",\\\n",
        "             \"msa - critical site\", \"sensitive - due to\", \"sensitive - description\", \"sensitive - department\", \\\n",
        "             \"notified - department\", \"notified - due to\", \"msa - sensitive\", \"msa - notified\", \\\n",
        "             \"notified - status\", \"notified - details\", \"msa - rectifier\", \"msa - battery\", \"msa - aircon\", \\\n",
        "             \"msa - revenue generating\", \"msa - date of decommissioning\", \"flag indicating bts site\",\\\n",
        "             \"msa - billing trigger date\", \"msa - dg-shelter\", \"msa - critical site in excess of the 15.5% cap\",\\\n",
        "             \"msa - form of active sharing\", \"msa - start date for active sharing arrangement\",\\\n",
        "             \"msa - end date for active sharing arrangement\", \"column18\", \"msa - rfai date\", \\\n",
        "             \"msa - site acceptance date\", \"msa - sub-lease reason\", \"msa - sub-lease applies\", \\\n",
        "             \"msa - date of equipment removal (vf)\", \"msa - date of equipment removal (wh)\"]\n",
        "guideline = list(map(lambda x: re.sub(r'^\\s+|\\s+$', '', str(x.lower())), guideline))\n",
        "\n",
        "date_columns = ['msa - start date for active sharing arrangement', 'msa - rfai date', 'msa - site acceptance date', 'msa - billing trigger date', \n",
        "            'msa - date of decommissioning', 'msa - date of equipment removal (vf)', 'msa - date of equipment removal (wh)', 'msa - billing trigger stop date',\n",
        "            'msa - end date for active sharing arrangement']\n",
        "\n",
        "path_tw = \"/content/GR_TowerDB Jun'21 FINAL (Billing version).xlsx\"\n",
        "sheet_tw = 'GR_TowerDB'\n",
        "towerdb = read_files(path_tw, sheet_tw, 0, 0, 'Identification - Site Key', date_columns)\n",
        "\n",
        "towerdb.rename(columns={'msa - legacy/bts': \"flag indicating bts site\"}, inplace=True)\n",
        "\n",
        "towerdb.columns = list(map(lambda x: re.sub(r'^\\s+|\\s+$', '', str(x.lower())), towerdb.columns.to_list()))\n",
        "towerdb = towerdb[guideline]\n",
        "towerdb['tenants - non-mno tenants'] = [int(i) if not pd.isnull(i) else '' for i in towerdb['tenants - non-mno tenants']]\n",
        "\n",
        "towerdb[tw_bts] = new_format(towerdb, tw_bts)\n",
        "\n",
        "towerdb.to_csv('/content/TowerDB_Greece_20210831.csv', index=False, encoding='iso-8859-1', errors='replace')\n",
        "\n",
        "towerdb.head(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHZpkm8tT9Pg"
      },
      "source": [
        "Csv(in month and true up) read\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8KzPSagUEwK"
      },
      "source": [
        "from unidecode import unidecode\n",
        "path_in_month = '/content/TowerDB_Greece_20210731_renan.csv'\n",
        "past_tw = pd.read_csv(path_in_month, engine='python').fillna('')\n",
        "past_tw.columns = lower_str(list(past_tw.columns))\n",
        "#towerdb = towerdb.reindex(columns = lower_str(col_order))\n",
        "past_tw['identification - site key'] = past_tw['identification - site key'].apply(unidecode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsZxVhNQZBbC"
      },
      "source": [
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    \"\"\"\n",
        "    for i in bill_cols:\n",
        "        if i not in twdb_col:\n",
        "            col_miss.append(i)\"\"\"\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "path_msa = '/content/TowerDB_Greece_20210731.csv'\n",
        "msa = pd.read_csv(path_msa, engine='python',encoding='latin1')\n",
        "msa.columns = lower_str(list(msa.columns))\n",
        "msa['identification - site key'] = msa['identification - site key'].apply(unidecode)\n",
        "\"\"\"Fazer a organização das colunas somente com o ficheiro correto\"\"\"\n",
        "\"\"\"Check Columns Received\"\"\"\n",
        "df_cols = check_columns_received(towerdb,list(msa.columns))\n",
        "df_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou27hRQ0LNDJ"
      },
      "source": [
        "Check columns received looking for missing columns that is gonna be used in rating engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PWcoSWU60Z-"
      },
      "source": [
        "Finding ? in site ID, if find change to A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMBRjWgO6zwb"
      },
      "source": [
        "def replace_id(df, col_find, find_value, replace_to):\n",
        "    lista = [re.sub(f'[{find_value}]',replace_to, str(x)) for x in df[col_find]]\n",
        "    return lista\n",
        "\n",
        "towerdb[tw_index] = replace_id(towerdb, tw_index.lower(), '?', 'A')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCkGz4T96CDM"
      },
      "source": [
        "Aditional formating for legacy/bts sites \n",
        "ONLY xlsx File\n",
        "\n",
        "Legacy to No\n",
        "\n",
        "BTS to Yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbNJS8mYnIvg"
      },
      "source": [
        "First Check - Dates Formats (dd/mm/YYYY) \n",
        "\n",
        "Columns: Billing trigger & DOER\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCI22E3Unc9Z"
      },
      "source": [
        "\"\"\"Verify before, cause you need to convert all values in cols for string format to check\"\"\"\n",
        "def check_date_columns(df, df_index,status_col,columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    def count_duplicates(lista):\n",
        "        count_dict = {}\n",
        "        for entry in lista:\n",
        "            if entry in count_dict.keys():\n",
        "                count_dict[entry] += 1\n",
        "            else:\n",
        "                count_dict[entry] = 1\n",
        "        \n",
        "        duplicates = {}\n",
        "        for k, v in count_dict.items():\n",
        "            if v > 1:\n",
        "                duplicates[k] = v\n",
        "        return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "# Columns to functions\n",
        "bill_col = [tw_index, tw_bill]\n",
        "doer_cols = [tw_index, tw_doer_vf, tw_doer_wh]\n",
        "\n",
        "\n",
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "no_actives = towerdb[towerdb[tw_status]=='Dismantled / Active Lease']\n",
        "\n",
        " #Checking columns for errors\n",
        "df_dates_bill_errors = check_date_columns(actives, tw_index, tw_status, bill_col, 2)\n",
        "#NO errors\n",
        "\n",
        "df_dates_doer_errors = check_date_columns(no_actives, tw_index,tw_status, doer_cols, 2) \n",
        "# Just Blank Values in DOER's Columns\n",
        "\n",
        "print(df_dates_bill_errors)\n",
        "print('\\n')\n",
        "print(df_dates_doer_errors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3neFPxjvrWhA"
      },
      "source": [
        "Second Check - TW Amount value General (xxx.xx)\n",
        "\n",
        "Column(s): Lease Contract - Current annual lease fee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjrxRcEhrowY",
        "outputId": "1b314661-5466-499f-ebfd-dd9e9c1ea341"
      },
      "source": [
        "def check_amounts(df_check, df_index, status_col, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    filtered = df_check[[df_index, status_col]]\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        df_new = pd.merge(df_new, filtered, how='left', left_on='identification - site key', right_on=tw_index)\n",
        "        df_new = df_new.set_index(tw_index)\n",
        "        df_new = df_new[[status_col]+ df_new.columns[:-1].tolist()]\n",
        "        #df_new = df_new[['identification - site key', status_col]]\n",
        "        return df_new\n",
        "    else: \n",
        "        print('\\nNo one columns with incorrect Amount format!\\n')\n",
        "\n",
        "amount_cols = [tw_index, tw_amount]\n",
        "df_amount_errors = check_amounts(actives, tw_index, tw_status, amount_cols,'.')\n",
        "df_amount_errors\n",
        "#No one error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No one columns with incorrect Amount format!\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOA4ST6kBBlI"
      },
      "source": [
        "Thirth - Check Picklist values\n",
        "Do this check only for sites In Service Status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljjGsYab6yqH"
      },
      "source": [
        "Check On Air Picklist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ4stbvLu694",
        "outputId": "618cc919-c616-4007-f090-423b99c52aee"
      },
      "source": [
        "def check_picklist_v1(df,df_index,df_status, df_cols, picklist_dict):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    df_picklist = df[df_cols].fillna('')\n",
        "    df_picklist = df_picklist.to_dict(orient='list')\n",
        "\n",
        "    new_dic = defaultdict(list)\n",
        "    \"\"\"for id in df_picklist[df_index]:\n",
        "        if id not in new_dic.keys():\n",
        "            new_dic[df_index].append(id)\"\"\" \n",
        "            \n",
        "    for column in set(picklist_dict.keys()):\n",
        "        for value in df_picklist[column]:\n",
        "            if value not in set(picklist_dict[column]): \n",
        "                if pd.isnull(value) or pd.isna(value) or value=='' or value=='nan':\n",
        "                    new_dic[column].append('Blank Value')\n",
        "                else:\n",
        "                    new_dic[column].append(f'Incorrect value: {value}')\n",
        "            else:\n",
        "                new_dic[column].append('Ok!')\n",
        "\n",
        "    df_errors = pd.DataFrame(new_dic)\n",
        "    df_errors = df_errors.replace('Ok!', np.nan)\n",
        "    df_errors = df_errors.dropna(how='all', axis=1)\n",
        "    \n",
        "    if not df_errors.empty:\n",
        "        df = df[[df_index, df_status]]\n",
        "        #df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors[df_index] = df_picklist[df_index]\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        df_errors = df_errors.dropna(how='all', axis=0)\n",
        "        df_errors = df_errors.reset_index()\n",
        "        df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        return df_errors\n",
        "    else:\n",
        "        print('\\nNo one Picklist Error Founded!\\n')\n",
        "        \n",
        "picklist_tw = {\n",
        "    'towerdb owner' : ['Vodafone','Wind'],\n",
        "    'category - categorisation by inhabitants': ['Heavy Urban','Urban','Rural', 'Suburban'],\n",
        "    'flag indicating bts site': ['Yes', 'No'],\n",
        "    'other - ownership': ['Collocation_VF_Own','Collocation_WH_Own','Shared_VF_Own','Shared_WH_Own','Standalone'],\n",
        "    'other - indoor vs outdoor': ['Indoor','Outdoor','N/A - No Active Equipment'],\n",
        "    'msa - construction type': ['DAS','Rooftop','Greenfield'],\n",
        "    'bp - consolidated classification': ['GBT','RTT','Transmission Site','Special','Transmission Site',\\\n",
        "                                         'Smaller Macro Site','Outdoor Small Cell','n/a - Data Unavailable'],\n",
        "    'msa - rectifier': ['Yes', 'No'],\n",
        "    'msa - battery': ['Yes', 'No'],\n",
        "    'msa - aircon': ['No', 'Yes'],\n",
        "    'msa - dg-shelter': ['Yes', 'No'],\n",
        "    'msa - sub-lease applies': ['Yes', 'No']\n",
        "}\n",
        "picklist_cols = ['Identification - Site Key', 'TowerDB owner','Category - Categorisation by inhabitants',\\\n",
        "                'flag indicating bts site','Other - Ownership','Other - Indoor vs Outdoor','MSA - Construction Type',\\\n",
        "                'BP - Consolidated classification','MSA - Rectifier','MSA - Battery','MSA - Aircon',\\\n",
        "                'MSA - DG-Shelter','MSA - Sub-Lease Applies']\n",
        "\n",
        "actives_1 = towerdb[towerdb[tw_status]=='In Service']\n",
        "picklist_cols = lower_str(picklist_cols)\n",
        "df_picklist = check_picklist_v1(actives_1, tw_index, tw_status, picklist_cols, picklist_tw)\n",
        "df_picklist\n",
        "#No Erros"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No one Picklist Error Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJPGS-Y4616j"
      },
      "source": [
        "Check General Picklist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbbSNoXcp-kB"
      },
      "source": [
        "general_pick = {\n",
        "    'towerdb owner' : ['Vodafone','Wind'],\n",
        "    'category - categorisation by inhabitants': ['Heavy Urban','Urban','Rural']}\n",
        "general_picklist_cols = ['Identification - Site Key', 'TowerDB owner','Category - Categorisation by inhabitants']\n",
        "df_picklist_general = check_picklist_v1(towerdb, tw_index,tw_status, lower_str(general_picklist_cols), general_pick)\n",
        "df_picklist_general\n",
        "#Error blank values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWzjva6JAMoL"
      },
      "source": [
        "Fourth Check - Remove \"N/A\", \"0\" or \"-\" values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYoShmlRMTv0"
      },
      "source": [
        "Fifth Check MoM Sites (BTS, decomissoned...)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVzDPUxiL8Wa",
        "outputId": "06498935-c462-4bf7-9689-b5291fb07226"
      },
      "source": [
        "\"\"\"Falta Coluna de Flag Indicating BTS Site no twerdb recebido\"\"\"\n",
        "def check_mom_bts(df_tw, tw_index,status_col, tw_col, df_msa, msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index,status_col, tw_col]]  \n",
        "    else:\n",
        "        print('\\nNo Errors founded!\\n')\n",
        "\n",
        "df_mom_bts = check_mom_bts(actives, tw_index, tw_status, tw_bts, msa, msa_index, msa_bts)\n",
        "# No one error df_mom_bts\n",
        "\n",
        "df_mom_decom = check_mom_bts(actives, tw_index,tw_status, tw_decom, msa, msa_index, msa_decom)\n",
        "# No one error df_mom_decom\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors founded!\n",
            "\n",
            "\n",
            "No Errors founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_oKz-xhQvww"
      },
      "source": [
        "Seventh Check If Bts sites has date in Billing Trigger Column "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6UkioJSZ2dM",
        "outputId": "7f89b0ea-f6f4-4a1d-9377-4f35bb1928f4"
      },
      "source": [
        "act = list(towerdb[towerdb[tw_status]=='In Service'][tw_index])\n",
        "sites = [i for i in act if i not in uip_vf_sites]\n",
        "len(sites)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4819"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5gd7RdzdmM7"
      },
      "source": [
        "UIS for IN month check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXIlh8TwdlpQ"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "path_vf = '/content/UserInput_Greece_VF_20210731.xlsx'\n",
        "path_wh = '/content/UserInput_Greece_WH_20210731.xlsx'\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Energy: Diesel Charges']\n",
        "uip_vf = pd.read_excel(path_vf ,sheet_name='SiteLevel',usecols=[0,1,4],skiprows=2)\n",
        "uip_wh = pd.read_excel(path_wh ,sheet_name='SiteLevel',usecols=[0,1,4],skiprows=2)\n",
        "uip = pd.concat([uip_vf, uip_wh], ignore_index=True)\n",
        "uip.columns = uip_names\n",
        "uip_vf.columns = uip_names\n",
        "uip_wh.columns = uip_names\n",
        "uip_vf_sites = [i for i in uip_vf['Site_ID']]\n",
        "uip_wh_sites = [i for i in uip_wh['Site_ID']]\n",
        "\n",
        "\n",
        "msa_sites = [i for i in msa[msa_index]]\n",
        "tw_sites = [i for i in towerdb[tw_index]]\n",
        "uip_sites = uip_vf_sites+uip_wh_sites"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJQkfp0cRrmW"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "path_vf = '/content/UserInput_Greece_VF_20210831.xlsx'\n",
        "path_wh = '/content/UserInput_Greece_WH_20210831.xlsx'\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Energy: Diesel Charges']\n",
        "uip_vf = pd.read_excel(path_vf ,sheet_name='SiteLevel',usecols=[0,1,4],skiprows=2)\n",
        "uip_wh = pd.read_excel(path_wh ,sheet_name='SiteLevel',usecols=[0,1,4],skiprows=2)\n",
        "#uip = pd.concat([uip_vf, uip_wh], ignore_index=True)\n",
        "#uip.columns = uip_names\n",
        "uip_vf.columns = uip_names\n",
        "uip_wh.columns = uip_names\n",
        "uip_vf_sites = [i for i in uip_vf['Site_ID']]\n",
        "uip_wh_sites = [i for i in uip_wh['Site_ID']]\n",
        "\n",
        "msa_sites_vf = [i for i in msa[msa['towerdb owner']=='Vodafone'][tw_index]]\n",
        "msa_sites_wh = [i for i in msa[msa['towerdb owner']=='Wind'][tw_index]]\n",
        "\n",
        "tw_sites_vf = [i for i in towerdb[towerdb['towerdb owner']=='Vodafone'][tw_index]]\n",
        "tw_sites_wh = [i for i in towerdb[towerdb['towerdb owner']=='Wind'][tw_index]]\n",
        "#uip_sites = uip_vf_sites+uip_wh_sites\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KetPiPEMvzm_",
        "outputId": "a4f5c858-5a5d-4251-db94-9ad72c345843"
      },
      "source": [
        "len(msa_sites_vf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2956"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "levK2zxZfPhZ"
      },
      "source": [
        "Vodafone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btx20uqqRBRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c160149-1950-45bd-bbbd-f6c43e706555"
      },
      "source": [
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [i for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = pd.to_datetime(df[bill_col], errors='coerce')\n",
        "    df_bts_demerged = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "    #if not (new_sites.empty or bts_out_uis.empty or df_site_bts.empty):\n",
        "    return new_sites, df_bts_demerged[[tw_index,status_col, bts_col, bill_col]]\n",
        "\n",
        "#actives_vf = actives[actives[\"towerdb owner\"]=='Vodafone']\n",
        "\n",
        "new_sites_vf, df_bts_demerged_vf = check_new_sites(towerdb, tw_index, tw_bts,\\\n",
        "                                                    tw_bill,tw_status, msa_sites_vf, tw_sites_vf)\n",
        "\n",
        "print(new_sites_vf)\n",
        "print('\\n')\n",
        "print(df_bts_demerged_vf)\n",
        "#No errors\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [New_Sites, other - status]\n",
            "Index: []\n",
            "\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [identification - site key, other - status, flag indicating bts site, msa - billing trigger date]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcBGQlV2fREY"
      },
      "source": [
        "Wind"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxIYAvTtfSiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7abd369-eaee-4a64-fc5c-d22f63be7be5"
      },
      "source": [
        "actives_wh = actives[~(actives[\"towerdb owner\"]=='Vodafone')]\n",
        "\n",
        "new_siteswh, bts_sites_out_uis_wh = check_new_sites(actives_wh, tw_index, tw_bts, tw_bill, \\\n",
        "                                                    tw_status, msa_sites_wh, tw_sites_wh)\n",
        "print(new_siteswh)\n",
        "print('\\n')\n",
        "print(bts_sites_out_uis_wh)\n",
        "# new = 13, out = 16, df_bts = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      New_Sites other - status\n",
            "0  90642-Α64201            NaN\n",
            "1  90750-Α75002     In Service\n",
            "Empty DataFrame\n",
            "Columns: [identification - site key, other - status, flag indicating bts site, msa - billing trigger date]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsoS_7srk3FE"
      },
      "source": [
        "Eighth - Check DOER "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCCW_LsSlwu2"
      },
      "source": [
        "\"\"\"In services Checks\"\"\"\n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]') < current_date)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "        else:\n",
        "            #bill_dates = pd.to_datetime(df_tw[date_col], errors==coerrce)\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]').empty)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "\n",
        "in_service = 'In Service'\n",
        "bill_col = [tw_index, tw_bill]\n",
        "#doer_cols = [tw_index,  tw_doer_vf, tw_doer_wh]\n",
        "actives_vf = actives[actives[\"towerdb owner\"]=='Vodafone']\n",
        "actives_wh = actives[~(actives[\"towerdb owner\"]=='Vodafone')]\n",
        "\n",
        "df_doer_vf_on = check_tw_bill_doer(actives_vf, tw_index, tw_doer_vf, tw_status, in_service, 'doer')\n",
        "\n",
        "df_doer_wh_on = check_tw_bill_doer(actives_wh, tw_index, tw_doer_wh, tw_status, in_service, 'doer')\n",
        "# NO errors in both files\n",
        "\n",
        "print(df_doer_vf_on)\n",
        "print('\\n')\n",
        "print(df_doer_wh_on)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUB1l3Smwy3s"
      },
      "source": [
        "\"\"\"Planned Checks\"\"\"\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col,bill_col, status_col, dt_format=\"%Y-%m-%d\"):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date, format=dt_format)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        df_tw[bill_col] = pd.to_datetime(df_tw[bill_col],errors='coerce', format=dt_format)\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)&\\\n",
        "                         (df_tw[bill_col].astype('datetime64[ns]') < current_date)]\n",
        "        return filtered[[tw_index, status_col, bill_col, doer_col]]  \n",
        "    else:\n",
        "        print('\\nNothing wrong in services sites!\\n')\n",
        "# or towerdb[tw_status]=='Planned To be dismantled' or towerdb[tw_status]=='Planned To Be Dismantled')\n",
        "twdb_vf = towerdb[(towerdb[\"towerdb owner\"]=='Vodafone')&(towerdb[tw_status]=='Planned')]\n",
        "twdb_wh = towerdb[(towerdb[\"towerdb owner\"]=='Wind')&(towerdb[tw_status]=='Planned')]\n",
        "format = \"%Y-%m-%d\"\n",
        "df_doer_vf_plann = check_tw_doer_planned(twdb_vf, tw_index, tw_doer_vf,tw_bill, tw_status)\n",
        "df_doer_wh_plann = check_tw_doer_planned(twdb_wh, tw_index, tw_doer_wh,tw_bill, tw_status)\n",
        "\n",
        "print(df_doer_vf_plann)\n",
        "print('\\n')\n",
        "print(df_doer_wh_plann)\n",
        "#No errors in both\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in1fx1_Cqg-A"
      },
      "source": [
        "Nineth - Check Decomissioned Sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnvVfn7uopVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439e82dc-d69e-447d-aea7-051ba4b4e071"
      },
      "source": [
        "def check_decommissioned(df,df_index,status_col, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    return filtered[[df_index,status_col, decom_col, doer_col]]\n",
        "  \n",
        "twdb_vf = towerdb[towerdb[\"towerdb owner\"]=='Vodafone']\n",
        "twdb_wh = towerdb[~(towerdb[\"towerdb owner\"]=='Vodafone')]\n",
        "\n",
        "df_doer_vf = check_decommissioned(twdb_vf, tw_index,tw_status, tw_decom, tw_doer_vf)\n",
        "df_doer_wh = check_decommissioned(twdb_wh, tw_index,tw_status, tw_decom, tw_doer_wh)\n",
        "\n",
        "print(df_doer_vf)\n",
        "print('\\n')\n",
        "print(df_doer_wh)\n",
        "#No ERRORS\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [identification - site key, other - status, msa - date of decommissioning, msa - date of equipment removal (vf)]\n",
            "Index: []\n",
            "\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [identification - site key, other - status, msa - date of decommissioning, msa - date of equipment removal (wh)]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jySB4PFNsGZs"
      },
      "source": [
        "Tenth - Check UIP Towerdb matches\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dq3IcUl37K9"
      },
      "source": [
        "## Vodafone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I83XwkisJrP"
      },
      "source": [
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, uip_sites):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "    in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['UIS Sites not active in TowerDB!'])\n",
        "    in_service_uip_sites = pd.merge(in_service_uip_sites, filtered, how='left', left_on='UIS Sites not active in TowerDB!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_uip_sites = in_service_uip_sites[['UIS Sites not active in TowerDB!', status_tw_col]]\n",
        "\n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out UIS'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out UIS',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out UIS', status_tw_col]]\n",
        "    \n",
        "    #Check Decomissioned Sites\n",
        "    tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "    decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "    decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                    right_on=tw_index)\n",
        "    decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "    \n",
        "    #Check BTS sites\n",
        "    bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "    bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "    bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site out UIS File'])\n",
        "    bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site out UIS File',\\\n",
        "                                    right_on=tw_index)\n",
        "    bts_sites_out_uip = bts_sites_out_uip[['BTS Site out UIS File', status_tw_col]]\n",
        "\n",
        "\n",
        "    bts_tw_critical = [i for i in df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]]\n",
        "    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "    critical = pd.DataFrame(critical, columns=['Sites Beyond 10% out UIS'])\n",
        "    critical = pd.merge(critical, filtered, how='left', left_on='Sites Beyond 10% out UIS',\\\n",
        "                                    right_on=tw_index)\n",
        "    critical = critical[['Sites Beyond 10% out UIS', status_tw_col]]\n",
        "\n",
        "    return in_service_uip_sites,in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "tw_vf = towerdb[towerdb[\"towerdb owner\"]=='Vodafone']\n",
        "in_service_uip_sites_vf,in_service_not_in_uis_vf, decomiss_sites_in_uip_vf, bts_sites_out_uip_vf, critical_vf = check_uip_tw(tw_vf,tw_index, tw_status, \\\n",
        "                                                            tw_decom, tw_bts, tw_critical, uip_vf_sites)\n",
        "\n",
        "print(in_service_uip_sites_vf)\n",
        "print('\\n')\n",
        "print(in_service_not_in_uis_vf)\n",
        "print('\\n')\n",
        "print(decomiss_sites_in_uip_vf)\n",
        "print('\\n')\n",
        "print(bts_sites_out_uip_vf)\n",
        "print('\\n')\n",
        "print(critical_vf)\n",
        "# No one errors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WVYRvB53zUz"
      },
      "source": [
        "## Wind"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4XdsxgnkFft"
      },
      "source": [
        "tw_wh = towerdb[towerdb[\"towerdb owner\"]=='Wind']\n",
        "uis_sites_not_in_towerdbwh, in_service_not_in_uiswh, decomiss_sites_in_uiswh, bts_sites_out_uipwh, critical_wh = check_uip_tw(tw_wh,tw_index, tw_status, \\\n",
        "                                                            tw_decom, tw_bts,tw_critical, uip_wh_sites)\n",
        "print(uis_sites_not_in_towerdbwh)\n",
        "print('\\n')\n",
        "print(in_service_not_in_uiswh)\n",
        "print('\\n')\n",
        "print(decomiss_sites_in_uiswh)\n",
        "print('\\n')\n",
        "print(bts_sites_out_uipwh)\n",
        "print('\\n')\n",
        "print(critical_wh)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPK_EnaCFRpE"
      },
      "source": [
        "wh_special = tw_wh[(tw_wh['bp - consolidated classification']=='Special')&(tw_wh[tw_status]=='In Service')][tw_index].to_list()\n",
        "wh_special\n",
        "sp_wh_not_uis = [i for i in uip_wh_sites if i not in wh_special]\n",
        "sp_wh_not_uis\n",
        "\n",
        "vf_special = tw_vf[(tw_vf['bp - consolidated classification']=='Special')&(tw_vf[tw_status]=='In Service')][tw_index].to_list()\n",
        "\n",
        "sp_vf_not_uis = [i for i in uip_vf_sites if i not in vf_special]\n",
        "print(sp_vf_not_uis)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x80odc_w8jiU"
      },
      "source": [
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    return df_commercial_diffs\n",
        "\n",
        "def check_diffs_v2(path_current, path_last):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def highlight_diff(data, color='yellow'):\n",
        "        attr = 'background-color: {}'.format(color)\n",
        "        other = data.xs('Current', axis='columns', level=-1)\n",
        "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                            index=data.index, columns=data.columns)\n",
        "\n",
        "    _actual = pd.read_excel(path_current,sheet_name='Commercial').fillna('')\n",
        "\n",
        "    _before = pd.read_excel(path_last,sheet_name='Commercial').fillna('')\n",
        "\n",
        "    df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "    df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "    #df_final.style.apply(highlight_diff, axis=None)\n",
        "    if not df_final.empty:\n",
        "        return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "    else:\n",
        "        print('\\nNo differences Founded!\\n')\n",
        "\n",
        "df_com_vf = check_diffs_v2('/content/UserInput_Greece_VF_20210831.xlsx', '/content/UserInput_Greece_VF_20210731.xlsx')\n",
        "\n",
        "df_com_wh = check_diffs_v2('/content/UserInput_Greece_WH_20210831.xlsx', '/content/UserInput_Greece_WH_20210731.xlsx')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w-2KOenxTAi"
      },
      "source": [
        "Eleventh - Check commercial values\n",
        "\n",
        "Made Changes in comparisons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0-RN108xF1p"
      },
      "source": [
        "#check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order)\n",
        "path_vf_before = '/content/UserInput_Greece_VF_20210630.xlsx'\n",
        "path_vf_curr = '/content/UserInput_Greece_WH_20210630.xlsx'\n",
        "names = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Param3','Param4', 'Data_Type', 'Input_Value',\\\n",
        "        'Description/Instruction', 'Frequency of Update']\n",
        "merge_cols = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Param3','Param4', 'Data_Type', \\\n",
        "              'Description/Instruction', 'Frequency of Update']\n",
        "cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Param3','Param4','Data_Type','Input_Value_actual',\\\n",
        "                'Input_Value_before','Equal Values','Description/Instruction', 'Frequency of Update']\n",
        "df_commercial_diffs_vf = check_commercial(path_vf, path_vf_before, 'Input_Value', names, merge_cols, cols_ordered)\n",
        "# No differences\n",
        "df_commercial_diffs_wh = check_commercial(path_wh, path_wh_before, 'Input_Value', names, merge_cols, cols_ordered)\n",
        "# No differences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFmJ8XexPYJT"
      },
      "source": [
        "Commercial Vodafone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "C7uEm9upPRkI",
        "outputId": "cf6af4af-6932-4ef4-b343-4021423ae0d7"
      },
      "source": [
        "path_vf_before = '/content/UserInput_Greece_VF_20210731.xlsx'\n",
        "path_vf_curr = '/content/UserInput_Greece_VF_20210831.xlsx'\n",
        "commercial_diffs = check_diffs_v2(path_vf_curr, path_vf_before, '')\n",
        "commercial_diffs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_48866f6a_f439_11eb_8a56_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" colspan=2>Sub_charge_Type</th>        <th class=\"col_heading level0 col2\" colspan=2>Param1</th>        <th class=\"col_heading level0 col4\" colspan=2>Param2</th>        <th class=\"col_heading level0 col6\" colspan=2>Param3</th>        <th class=\"col_heading level0 col8\" colspan=2>Param4</th>        <th class=\"col_heading level0 col10\" colspan=2>Data_Type</th>        <th class=\"col_heading level0 col12\" colspan=2>Input_Value</th>        <th class=\"col_heading level0 col14\" colspan=2>Description/Instruction</th>        <th class=\"col_heading level0 col16\" colspan=2>Frequency of Update</th>    </tr>    <tr>        <th class=\"blank level1\" ></th>        <th class=\"col_heading level1 col0\" >Current</th>        <th class=\"col_heading level1 col1\" >Last</th>        <th class=\"col_heading level1 col2\" >Current</th>        <th class=\"col_heading level1 col3\" >Last</th>        <th class=\"col_heading level1 col4\" >Current</th>        <th class=\"col_heading level1 col5\" >Last</th>        <th class=\"col_heading level1 col6\" >Current</th>        <th class=\"col_heading level1 col7\" >Last</th>        <th class=\"col_heading level1 col8\" >Current</th>        <th class=\"col_heading level1 col9\" >Last</th>        <th class=\"col_heading level1 col10\" >Current</th>        <th class=\"col_heading level1 col11\" >Last</th>        <th class=\"col_heading level1 col12\" >Current</th>        <th class=\"col_heading level1 col13\" >Last</th>        <th class=\"col_heading level1 col14\" >Current</th>        <th class=\"col_heading level1 col15\" >Last</th>        <th class=\"col_heading level1 col16\" >Current</th>        <th class=\"col_heading level1 col17\" >Last</th>    </tr></thead><tbody>\n",
              "        </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f7cf06609d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X87urnLjPngg"
      },
      "source": [
        "Commercial Wind"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "SGf6JvytP1Gd",
        "outputId": "8a817a52-270e-4c7c-9f86-4e811e2e5f45"
      },
      "source": [
        "path_wh_before = '/content/UserInput_Greece_WH_20210731.xlsx'\n",
        "path_wh_curr = '/content/UserInput_Greece_WH_20210831.xlsx'\n",
        "commercial_wh = check_diffs_v2(path_vf_curr, path_vf_before, '')\n",
        "commercial_wh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_837a4ce0_f439_11eb_8a56_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" colspan=2>Sub_charge_Type</th>        <th class=\"col_heading level0 col2\" colspan=2>Param1</th>        <th class=\"col_heading level0 col4\" colspan=2>Param2</th>        <th class=\"col_heading level0 col6\" colspan=2>Param3</th>        <th class=\"col_heading level0 col8\" colspan=2>Param4</th>        <th class=\"col_heading level0 col10\" colspan=2>Data_Type</th>        <th class=\"col_heading level0 col12\" colspan=2>Input_Value</th>        <th class=\"col_heading level0 col14\" colspan=2>Description/Instruction</th>        <th class=\"col_heading level0 col16\" colspan=2>Frequency of Update</th>    </tr>    <tr>        <th class=\"blank level1\" ></th>        <th class=\"col_heading level1 col0\" >Current</th>        <th class=\"col_heading level1 col1\" >Last</th>        <th class=\"col_heading level1 col2\" >Current</th>        <th class=\"col_heading level1 col3\" >Last</th>        <th class=\"col_heading level1 col4\" >Current</th>        <th class=\"col_heading level1 col5\" >Last</th>        <th class=\"col_heading level1 col6\" >Current</th>        <th class=\"col_heading level1 col7\" >Last</th>        <th class=\"col_heading level1 col8\" >Current</th>        <th class=\"col_heading level1 col9\" >Last</th>        <th class=\"col_heading level1 col10\" >Current</th>        <th class=\"col_heading level1 col11\" >Last</th>        <th class=\"col_heading level1 col12\" >Current</th>        <th class=\"col_heading level1 col13\" >Last</th>        <th class=\"col_heading level1 col14\" >Current</th>        <th class=\"col_heading level1 col15\" >Last</th>        <th class=\"col_heading level1 col16\" >Current</th>        <th class=\"col_heading level1 col17\" >Last</th>    </tr></thead><tbody>\n",
              "        </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f7cf1ee7550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwPSUPdzBeJc"
      },
      "source": [
        "Check Diesel Charges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuSZfrykAUsQ",
        "outputId": "d5da9ed4-a824-4c82-9edd-a403d3d39fe7"
      },
      "source": [
        "uip_vf_legacy_sites = [i for i in uip_vf[uip_vf['BTS site applicable charge (Annual)'].notnull()]['Site_ID']]\n",
        "uip_vf_diesel_sites = [i for i in uip_vf[uip_vf['Energy: Diesel Charges'].notnull()]['Site_ID']]\n",
        "\n",
        "actives_tw_sites = [i for i in towerdb[(towerdb[tw_status]=='In service') | (towerdb[tw_status]=='Dismantled / Active Lease')][tw_index]]\n",
        "print(len(uip_vf_legacy_sites))\n",
        "print(len(uip_vf_diesel_sites))\n",
        "#legacies_vf_not_active_or_dismantled = [i for i in uip_vf_legacy_sites if i not in actives_tw_sites]\n",
        "print(len(legacies_vf_not_active_or_dismantled))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yAZG7ssprSW",
        "outputId": "11c50efb-3b4f-48c3-a9e4-6d0302d4db43"
      },
      "source": [
        "vf_special"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21HkBL38Fqbp",
        "outputId": "dc0c60e0-ed66-4313-b781-29e36df0e70b"
      },
      "source": [
        "charge = uip_vf['BTS site applicable charge (Annual)'].notnull().isin(actives_tw_sites)\n",
        "ind = uip_vf.index[charge==True]\n",
        "ind"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([], dtype='int64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDndpyDrJNhj",
        "outputId": "d3be6dcb-58fc-40d9-c04c-1ca8a30f9c8d"
      },
      "source": [
        "diesel = uip_vf['Energy: Diesel Charges'].notnull().isin(actives_tw_sites)\n",
        "ind = uip_vf.index[diesel==True]\n",
        "ind"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([], dtype='int64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llv9PF9LlJLA"
      },
      "source": [
        "log_dfs = {'Bill Dates Errors': df_dates_bill_errors, \n",
        "'DOER Dates Errors': df_dates_doer_errors, \n",
        "'General Picklist Errors': df_picklist_general\n",
        "'Actives Picklist Errors': df_picklist,\n",
        "'Month BTS Check': df_mom_bts,\n",
        "'Month Decomissioned Check': df_mom_decom,\n",
        "'VF - New Sites':new_sites_vf,\n",
        "'VF - Demerged Date Error': df_bts_demerged_vf,\n",
        "'WH - New Sites': new_siteswh,\n",
        "'WH - Demerged Date Error': bts_sites_out_uis_wh,\n",
        "'VF - DOER Dates Erros': df_doer_vf_on,\n",
        "'WH - DOER Dates Erros': df_doer_wh_on,\n",
        "'VF - DOER Plann Dates Error': df_doer_vf_plann,\n",
        "'WH - DOER Plann Dates Error': df_doer_wh_plann,\n",
        "'VF - Decom Dates Errors': df_doer_vf,\n",
        "'WH - Decom Dates Errors': df_doer_wh,\n",
        "'VF(UIS not active in Twdb)': in_service_uip_sites_vf,\n",
        "'VF(twdb Sites out UIS)' : in_service_not_in_uis_vf,\n",
        "'VF(Decom sites in UIS)' : decomiss_sites_in_uip_vf,\n",
        "'VF(BTS\\'s out UIS File)': bts_sites_out_uip_vf,\n",
        "'VF(Critical out UIS)':critical_vf,\n",
        "'WH(UIS not active in Twdb)': uis_sites_not_in_towerdbwh,\n",
        "'WH(twdb Sites out UIS)' : in_service_not_in_uiswh,\n",
        "'WH(Decom sites in UIS)' : decomiss_sites_in_uiswh,\n",
        "'WH(BTS\\'s out UIS File)': bts_sites_out_uipwh,\n",
        "'WH(Critical out UIS)': critical_wh}\n",
        "\n",
        "dfs = [in_service_not_in_uiswh, in_service_not_in_uis, uis_sites_not_in_towerdb]\n",
        "names = ['WH In Serv Sites not UIS', 'VF In Serv Sites not UIS', 'VF UIS Sites not in TwDB']\n",
        "\n",
        "general_log_erros(dfs, names, '/content/errors_wh_vf.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91g-sRUVFDIp"
      },
      "source": [
        "df_tw = [df_dates_doer_errors, df_picklist_general, bts_sites_out_uis_v, bts_sites_out_uis_wh]\n",
        "sheetnames_tw = ['DOER Dates_Errors', 'Picklist Invalid Value General', 'VF_BTS_OUT_UIS', 'WH_BTS_OUT_UIS']\n",
        "\n",
        "path_tw = '/content/TWDB_GR_In-Month_Errors_20210803.xlsx'\n",
        "general_log_erros(df_tw, sheetnames_tw, path_tw)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}