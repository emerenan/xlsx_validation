{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cz_validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfuV39szWSni+xiQuS91/o",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emerenan/xlsx_validation/blob/main/cz_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDANxxNd5efG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ef194f-7cff-45d0-cbac-3d91f5abe05c"
      },
      "source": [
        "!pip install unidecode\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def change_incorret(df, df_index, col_chg, incorrect_value, new_value):\n",
        "    list_sites = list(df[df[col_chg]==incorrect_value][tw_index])\n",
        "    df.loc[df[df_index].isin(list_sites), col_chg] = new_value\n",
        "    #print(df.loc[df[df_index].isin(list_sites)][[df_index, col_chg]])\n",
        "       "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 241 kB 6.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXC51sanKFX5"
      },
      "source": [
        "# Reading and Fit Excel File\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSFDJzKG0c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "fc12f7dd-1527-436f-cc4f-22a16d28d9ad"
      },
      "source": [
        "# Read CZ towerdb files\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index, cols_number, col_dates, format='normal', type_date=\"%d/%m/%Y\"):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ']\n",
        "        #lista = []\n",
        "\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(0)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df\n",
        "\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) and not isinstance(date_obj, str) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    date_parser(df, col_dates, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_number)\n",
        "\n",
        "    for col in cols_number:\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "    \n",
        "    df = df.fillna('')\n",
        "\n",
        "    return df\n",
        "\n",
        "path_towerdb = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "towerdb_col = lower_str([\"Code (Duplicate)\",\"Site Status\",\"VF - In scope / out of scope (Generalised scoping)\",\"Site in Skylon scope Actual (From Site List Sheet )\",\\\n",
        "               \"Legacy Site Code(Duplicate)\",\"TIMS Site Code\",\"Legacy Site Code\",\"Site Name\",\"Macro Region\",\"Province\",\"Municipality\",\"Inhabitants\",\"Address\",\\\n",
        "               \"Ground Register\",\"Altitude\",\"Latitude\",\"Longitude\",\"Categorization by Inhabitants\",\"Categorization by Transmission Sys\",\\\n",
        "               \"Categorization by Site Type\",\"Categorization by Transmission Sys (subcluster)\",\"Other internal Categorization 1 (Identify ACQ Sites)\",\\\n",
        "               \"Other internal Categorization 2 Energy provider (Eon/ LL)\",\"DAS+Macro\",\"DAS (Yes/ No)\",\"DAS Ownership (Complete/ Partial/ 3rd Party)\",\\\n",
        "               \"Active/ Passive DAS\",\"# of remote units/ radiating points\",\"Type of Structure\",\"Distance highest antenna to ground level\",\"GBT Tower height\",\\\n",
        "               \"POD ID\",\"Energy Consumption LTM (kwh)\",\"Annual Energy cost LTM (Euros)\",\"Infrastructure ready (existing)/ to be ready (new)\",\\\n",
        "               \"Infrastructure to be dismantled by\",\"Radio equipments to be deactivated by\",\"Infrastructure to be shared by\",\"Technology VOD\",\"Fibre / Microwave\",\\\n",
        "               \"Vertical passive structure owner\",\"Room configuration (detailed)\",\"Shelter passive structure ownership\",\"Type of Air Conditioning\",\\\n",
        "               \"Number of cabinets (Full Capacity)\",\"Number of Antenna (Full Capacity)\",\"Number of MW (Full Capacity)\",\"Counterpart\",\"# of Lease Contracts\",\\\n",
        "               \"Current annual lease fees \",\"Current other fees (Maintenance)\",\"Current other fees\",\"(Average) residual duration - Lease contract\",\\\n",
        "               \"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\"# of Tenants Agreements\",\\\n",
        "               \"Current Total Annual Hosting Fees\",\"Tenant (name/ID) MNO1 (Česká telekomunikační infrastruktura a.s.)\",\"Annual Fee per Tenant MNO1\",\\\n",
        "               \"Annual Energy Fee MNO1\",\"Annual Maintenance Fee MNO1\",\"Other Services Fee MNO1\",\"Residual duration MNO1 (Years)\",\\\n",
        "               \"Tenant (name/ID) MNO2 (T-Mobile Czech Republic a.s.)\",\"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\"Annual Maintenance Fee MNO2\",\\\n",
        "               \"Other Services Fee MNO2\",\"Residual duration MNO2 (days)\",\"Tenant (name/ID) MNO3\",\"Annual Fee per Tenant MNO3\",\"Annual Energy Fee MNO3\",\\\n",
        "               \"Annual Maintenance Fee MNO3\",\"Other Services Fee MNO3\",\"Residual duration MNO3\",\"# of OTMOs\",\"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\\\n",
        "               \"Annual Maintenance Fee OTMOs\",\"Other Services Fee OTMOs\",\"Average residual duration (days)\",\"Check\",\"Strategic Macro Sites\",\"Critical Sites\",\\\n",
        "               \"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\"Transmission Sites\",\\\n",
        "               \"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\"VF-CZ Demerger phase\",\\\n",
        "               \"EVO Location [FAR Site ID] \",\"Billing Trigger date \",\"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\\\n",
        "               \"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\"Wip_Site\",\\\n",
        "               \"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"])\n",
        "\n",
        "dates = ['Billing Trigger date ', \"Infrastructure ready (existing)/ to be ready (new)\", \"Infrastructure to be dismantled by\",\\\n",
        "         'First_Active_Sharing_Start_Date','First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "         'Date_Of_Equipment_Removal']\n",
        "num_parse = [\"Inhabitants\", \"Current annual lease fees \",\"(Average) residual duration - Lease contract\",\\\n",
        "             \"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\\\n",
        "             \"# of Tenants Agreements\",\"Current Total Annual Hosting Fees\", \"Annual Fee per Tenant MNO1\",\\\n",
        "             \"Annual Energy Fee MNO1\", \"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\\\n",
        "             \"Residual duration MNO2 (days)\", \"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\\\n",
        "             \"Other Services Fee OTMOs\",\"Average residual duration (days)\"]\n",
        "tab = 'Tower DB'\n",
        "row = 3\n",
        "towerdb = read_files(path_towerdb,tab, 3, 0,\"Code (Duplicate)\",num_parse,dates)\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "towerdb = towerdb[towerdb_col]\n",
        "\n",
        "path_msa = '/content/TowerDB_CzechRepublic_20210731 (3).csv'\n",
        "msa = pd.read_csv(path_msa,encoding='ISO-8859-1').fillna('')\n",
        "msa.columns = lower_str(list(msa.columns))\n",
        "msa = msa.rename(columns={'ï»¿code (duplicate)': 'code (duplicate)',\n",
        "                          'tenant (name/id) mno1 (ä\\x8ceskã¡ telekomunikaä\\x8dnã\\xad infrastruktura a.s.)': 'tenant (name/id) mno1 (česká telekomunikační infrastruktura a.s.)'})\n",
        "msa_cols = list(msa.columns)\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "\"\"\"Check columns received\"\"\"\n",
        "df_cols = check_columns_received(towerdb, msa_cols)\n",
        "df_cols\n",
        "#No colum"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column(s) Missing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Column(s) Missing]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT7nbdJzTFXm"
      },
      "source": [
        "def change_incorret(df, df_index, col_chg, incorrect_value, new_value):\n",
        "    list_sites = list(df[df[col_chg]==incorrect_value][tw_index])\n",
        "    df.loc[df[df_index].isin(list_sites), col_chg] = new_value\n",
        "    print(df.loc[df[df_index].isin(list_sites)][[df_index, col_chg]])\n",
        "\n",
        "change_incorret(towerdb, tw_index, 'subsequent_sharing_arrangement', 'NO', 'No')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoqB1yjZ0n1r"
      },
      "source": [
        "towerdb.to_excel('/content/TowerDB_CzechRepublic_20210831.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKLpXMuub_65"
      },
      "source": [
        "towerdb.to_csv('/content/TowerDB_CzechRepublic_20210831.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhdRKjcRBC4b"
      },
      "source": [
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = \"code (duplicate)\"\n",
        "tw_doer = \"date_of_equipment_removal\"\n",
        "tw_status = \"site status\"\n",
        "tw_bts = 'bts_site'\n",
        "tw_bill = \"billing trigger date \"\n",
        "tw_wip = 'wip_site'\n",
        "tw_decom = 'decommissioned_site'\n",
        "tw_critical = 'critical sites'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__VBNgJTxRTD"
      },
      "source": [
        "Ver essa necessidade de unidecode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BXkWi_-4P3v"
      },
      "source": [
        "path_msa = '/content/TowerDB_CzechRepublic_20210731 (3).csv'\n",
        "msa = pd.read_csv(path_msa,encoding='ISO-8859-1').fillna('')\n",
        "cols_con = ['Province','Municipality','Address']\n",
        "for col in cols_con:\n",
        "   msa[col] = msa[col].astype(str).apply(unidecode)\n",
        "msa = msa.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "msa.columns = lower_str(towerdb_col)\n",
        "msa.head(1)\n",
        "#msa.columns = msa_cols\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT75KHTjUyMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "fc4408c1-8838-4419-f9b5-5bdadef026d0"
      },
      "source": [
        "# Check Dates Format (dd-mm-YYYY)\n",
        "\"\"\"Não deve haver NAN ou NAT na coluna, fazer fillna('') antes\"\"\"\n",
        "def check_date_columns(df, df_index,status_col, df_bts, df_wip, columns):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "\n",
        "    df_dates = df[columns].fillna('')\n",
        "    df_dates = df_dates.to_dict(orient='list')\n",
        "    #df_dates['sites'] = df_dates[df_index]\n",
        "    #df_dates = df_dates.set_index('sites')\n",
        "    new_dic = defaultdict(list)\n",
        "\n",
        "    date_format = re.compile(r\"[0-9]{1,2}[\\_|\\-|\\/][0-9]{1,2}[\\_|\\-|\\/][0-9]{1,4}\")\n",
        "\n",
        "    for column in set(df_dates.keys()):\n",
        "        for value in df_dates[column]:\n",
        "            if date_format.match(value) == None:\n",
        "                if value in (None, '', 'nan'):\n",
        "                    new_dic[column].append('Blank Value')\n",
        "                else:\n",
        "                    new_dic[column].append(f'Incorret picklist value: {value}')\n",
        "            else:\n",
        "                new_dic[column].append('Ok')\n",
        "\n",
        "    df_errors = pd.DataFrame(new_dic)\n",
        "    df_errors[df_index] = [i for i in df[df_index]]\n",
        "    df_errors = df_errors.replace('Ok', np.nan)   \n",
        "    df_errors = df_errors.set_index(df_index)\n",
        "    df_errors = df_errors.dropna(how='all', axis=0)\n",
        "    df_errors = df_errors.dropna(how='all', axis=1)\n",
        "    df_errors = df_errors.reset_index()\n",
        "    \n",
        "    df = df[[df_index, status_col, df_bts, df_wip]]\n",
        "    df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "    df_errors = df_errors.set_index(df_index)\n",
        "    df_errors = df_errors[[status_col,df_bts, df_wip]+ df_errors.columns[:-3].tolist()]\n",
        "    df_errors = df_errors.reset_index()\n",
        "    return df_errors\n",
        "\n",
        "dates_cols = lower_str(['Billing Trigger date ', 'First_Active_Sharing_Start_Date',\\\n",
        "              'First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "              'Date_Of_Equipment_Removal'])\n",
        "\n",
        "#date_parser(towerdb, lower_str(dates), 2, 1, 'mixed')\n",
        "#tw_in_service = tw_in_service[dates_cols]\n",
        "tw_in_service = towerdb[towerdb['site status']=='In Service']\n",
        "df_dates_errors = check_date_columns(tw_in_service,tw_index, tw_status, tw_bts, tw_wip, dates_cols)\n",
        "df_dates_errors\n",
        "#Errors em varias colunas"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code (duplicate)</th>\n",
              "      <th>site status</th>\n",
              "      <th>bts_site</th>\n",
              "      <th>wip_site</th>\n",
              "      <th>date_of_equipment_removal</th>\n",
              "      <th>first_active_sharing_start_date</th>\n",
              "      <th>first_active_sharing_end_date</th>\n",
              "      <th>billing trigger date</th>\n",
              "      <th>transfer_date_of_consent_required_sites</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BADA1G</td>\n",
              "      <td>In Service</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>240190</td>\n",
              "      <td>In Service</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BBCV2G</td>\n",
              "      <td>In Service</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>221280</td>\n",
              "      <td>In Service</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>112610</td>\n",
              "      <td>In Service</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3904</th>\n",
              "      <td>750210</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3905</th>\n",
              "      <td>41690X</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3906</th>\n",
              "      <td>226750</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3907</th>\n",
              "      <td>259220</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3908</th>\n",
              "      <td>241690</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3909 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     code (duplicate)  ... transfer_date_of_consent_required_sites\n",
              "0              BADA1G  ...                                     NaN\n",
              "1              240190  ...                                     NaN\n",
              "2              BBCV2G  ...                                     NaN\n",
              "3              221280  ...                             Blank Value\n",
              "4              112610  ...                                     NaN\n",
              "...               ...  ...                                     ...\n",
              "3904           750210  ...                             Blank Value\n",
              "3905           41690X  ...                             Blank Value\n",
              "3906           226750  ...                             Blank Value\n",
              "3907           259220  ...                             Blank Value\n",
              "3908           241690  ...                             Blank Value\n",
              "\n",
              "[3909 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtUVfvVPdP76"
      },
      "source": [
        "Validaton for general sites picklist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUXH4ckBaL4Q"
      },
      "source": [
        "def check_picklist_v1(df,df_index,df_status, df_cols, picklist_dict):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    df_picklist = df[df_cols].fillna('')\n",
        "    df_picklist = df_picklist.to_dict(orient='list')\n",
        "\n",
        "    new_dic = defaultdict(list)\n",
        "    \"\"\"for id in df_picklist[df_index]:\n",
        "        if id not in new_dic.keys():\n",
        "            new_dic[df_index].append(id)\"\"\" \n",
        "            \n",
        "    for column in set(picklist_dict.keys()):\n",
        "        for value in df_picklist[column]:\n",
        "            if value not in set(picklist_dict[column]): \n",
        "                if pd.isnull(value) or pd.isna(value) or value=='' or value=='nan':\n",
        "                    new_dic[column].append('Blank Value')\n",
        "                else:\n",
        "                    new_dic[column].append(f'Incorrect value: {value}')\n",
        "            else:\n",
        "                new_dic[column].append('Ok!')\n",
        "\n",
        "    df_errors = pd.DataFrame(new_dic)\n",
        "    df_errors = df_errors.replace('Ok!', np.nan)\n",
        "    df_errors[df_index] = df_picklist[df_index]\n",
        "    df_errors = df_errors.set_index(df_index)\n",
        "    df_errors = df_errors.dropna(how='all', axis=0)\n",
        "    df_errors = df_errors.dropna(how='all', axis=1)\n",
        "    df_errors = df_errors.reset_index()\n",
        "\n",
        "    df = df[[df_index, df_status]]\n",
        "    df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "    df_errors = df_errors.set_index(df_index)\n",
        "    df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "    #df_errors = df_errors.reset_index()\n",
        "    return df_errors\n",
        "\n",
        "dic_gen = ['code (duplicate)', 'site status', 'categorization by transmission sys']\n",
        "pick_gen = {\n",
        "    'categorization by transmission sys': ['Macro','Public DAS','Repeater','Transmission']\n",
        "    }\n",
        "df_pick_general_errors = check_picklist_v1(towerdb, tw_index, tw_status, dic_gen, pick_gen)\n",
        "df_pick_general_errors\n",
        "#Pipeline sites without categorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdgc55EAdJZV"
      },
      "source": [
        "Validation for On air sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-cTM_dnU0Gk"
      },
      "source": [
        "picklist_cz = {\n",
        "    'categorization by transmission sys': ['Macro','Public DAS','Repeater','Transmission'],\n",
        "    'categorization by site type':['DAS passive','GBT','RTT'],\n",
        "    'strategic macro sites':['Yes','No'],\n",
        "    'critical sites':['Yes','No'],\n",
        "    'case a core site':['Non Core', 'Case A','Case B'],\n",
        "    'macro site - transmission hub site':['Yes','No'],\n",
        "    'macro site - transmission hub site with/without shelters': ['With shelters','Without shelters','Non Transmission Hub Site'],\n",
        "    'transmission sites': ['No','Yes'],\n",
        "    'room configuration': ['Indoor','Outdoor',\"Indoor & Outdoor\"],\n",
        "    'power supply':['AC','DC','AC & DC','No Power'],\n",
        "    'air conditioning': ['No','Yes; Indoor Air Conditioning','Yes; Indoor Free Air cooling / Free cooling units'],\n",
        "    'active sharing arrangements involving the operator':['MORAN (On VF equipment)','MORAN (On non-VF equipment)','MOCN with Spectrum Pooling','Partial Active-Passive','No Active Sharing'],\n",
        "    'vf-cz demerger phase': ['Cancelled','Phase 1','Phase 2','Skylon BTS'],\n",
        "    'strategic_site_bucket': ['Yes - 0-5%','Yes - 5-10%','Non Strategic'],\n",
        "    'critical site bucket': ['Beyond 10%','Within 10%','Non Critical'],\n",
        "    'subsequent_sharing_arrangement': ['Yes','No'],\n",
        "    'decommissioned_site': ['Yes','No'],\n",
        "    'wip_site': ['Yes','No'],\n",
        "    'bts_site': ['Yes','No'],\n",
        "    'sites_as_metered_estimated': ['Estimated Model','Metered Model']\n",
        "}\n",
        "dic_cols = [tw_index,'categorization by transmission sys',\\\n",
        "            'categorization by site type',\\\n",
        "            'strategic macro sites',\\\n",
        "            'critical sites',\\\n",
        "            'case a core site',\\\n",
        "            'macro site - transmission hub site',\\\n",
        "            'macro site - transmission hub site with/without shelters',\\\n",
        "            'transmission sites',\\\n",
        "            'room configuration',\\\n",
        "            'power supply',\\\n",
        "            'air conditioning',\\\n",
        "            'active sharing arrangements involving the operator',\\\n",
        "            'vf-cz demerger phase',\\\n",
        "            'strategic_site_bucket',\\\n",
        "            'critical site bucket',\\\n",
        "            'subsequent_sharing_arrangement',\\\n",
        "            'decommissioned_site',\\\n",
        "            'wip_site',\\\n",
        "            'bts_site',\\\n",
        "            'sites_as_metered_estimated']\n",
        "df_pick_actives_errors = check_picklist_v1(tw_in_service, tw_index, tw_status, dic_cols, picklist_cz)\n",
        "df_pick_actives_errors\n",
        "#Erros em varias colunas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWLLVcK3cPRZ"
      },
      "source": [
        "Month-on-Month BTS sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "iQJPPtAXZiJr",
        "outputId": "ba7f0e7d-2331-47de-fe00-7d5588eb7203"
      },
      "source": [
        "def check_mom_bts(df_tw, tw_index,status_col, tw_col, df_msa, msa_index, msa_col):\n",
        "\n",
        "    msa_bts = [i for i in df_msa[df_msa[msa_col]=='Yes'][msa_index]]\n",
        "\n",
        "    tw_bts = [i for i in df_tw[df_tw[tw_col]=='Yes'][tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts if i not in tw_bts]\n",
        "    filtered = df_tw.loc[df_tw[tw_index].isin(out_tower_bts)]\n",
        "\n",
        "    return filtered[[tw_index,status_col, tw_col]]    \n",
        "        \n",
        "tw_filtered = towerdb[towerdb['site status']=='In Service']\n",
        "\n",
        "df_mom_bts = check_mom_bts(tw_filtered, tw_index, tw_status, tw_bts, msa, tw_index, tw_bts)\n",
        "df_mom_bts\n",
        "\n",
        "#df_mom_decom = check_mom_bts(tw_filtered, tw_index,tw_status, tw_decom, msa, tw_index, tw_decom)\n",
        "#df_mom_decom\n",
        "# No errors in Both"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code (duplicate)</th>\n",
              "      <th>site status</th>\n",
              "      <th>decommissioned_site</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [code (duplicate), site status, decommissioned_site]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vv5LQ99m5mc"
      },
      "source": [
        "#Read UIP File\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip = pd.read_excel('/content/UserInput_CzechRepublic_20210831.xlsx',sheet_name='SiteLevel',usecols=[0,2,3],skiprows=3).fillna('')\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [i for i in msa[tw_index]]\n",
        "tw_sites = [str(i) for i in towerdb[tw_index]]\n",
        "uip_sites = [str(i) for i in uip['Site_ID']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQSv5rvy6Mev"
      },
      "source": [
        "Verificar Errors de Sites Numericos com String\n",
        "Alguns sites estão com Integer ou String no ficheiro anterior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3NGucZF02V4"
      },
      "source": [
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [i for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites['New_Sites'] =  new_sites['New_Sites'].astype(int)\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    \n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uis = [i for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index] if i not in uip_list]\n",
        "    bts_out_uis = pd.DataFrame(bts_out_uis, columns=['Bts_Sites_Out_UIS_File'])\n",
        "    bts_out_uis = pd.merge(bts_out_uis, filtered, how='left', left_on=['Bts_Sites_Out_UIS_File'], right_on=tw_index)\n",
        "    bts_out_uis = bts_out_uis[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    #print(current_date)\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "\n",
        "    return new_sites, bts_out_uis, df_site_bts[[tw_index,status_col, bts_col, bill_col]]\n",
        "\n",
        "new_sites, bts_sites_out_uis, df_bts_demerged = check_new_sites(towerdb, tw_index, tw_bts, tw_bill, tw_status, msa_sites, tw_sites, uip_sites)\n",
        "\n",
        "print(new_sites)\n",
        "print('\\n')\n",
        "print(bts_sites_out_uis)\n",
        "print('\\n')\n",
        "print(df_bts_demerged)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROTHG9-NqPcD"
      },
      "source": [
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, df_msa, msa_index, msa_wip):\n",
        "\n",
        "    wip_msa = [i for i in df_msa[df_msa[msa_wip]=='Yes'][msa_index]]\n",
        "    \n",
        "    tw_wip_sites = [str(i) for i in df_tw[df_tw[wip_tw]=='Yes'][tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, wip_tw, tw_bts]]\n",
        "    \n",
        "    wip_out_tw_list = [i for i in tw_wip_sites if i not in wip_msa]\n",
        "\n",
        "    return tw_wip_site_bts_flagged\n",
        "\n",
        "    # Falta os outros países\n",
        "tw_wip_site_bts_flagged = check_wip(towerdb, tw_index, tw_wip, tw_bts, msa, tw_index, tw_wip)\n",
        "\n",
        "print(tw_wip_site_bts_flagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6HAiaPFrV6F"
      },
      "source": [
        "Validation DOER sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68rdgmyprVgu",
        "outputId": "3e5f25e4-c686-4fc3-a865-d828d136d126"
      },
      "source": [
        "#Validate Decomissioned Sites\n",
        "def check_decommissioned(df,df_index,status_col, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "\n",
        "    return filtered[[df_index,status_col, decom_col, doer_col]]\n",
        "\n",
        "\n",
        "df_decom_errors = check_decommissioned(towerdb,tw_index, tw_status, tw_decom, tw_doer)\n",
        "df_decom_errors\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onf4s48gr6Yw"
      },
      "source": [
        "DOER Valitadions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bsHWICor963"
      },
      "source": [
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    df_tw = df_tw[df_tw[status_col]==status][[tw_index, status_col, date_col]] \n",
        "    #print(df_tw)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[df_tw[date_col].astype('datetime64[ns]') < current_date]\n",
        "            return filtered\n",
        "        else:\n",
        "            #bill_dates = pd.to_datetime(df_tw[date_col], errors==coerrce)\n",
        "            filtered = df_tw[df_tw[date_col]=='']\n",
        "            return filtered\n",
        "    else:\n",
        "        print('\\nNo errors founded!\\n')\n",
        "actives  = towerdb[towerdb[tw_status]=='In Service']\n",
        "#Validate sites in service which has DOER less than current date\n",
        "\n",
        "df_doer_errors = check_tw_bill_doer(actives, tw_index, tw_doer, tw_status, 'In Service', 'doer')\n",
        "df_doer_errors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzT8BJ8NtF9j"
      },
      "source": [
        "TowerDb vs UIS validations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYjPay1Rx7OD"
      },
      "source": [
        "UIS In Month"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80qFRKGovOKf"
      },
      "source": [
        "\"\"\"Check UIP In Month ites matches with Towerdb sites\"\"\"\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [str(i) for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "\n",
        "    uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS In Month not active in TowerDB!'])\n",
        "    #uis_sites_not_in_towerdb['UIS In Month not active in TowerDB!'] =  uis_sites_not_in_towerdb['UIS In Month not active in TowerDB!'].astype(int)\n",
        "    uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS In Month not active in TowerDB!',\\\n",
        "                                    right_on=tw_index)\n",
        "    uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS In Month not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS In Month!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS In Month!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS In Month!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "    decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "    decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                    right_on=tw_index)\n",
        "    decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "\n",
        "    #Check BTS sites\n",
        "    bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "    uip_bts = [i for i in df_uip[df_uip['BTS site applicable charge (Annual)']!='']['Site_ID']]\n",
        "\n",
        "    #if not set(bts_sites).intersection(uip_sites):\n",
        "    bts_sites_out_uip = [i for i in uip_bts if i not in bts_sites]\n",
        "    bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['UIS BTS not in TowerDB(BTS)'])\n",
        "    #bts_sites_out_uip['UIS BTS not in TowerDB(BTS)'] = bts_sites_out_uip['UIS BTS not in TowerDB(BTS)'].astype(int)\n",
        "    bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='UIS BTS not in TowerDB(BTS)',\\\n",
        "                                    right_on=tw_index)\n",
        "    bts_sites_out_uip = bts_sites_out_uip[['UIS BTS not in TowerDB(BTS)', status_tw_col]]\n",
        "\n",
        "    #  Check for UIP critical sites \n",
        "    uip_critical = [i for i in df_uip[df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='']['Site_ID']]\n",
        "    bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "\n",
        "    critical = [i for i in uip_critical if i not in bts_tw_critical]\n",
        "    critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "    critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                right_on=tw_index)\n",
        "    critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "    #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "    return uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "uis_sites_not_in_towerdb,in_service_uis_sites, decomiss_sites_in_uis, bts_sites_out_uis, critical = check_uip_tw(towerdb,tw_index, tw_status, \\\n",
        "                                                                tw_decom, tw_bts,tw_critical, uip, uip_sites)\n",
        "\n",
        "print(uis_sites_not_in_towerdb)\n",
        "print('\\n')\n",
        "print(in_service_uis_sites)\n",
        "print('\\n')\n",
        "print(decomiss_sites_in_uis)\n",
        "print('\\n')\n",
        "print(bts_sites_out_uis)\n",
        "print('\\n')\n",
        "print(critical)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxo8qoPHx2aD"
      },
      "source": [
        "UIS True UP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8UcruIH8ikt"
      },
      "source": [
        "\"\"\"Check UIP In Month ites matches with Towerdb sites\"\"\"\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [str(i) for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "\n",
        "    uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS In Month not active in TowerDB!'])\n",
        "    #uis_sites_not_in_towerdb['UIS In Month not active in TowerDB!'] =  uis_sites_not_in_towerdb['UIS In Month not active in TowerDB!'].astype(int)\n",
        "    uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS In Month not active in TowerDB!',\\\n",
        "                                    right_on=tw_index)\n",
        "    uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS In Month not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS In Month!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS In Month!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS In Month!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "    decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "    decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                    right_on=tw_index)\n",
        "    decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "\n",
        "    #Check BTS sites\n",
        "    bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "    uip_bts = [i for i in df_uip[df_uip['BTS site applicable charge (Annual)']!='']['Site_ID']]\n",
        "\n",
        "    #if not set(bts_sites).intersection(uip_sites):\n",
        "    bts_sites_out_uip = [i for i in uip_bts if i not in bts_sites]\n",
        "    bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['UIS BTS not in TowerDB(BTS)'])\n",
        "    #bts_sites_out_uip['UIS BTS not in TowerDB(BTS)'] = bts_sites_out_uip['UIS BTS not in TowerDB(BTS)'].astype(int)\n",
        "    bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='UIS BTS not in TowerDB(BTS)',\\\n",
        "                                    right_on=tw_index)\n",
        "    bts_sites_out_uip = bts_sites_out_uip[['UIS BTS not in TowerDB(BTS)', status_tw_col]]\n",
        "\n",
        "    #  Check for UIP critical sites \n",
        "    uip_critical = [i for i in df_uip[df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='']['Site_ID']]\n",
        "    bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "\n",
        "    critical = [i for i in uip_critical if i not in bts_tw_critical]\n",
        "    critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "    critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                right_on=tw_index)\n",
        "    critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "    #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "    return uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "#Read UIP File\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip_true = pd.read_excel('/content/UserInput_CzechRepublic_20210630 TrueUp.xlsx',sheet_name='SiteLevel',usecols=[0,2,3],skiprows=3).fillna('')\n",
        "uip_true.columns = uip_names\n",
        "\n",
        "uip_true_sites = [str(i) for i in uip['Site_ID']]\n",
        "\n",
        "uis_sites_not_in_towerdb_tu , in_service_uis_sites_tu, decomiss_sites_in_uis_tu, bts_sites_out_uis_tu, critical_tu = check_uip_tw(towerdb,tw_index, tw_status, \\\n",
        "                                                                tw_decom, tw_bts,tw_critical, uip_true, uip_true_sites)\n",
        "\n",
        "print(uis_sites_not_in_towerdb_tu)\n",
        "print('\\n')\n",
        "print(in_service_uis_sites_tu)\n",
        "print('\\n')\n",
        "print(decomiss_sites_in_uis_tu)\n",
        "print('\\n')\n",
        "print(bts_sites_out_uis_tu)\n",
        "print('\\n')\n",
        "print(critical_tu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKOd5LyjuMVc"
      },
      "source": [
        "Commercial Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BClu4PcvuOEo"
      },
      "source": [
        "def check_diffs_v2(path_current, path_last, sheet='Commercial'):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def highlight_diff(data, color='yellow'):\n",
        "        attr = 'background-color: {}'.format(color)\n",
        "        other = data.xs('Current', axis='columns', level=-1)\n",
        "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                            index=data.index, columns=data.columns)\n",
        "\n",
        "    _actual = pd.read_excel(path_current,sheet_name=sheet).fillna('')\n",
        "    _before = pd.read_excel(path_last,sheet_name=sheet).fillna('')\n",
        "\n",
        "    df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "    df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "    return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "\n",
        "path_current = '/content/UserInput_CzechRepublic_20210831.xlsx'\n",
        "path_last = '/content/UserInput_CzechRepublic_20210731.xlsx'\n",
        "df_comm_errors = check_diffs_v2(path_current, path_last,sheet='Commercial')\n",
        "df_comm_errors\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__UxAcFX7JHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d57dfe-7e0e-43c2-e6bd-2b14a6fd5895"
      },
      "source": [
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "        #lista = []\n",
        "\n",
        "        df.fillna(\"\", inplace=True)\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df\n",
        "\n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    #df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    if not df_commercial_diffs.empty:\n",
        "        return df_commercial_diffs\n",
        "    else:\n",
        "        print('\\nNo Errors founded!')\n",
        "\n",
        "names = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', 'MSA Sites (Phase 1)\\nInput_Value',\\\n",
        "        'PMA Sites (Phase 2)\\nInput_Value','Description/Instruction', 'Frequency of Update']\n",
        "merge_cols = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', \\\n",
        "              'Description/Instruction', 'Frequency of Update']\n",
        "cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Data_Type','Input_Value_1_actual',\\\n",
        "                'Input_Value_1_before','Input_Value_2_actual','Input_Value_2_before','Equal Values','Description/Instruction', 'Frequency of Update']\n",
        "# Check for commercial Values into current UIP File and compare with UIP File before\n",
        "\n",
        "df_com = check_commercial(path_current, path_last,['MSA Sites (Phase 1)\\nInput_Value','PMA Sites (Phase 2)\\nInput_Value'], names, merge_cols, cols_ordered)\n",
        "df_com\n",
        "# No Errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors founded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRJDetTx6r31"
      },
      "source": [
        "def general_log_erros(df_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')  \n",
        "    for lista in df_list:\n",
        "        try:\n",
        "            if not lista[1].empty:\n",
        "                lista[1].to_excel(writer, sheet_name=lista[0], index=False)\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        except:\n",
        "            if lista[1] is not None and lista[1].is_valid():\n",
        "                lista[1].to_excel(writer, sheet_name=lista[0], index=False)\n",
        "    try: \n",
        "        writer.save() \n",
        "    except IndexError:\n",
        "        print('No one log to write!')\n",
        "        \n",
        "logs = [['Actives Dates Columns Errors', df_dates_errors],\n",
        "['General Picklist Errors', df_pick_general_errors],\n",
        "['Active Picklist Errors', df_pick_actives_errors],\n",
        "['MSA BTS not in TowerDB', df_mom_bts],\n",
        "['New Sites',  new_sites],\n",
        "['BTS Billing trigger Date Errors', df_bts_demerged],\n",
        "['BTS and WIP Flagged Errors',tw_wip_site_bts_flagged],\n",
        "['Decomm Sites Dates Errors', df_decom_errors],\n",
        "['DOER Errors', df_doer_errors],\n",
        "['UIS Sites not active in TowerDB', uis_sites_not_in_towerdb],\n",
        "['TowerDB Sites out of UIS', in_service_uis_sites],\n",
        "['Decom Sites In UIS', decomiss_sites_in_uis],\n",
        "['UIS BTS not in TowerDB(BTS)', bts_sites_out_uis],\n",
        "['Sites Beyond 10% out UIS', critical],\n",
        "['UIS(True Up) not active in TowerDB', uis_sites_not_in_towerdb_tu],\n",
        "['TowerDB\\'s out of UIS(True up)', in_service_uis_sites_tu],\n",
        "['Decom\\'s In UIS(True Up)', decomiss_sites_in_uis_tu],\n",
        "['UIS(True up) not in TowerDB(BTS)', bts_sites_out_uis_tu],\n",
        "['Sites Beyond 10% out UIS(True)', critical_tu],\n",
        "['Comercial Differences', df_comm_errors]]\n",
        "\n",
        "general_log_erros(logs, '/content/Towerdb_CZ_Errors.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmEVTURV2gCK"
      },
      "source": [
        "Create CSV File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxxPNjgByHcw"
      },
      "source": [
        "Script to make TA Input file validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOREPa3GfFb_"
      },
      "source": [
        "#Read TA Input File\n",
        "def read_files(path, sheetname, col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int, cols_amount, bill_cols, \\\n",
        "               format='mix', type_date=\"%d/%m/%Y\"):\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "        \n",
        "    def replace_values_am(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df \n",
        "\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname,header=0, names = col_name, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values_am(df, cols_amount, 0)\n",
        "    for col in cols_amount:\n",
        "        #df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f' {x:,} ')\n",
        "\n",
        "    for col in cols_int:\n",
        "        df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int)\n",
        "    df = df.fillna('')\n",
        "    replace_values_am(df, bill_cols)\n",
        "    date_parser(df, cols_date, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    return df\n",
        "\n",
        "def replace_values_ta(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ']\n",
        "    #lista = []\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        df[column] = df[column].fillna(0)\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "path_ta_input = '/content/TA_Input_CzechRepublic_20210731.csv'\n",
        "col = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type',\\\n",
        "       'Contract start date', 'Contract end date', 'Status of contract',\\\n",
        "       'Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart',\\\n",
        "       'Classification of Tenant', 'Annual amount - billing currency',\\\n",
        "       'Billing Currency', 'Annual Amount in CZK', 'Amount in EUR',\\\n",
        "       'Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "       'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key',\\\n",
        "       'Classification', 'Residual Period in Years', 'Remarks',\\\n",
        "       'Count of unique key', 'Count of contracts', 'Scoping classification',\\\n",
        "       'DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "       'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x',\\\n",
        "       'SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count','Quota of Unique Key','unamed']\n",
        "\n",
        "#ta = pd.read_csv(path_ta_input, header=0, names=col)\n",
        "dates = ['Contract start date', 'Contract end date', 'Update in month']\n",
        "interger = ['Residual Period in Years','Count of unique key', 'Count of contracts', 'Unique Tenant Count',\\\n",
        "            'Quota of Unique Key']\n",
        "amount = ['Annual Amount in CZK', 'Amount in EUR']\n",
        "ta_bill_cols = ['Contract start date','Contract end date',\\\n",
        "                'Counterpart','Classification of Tenant']\n",
        "ta = read_files('/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx', 'Tenant ',col, 0, 0, 'Code', dates, interger,amount, ta_bill_cols)\n",
        "\n",
        "ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "\n",
        "ta = ta.reindex(columns=ta_cols)\n",
        "ta = ta.rename(columns={'unamed': '',\n",
        "                        'unamed_2': ''})\n",
        "\n",
        "#ta.to_excel('/content/TA_Input_CzechRepublic_20210831.xlsx', index=False)\n",
        "ta.to_csv('/content/TA_Input_CzechRepublic_20210831.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbxbGMi7F9aL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6cf92cb-39ee-4e46-d2f1-37814acf7a21"
      },
      "source": [
        "ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "len(ta_cols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqMPxE4jijDQ"
      },
      "source": [
        "ta_msa = pd.read_csv('/content/TA_Input_CzechRepublic_20210731.csv')\n",
        "msa_cols = list(ta_msa.columns)\n",
        "ta_cols = list(ta.columns)\n",
        "df_cols = check_columns_received(ta, msa_cols)\n",
        "df_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqSk2BMDCK6W",
        "outputId": "ba7c4578-b3f7-479a-ff25-3c1c954cece8"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "    #Fit dates to correct format to compare\n",
        "    df[start_date] = pd.to_datetime(df[start_date], format='%d/%m/%Y', errors='coerce')\n",
        "    df[end_date] = pd.to_datetime(df[end_date], format='%d/%m/%Y', errors='coerce')\n",
        "\n",
        "    #filtering Dateframe for specific dates\n",
        "    filtered = df.loc[(df[start_date] > df[end_date])|(df[start_date]=='')|(df[end_date]==''), [tw_index, start_date,end_date]]\n",
        "\n",
        "    filtered[start_date] = list(map(lambda x: f'{x:%d/%m/%Y}',filtered[start_date]))\n",
        "    filtered[end_date] = list(map(lambda x: f'{x:%d/%m/%Y}',filtered[end_date]))\n",
        "    filtered = filtered[[tw_index, start_date,end_date]] \n",
        "    return filtered\n",
        "    \n",
        "df_ta_dates = check_lc_ta_dates(ta,'Code', 'Contract start date', 'Contract end date')\n",
        "df_ta_dates"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4BraJ7gDw5V",
        "outputId": "1b7b78dc-a1b9-4bc1-fc57-a8df925663f9"
      },
      "source": [
        "ta['Classification of Tenant'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MNO     323\n",
              "OTMO    295\n",
              "Name: Classification of Tenant, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYc-9mpk18Ip"
      },
      "source": [
        "Script to read LC Input file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33z-44v_EOl1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "83566700-3fe7-4e44-a100-2c8b9c77bf60"
      },
      "source": [
        "def read_files(path, sheetname,col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int, cols_amount, bill_cols, format='mix', type_date=\"%d/%m/%Y\"):\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname,header=0, names = col_name, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_amount, 0)\n",
        "    for col in cols_amount:\n",
        "        #df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].apply(lambda x: '{:0,.2f}'.format(x))\n",
        "\n",
        "    df = df.fillna('')\n",
        "    replace_values(df, bill_cols)\n",
        "    date_parser(df, cols_date, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    return df\n",
        "\n",
        "def replace_values_ta(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', ]\n",
        "    #lista = []\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        df[column] = df[column].fillna(0)\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "col_names = ['Contract ID - NEW', 'FIN ID', 'Contract Crncy', 'Contract Type', 'Freq.', 'Freq. Unit', 'Indexation',\\\n",
        "         'Index upon request', 'Counterpart ID', 'Counterpart', 'LC Amount CZK\\nyearly', 'Amount in EUR yearly (Actual)', \\\n",
        "         'Payment terms Code', 'VAT Subject', '% of VAT', 'Contr. Start date', 'Contr. 1st End', 'Contr. Term End', \\\n",
        "         'Sublease Consession', 'Renewal Option', 'Unique Key', 'Count of Key', 'Count of ContrBct ', \\\n",
        "         'Remarks - Consistency check', 'Residual Period in years', 'Scoping classification', 'DAS sites', \\\n",
        "         'DAS ownership', '31_12_9999', 'Unique Key1', 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021', \\\n",
        "         'Check LC amount', 'Updated Item\\nAmount yearly', 'Contr. 1st End as of 31.05.2021', \\\n",
        "         'Contr. 1st End as of 30.06.2021', 'Check Contr. 1st End', 'Updated Item\\n1st End date', 'Comment', \\\n",
        "         'Comment date', 'Date']\n",
        "\n",
        "#ta = pd.read_csv(path_ta_input, header=0, names=col)\n",
        "dates_lc = ['Contr. Start date', 'Contr. 1st End', 'Contr. 1st End as of 31.05.2021', 'Contr. 1st End as of 30.06.2021']\n",
        "interger_lc = []\n",
        "amount_lc = [\"Amount in EUR yearly (Actual)\", 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021']\n",
        "lc_bill_cols = ['Contract ID - NEW','Counterpart']\n",
        "lc = read_files('/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx', 'Final data_Lease',col_names, 1, 0, 'FIN ID', \\\n",
        "                dates_lc, interger_lc,amount_lc, lc_bill_cols)\n",
        "\n",
        "col_order_lc = [\"Contract ID - NEW\",\"FIN ID\",\"Contract Crncy\",\"Contract Type\",\"Freq.\",\"Freq. Unit\",\"Indexation\",\\\n",
        "                \"Index upon request\",\"Counterpart ID\",\"Counterpart\",\"LC Amount CZK\\nyearly\",\"Amount in EUR yearly (Actual)\",\\\n",
        "                \"Payment terms Code\",\"VAT Subject\",\"% of VAT\",\"Contr. Start date\",\"Contr. 1st End\",\"Contr. Term End\",\\\n",
        "                \"Sublease Consession\",\"Renewal Option\",\"Unique Key\",\"Count of Key\",\"Count of ContrBct \",\\\n",
        "                \"Remarks - Consistency check\",\"Residual Period in years\",\"Scoping classification\",\"DAS sites\",\\\n",
        "                \"DAS ownership\",\"31_12_9999\",\"Unique Key1\",'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021',\\\n",
        "                \"Check LC amount\",\"Updated Item\\nAmount yearly\",'Contr. 1st End as of 31.05.2021','Contr. 1st End as of 30.06.2021',\\\n",
        "                \"Check Contr. 1st End\",\"Updated Item\\n1st End date\",\"Comment\",\"Comment date\",\"Date\"]\n",
        "lc = lc[col_order_lc]\n",
        "lc.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Contract ID - NEW</th>\n",
              "      <th>FIN ID</th>\n",
              "      <th>Contract Crncy</th>\n",
              "      <th>Contract Type</th>\n",
              "      <th>Freq.</th>\n",
              "      <th>Freq. Unit</th>\n",
              "      <th>Indexation</th>\n",
              "      <th>Index upon request</th>\n",
              "      <th>Counterpart ID</th>\n",
              "      <th>Counterpart</th>\n",
              "      <th>LC Amount CZK\\nyearly</th>\n",
              "      <th>Amount in EUR yearly (Actual)</th>\n",
              "      <th>Payment terms Code</th>\n",
              "      <th>VAT Subject</th>\n",
              "      <th>% of VAT</th>\n",
              "      <th>Contr. Start date</th>\n",
              "      <th>Contr. 1st End</th>\n",
              "      <th>Contr. Term End</th>\n",
              "      <th>Sublease Consession</th>\n",
              "      <th>Renewal Option</th>\n",
              "      <th>Unique Key</th>\n",
              "      <th>Count of Key</th>\n",
              "      <th>Count of ContrBct</th>\n",
              "      <th>Remarks - Consistency check</th>\n",
              "      <th>Residual Period in years</th>\n",
              "      <th>Scoping classification</th>\n",
              "      <th>DAS sites</th>\n",
              "      <th>DAS ownership</th>\n",
              "      <th>31_12_9999</th>\n",
              "      <th>Unique Key1</th>\n",
              "      <th>LC amount as of 31.05.2021</th>\n",
              "      <th>LC amount as of 30.06.2021</th>\n",
              "      <th>Check LC amount</th>\n",
              "      <th>Updated Item\\nAmount yearly</th>\n",
              "      <th>Contr. 1st End as of 31.05.2021</th>\n",
              "      <th>Contr. 1st End as of 30.06.2021</th>\n",
              "      <th>Check Contr. 1st End</th>\n",
              "      <th>Updated Item\\n1st End date</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Comment date</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1100000000</td>\n",
              "      <td>111530</td>\n",
              "      <td>CZK</td>\n",
              "      <td>Lease</td>\n",
              "      <td>3</td>\n",
              "      <td>Months</td>\n",
              "      <td>Free - Modifiable</td>\n",
              "      <td>yes</td>\n",
              "      <td>800222850</td>\n",
              "      <td>Sabadinová Marie</td>\n",
              "      <td>90176.97</td>\n",
              "      <td>3,281.55</td>\n",
              "      <td>Due 14days from invoice receipt date</td>\n",
              "      <td>no</td>\n",
              "      <td></td>\n",
              "      <td>28/06/2002</td>\n",
              "      <td>28/06/2022</td>\n",
              "      <td></td>\n",
              "      <td>Yes, but announcement needed</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1100000000|800222850</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mapped</td>\n",
              "      <td>2</td>\n",
              "      <td>In-scope</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>T00</td>\n",
              "      <td>1100000000|800222850|T00</td>\n",
              "      <td>90,176.97</td>\n",
              "      <td>90,176.97</td>\n",
              "      <td>0.0</td>\n",
              "      <td>JULY - no change, OK</td>\n",
              "      <td>28/06/2022</td>\n",
              "      <td>28/06/2022</td>\n",
              "      <td>0</td>\n",
              "      <td>JULY - no change, OK</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1100000001</td>\n",
              "      <td>111470</td>\n",
              "      <td>CZK</td>\n",
              "      <td>Lease</td>\n",
              "      <td>3</td>\n",
              "      <td>Months</td>\n",
              "      <td>Free - Modifiable</td>\n",
              "      <td>yes</td>\n",
              "      <td>800199147</td>\n",
              "      <td>SPOLECENSTVI VLASTNIKU JEDNOTEK DOMU C.P. 589,...</td>\n",
              "      <td>88819.08</td>\n",
              "      <td>3,232.14</td>\n",
              "      <td>Due 14days from invoice receipt date</td>\n",
              "      <td>yes</td>\n",
              "      <td>0.21</td>\n",
              "      <td>25/09/2002</td>\n",
              "      <td>31/12/2025</td>\n",
              "      <td></td>\n",
              "      <td>Yes, but announcement needed</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1100000001|800199147</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mapped</td>\n",
              "      <td>5</td>\n",
              "      <td>In-scope</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>T00</td>\n",
              "      <td>1100000001|800199147|T00</td>\n",
              "      <td>88,819.08</td>\n",
              "      <td>88,819.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>JULY - no change, OK</td>\n",
              "      <td>31/12/2025</td>\n",
              "      <td>31/12/2025</td>\n",
              "      <td>0</td>\n",
              "      <td>JULY - no change, OK</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Contract ID - NEW  FIN ID Contract Crncy  ... Comment Comment date Date\n",
              "0         1100000000  111530            CZK  ...                          \n",
              "1         1100000001  111470            CZK  ...                          \n",
              "\n",
              "[2 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i4SMB8fY8Hs"
      },
      "source": [
        "lc.to_excel('/content/LC_Input_CzechRepublic_20210831.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVsiMGPtbHWo"
      },
      "source": [
        "lc.to_csv('/content/LC_Input_CzechRepublic_20210831.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBLRb-ydh9uM"
      },
      "source": [
        "def change_incorret(df, df_index, col_chg, incorrect_value, new_value):\n",
        "    list_sites = list(df[df[col_chg]==incorrect_value][df_index])\n",
        "    df.loc[df[df_index].isin(list_sites), col_chg] = new_value\n",
        "    print(df.loc[df[df_index].isin(list_sites)][[df_index, col_chg]])\n",
        "\n",
        "change_incorret(lc, \"Contract ID - NEW\", '% of VAT', 0.21,'21%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f85vYdkE1_NL"
      },
      "source": [
        "def check_amounts(df_check, df_index, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            value = str(df.loc[site,column])\n",
        "            #if not re.match(r'^(([1-9]\\d{0,2}(\\,\\d{0,3})*)|(([1-9]\\,\\d*)?\\d))(\\.\\d{9})$', value)\n",
        "            if not value.__contains__(pattern):\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    if value == '':\n",
        "                        df_new.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_new.loc[site,column] = f'Incorret Format: {value}'\n",
        "                else:\n",
        "                    if value == '':\n",
        "                        df_new.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_new.loc[site,column] = f'Incorret Format: {value}'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "\n",
        "    return df_new\n",
        "\n",
        "lc_cols = [\"Contract ID - NEW\", 'LC Amount CZK\\nyearly']\n",
        "#lc['Importe anual'] = lc['   Importe anual'].astype(str)\n",
        "#Python interpretou como float a coluna de Import Anual\n",
        "df_lc_amount = check_amounts(lc, \"Contract ID - NEW\", lc_cols, '.')\n",
        "df_lc_amount\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjcRUHSCO0_W",
        "outputId": "84313014-d21d-4f09-8e67-a8104e65d064"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "    #Fit dates to correct format to compare\n",
        "    df[start_date] = pd.to_datetime(df[start_date], format='%d/%m/%Y', errors='coerce')\n",
        "    df[end_date] = pd.to_datetime(df[end_date], format='%d/%m/%Y', errors='coerce')\n",
        "\n",
        "    #filtering Dateframe for specific dates\n",
        "    filtered = df.loc[(df[start_date] > df[end_date])|(df[start_date]=='')|(df[end_date]==''), [tw_index, start_date,end_date]]\n",
        "\n",
        "    filtered[start_date] = list(map(lambda x: f'{x:%d/%m/%Y}',filtered[start_date]))\n",
        "    filtered[end_date] = list(map(lambda x: f'{x:%d/%m/%Y}',filtered[end_date]))\n",
        "    filtered = filtered[[tw_index, start_date,end_date]] \n",
        "    return filtered\n",
        "          \n",
        "df_lc_dates = check_lc_ta_dates(lc,\"FIN ID\", \"Contr. Start date\",\"Contr. 1st End\")\n",
        "df_lc_dates\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS0rLNMZPW70"
      },
      "source": [
        "def general_log_erros(df_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')  \n",
        "    for lista in df_list:\n",
        "        try:\n",
        "            if not lista[1].empty:\n",
        "                lista[1].to_excel(writer, sheet_name=lista[0], index=False)\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        except:\n",
        "            if lista[1] is not None and lista[1].is_valid():\n",
        "                lista[1].to_excel(writer, sheet_name=lista[0], index=False)\n",
        "    try: \n",
        "        writer.save() \n",
        "    except IndexError:\n",
        "        print('No one log to write!')\n",
        "\n",
        "logs_lc = [['Amount Errors', df_lc_amount],\n",
        "        ['Dates Errors', df_lc_dates]]\n",
        "        \n",
        "path_log_lc = '/content/LC_CZ_Errors.xlsx'\n",
        "general_log_erros(logs_lc, path_log_lc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}