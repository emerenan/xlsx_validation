{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cz_validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3gH2qsIxao/RxYKMPorlm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDANxxNd5efG"
      },
      "source": [
        "!pip install unidecode\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "def csv_files(path):\n",
        "    \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "    import csv\n",
        "    f = open(path, encoding='windows-1252', errors='ignore')\n",
        "    data = []\n",
        "    for row in csv.reader(f, delimiter=','):\n",
        "        data.append(row)\n",
        "    col = [*data[0]]\n",
        "    data.pop(0)\n",
        "    df = pd.DataFrame(data, columns=col)\n",
        "    return df, col\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def change_incorret(df, df_index, col_chg, incorrect_value, new_value):\n",
        "    list_sites = list(df[df[col_chg]==incorrect_value][tw_index])\n",
        "    df.loc[df[df_index].isin(list_sites), col_chg] = new_value\n",
        "    #print(df.loc[df[df_index].isin(list_sites)][[df_index, col_chg]])\n",
        "       "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXC51sanKFX5"
      },
      "source": [
        "# Reading and Fit Excel File\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSFDJzKG0c20"
      },
      "source": [
        "# Read CZ towerdb files\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index, cols_number, col_dates, format='normal', type_date=\"%d/%m/%Y\"):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ']\n",
        "        #lista = []\n",
        "\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(0)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df\n",
        "\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) and not isinstance(date_obj, str) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    date_parser(df, col_dates, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_number)\n",
        "\n",
        "    for col in cols_number:\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "    \n",
        "    df = df.fillna('')\n",
        "\n",
        "    return df\n",
        "\n",
        "path_towerdb = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "towerdb_col = lower_str([\"Code (Duplicate)\",\"Site Status\",\"VF - In scope / out of scope (Generalised scoping)\",\"Site in Skylon scope Actual (From Site List Sheet )\",\\\n",
        "               \"Legacy Site Code(Duplicate)\",\"TIMS Site Code\",\"Legacy Site Code\",\"Site Name\",\"Macro Region\",\"Province\",\"Municipality\",\"Inhabitants\",\"Address\",\\\n",
        "               \"Ground Register\",\"Altitude\",\"Latitude\",\"Longitude\",\"Categorization by Inhabitants\",\"Categorization by Transmission Sys\",\\\n",
        "               \"Categorization by Site Type\",\"Categorization by Transmission Sys (subcluster)\",\"Other internal Categorization 1 (Identify ACQ Sites)\",\\\n",
        "               \"Other internal Categorization 2 Energy provider (Eon/ LL)\",\"DAS+Macro\",\"DAS (Yes/ No)\",\"DAS Ownership (Complete/ Partial/ 3rd Party)\",\\\n",
        "               \"Active/ Passive DAS\",\"# of remote units/ radiating points\",\"Type of Structure\",\"Distance highest antenna to ground level\",\"GBT Tower height\",\\\n",
        "               \"POD ID\",\"Energy Consumption LTM (kwh)\",\"Annual Energy cost LTM (Euros)\",\"Infrastructure ready (existing)/ to be ready (new)\",\\\n",
        "               \"Infrastructure to be dismantled by\",\"Radio equipments to be deactivated by\",\"Infrastructure to be shared by\",\"Technology VOD\",\"Fibre / Microwave\",\\\n",
        "               \"Vertical passive structure owner\",\"Room configuration (detailed)\",\"Shelter passive structure ownership\",\"Type of Air Conditioning\",\\\n",
        "               \"Number of cabinets (Full Capacity)\",\"Number of Antenna (Full Capacity)\",\"Number of MW (Full Capacity)\",\"Counterpart\",\"# of Lease Contracts\",\\\n",
        "               \"Current annual lease fees \",\"Current other fees (Maintenance)\",\"Current other fees\",\"(Average) residual duration - Lease contract\",\\\n",
        "               \"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\"# of Tenants Agreements\",\\\n",
        "               \"Current Total Annual Hosting Fees\",\"Tenant (name/ID) MNO1 (Česká telekomunikační infrastruktura a.s.)\",\"Annual Fee per Tenant MNO1\",\\\n",
        "               \"Annual Energy Fee MNO1\",\"Annual Maintenance Fee MNO1\",\"Other Services Fee MNO1\",\"Residual duration MNO1 (Years)\",\\\n",
        "               \"Tenant (name/ID) MNO2 (T-Mobile Czech Republic a.s.)\",\"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\"Annual Maintenance Fee MNO2\",\\\n",
        "               \"Other Services Fee MNO2\",\"Residual duration MNO2 (days)\",\"Tenant (name/ID) MNO3\",\"Annual Fee per Tenant MNO3\",\"Annual Energy Fee MNO3\",\\\n",
        "               \"Annual Maintenance Fee MNO3\",\"Other Services Fee MNO3\",\"Residual duration MNO3\",\"# of OTMOs\",\"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\\\n",
        "               \"Annual Maintenance Fee OTMOs\",\"Other Services Fee OTMOs\",\"Average residual duration (days)\",\"Check\",\"Strategic Macro Sites\",\"Critical Sites\",\\\n",
        "               \"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\"Transmission Sites\",\\\n",
        "               \"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\"VF-CZ Demerger phase\",\\\n",
        "               \"EVO Location [FAR Site ID] \",\"Billing Trigger date \",\"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\\\n",
        "               \"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\"Wip_Site\",\\\n",
        "               \"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"])\n",
        "\n",
        "dates = ['Billing Trigger date ', \"Infrastructure ready (existing)/ to be ready (new)\", \"Infrastructure to be dismantled by\",\\\n",
        "         'First_Active_Sharing_Start_Date','First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "         'Date_Of_Equipment_Removal']\n",
        "num_parse = [\"Inhabitants\", \"Current annual lease fees \",\"(Average) residual duration - Lease contract\",\\\n",
        "             \"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\\\n",
        "             \"# of Tenants Agreements\",\"Current Total Annual Hosting Fees\", \"Annual Fee per Tenant MNO1\",\\\n",
        "             \"Annual Energy Fee MNO1\", \"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\\\n",
        "             \"Residual duration MNO2 (days)\", \"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\\\n",
        "             \"Other Services Fee OTMOs\",\"Average residual duration (days)\"]\n",
        "tab = 'Tower DB'\n",
        "row = 3\n",
        "towerdb = read_files(path_towerdb,tab, 3, 0,\"Code (Duplicate)\",num_parse,dates)\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "towerdb = towerdb[towerdb_col]\n",
        "\n",
        "path_msa = '/content/TowerDB_CzechRepublic_20210731 (3).csv'\n",
        "msa = pd.read_csv(path_msa,encoding='ISO-8859-1').fillna('')\n",
        "msa.columns = lower_str(list(msa.columns))\n",
        "msa = msa.rename(columns={'ï»¿code (duplicate)': 'code (duplicate)',\n",
        "                          'tenant (name/id) mno1 (ä\\x8ceskã¡ telekomunikaä\\x8dnã\\xad infrastruktura a.s.)': 'tenant (name/id) mno1 (česká telekomunikační infrastruktura a.s.)'})\n",
        "msa_cols = list(msa.columns)\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "\"\"\"Check columns received\"\"\"\n",
        "df_cols = check_columns_received(towerdb, msa_cols)\n",
        "df_cols\n",
        "#No colum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT7nbdJzTFXm"
      },
      "source": [
        "def change_incorret(df, df_index, col_chg, incorrect_value, new_value):\n",
        "    list_sites = list(df[df[col_chg]==incorrect_value][tw_index])\n",
        "    df.loc[df[df_index].isin(list_sites), col_chg] = new_value\n",
        "    print(df.loc[df[df_index].isin(list_sites)][[df_index, col_chg]])\n",
        "\n",
        "change_incorret(towerdb, tw_index, 'subsequent_sharing_arrangement', 'NO', 'No')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoqB1yjZ0n1r"
      },
      "source": [
        "towerdb.to_excel('/content/TowerDB_CzechRepublic_20210831.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKLpXMuub_65"
      },
      "source": [
        "towerdb.to_csv('/content/TowerDB_CzechRepublic_20210831.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhdRKjcRBC4b"
      },
      "source": [
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = \"code (duplicate)\"\n",
        "tw_doer = \"date_of_equipment_removal\"\n",
        "tw_status = \"site status\"\n",
        "tw_bts = 'bts_site'\n",
        "tw_bill = \"billing trigger date \"\n",
        "tw_wip = 'wip_site'\n",
        "tw_decom = 'decommissioned_site'\n",
        "tw_critical = 'critical sites'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__VBNgJTxRTD"
      },
      "source": [
        "Ver essa necessidade de unidecode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BXkWi_-4P3v"
      },
      "source": [
        "path_msa = '/content/TowerDB_CzechRepublic_20210731 (3).csv'\n",
        "msa = pd.read_csv(path_msa,encoding='ISO-8859-1').fillna('')\n",
        "cols_con = ['Province','Municipality','Address']\n",
        "for col in cols_con:\n",
        "   msa[col] = msa[col].astype(str).apply(unidecode)\n",
        "msa = msa.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "msa.columns = lower_str(towerdb_col)\n",
        "msa.head(1)\n",
        "#msa.columns = msa_cols\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT75KHTjUyMe"
      },
      "source": [
        "# Check Dates Format (dd-mm-YYYY)\n",
        "\"\"\"Não deve haver NAN ou NAT na coluna, fazer fillna('') antes\"\"\"\n",
        "def check_date_columns(df, df_index,status_col,bts_col, wip_col, columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col,bts_col, wip_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col,bts_col, wip_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "\n",
        "dates_cols = [tw_index,'Billing Trigger date ', 'First_Active_Sharing_Start_Date',\\\n",
        "              'First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "              'Date_Of_Equipment_Removal'] \n",
        "dates_cols = lower_str(dates_cols)\n",
        "\n",
        "dates = ['Billing Trigger date ', 'First_Active_Sharing_Start_Date',\\\n",
        "              'First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "              'Date_Of_Equipment_Removal']           \n",
        "dates = lower_str(dates)\n",
        "\n",
        "#date_parser(towerdb, lower_str(dates), 2, 1, 'mixed')\n",
        "#tw_in_service = tw_in_service[dates_cols]\n",
        "tw_in_service = towerdb[towerdb['site status']=='In Service']\n",
        "df_dates_errors = check_date_columns(tw_in_service,tw_index, tw_status, tw_bts, tw_wip, lower_str(dates_cols), 2)\n",
        "df_dates_errors\n",
        "#Errors em varias colunas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtUVfvVPdP76"
      },
      "source": [
        "Validaton for general sites picklist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUXH4ckBaL4Q"
      },
      "source": [
        "def check_picklist_v1(df,df_index,df_status, df_cols, picklist_dict):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    df_picklist = df[df_cols].fillna('')\n",
        "    df_picklist = df_picklist.to_dict(orient='list')\n",
        "\n",
        "    new_dic = defaultdict(list)\n",
        "    \"\"\"for id in df_picklist[df_index]:\n",
        "        if id not in new_dic.keys():\n",
        "            new_dic[df_index].append(id)\"\"\" \n",
        "            \n",
        "    for column in set(picklist_dict.keys()):\n",
        "        for value in df_picklist[column]:\n",
        "            if value not in set(picklist_dict[column]): \n",
        "                if pd.isnull(value) or pd.isna(value) or value=='' or value=='nan':\n",
        "                    new_dic[column].append('Blank Value')\n",
        "                else:\n",
        "                    new_dic[column].append(f'Incorrect value: {value}')\n",
        "            else:\n",
        "                new_dic[column].append('Ok!')\n",
        "\n",
        "    df_errors = pd.DataFrame(new_dic)\n",
        "    df_errors = df_errors.replace('Ok!', np.nan)\n",
        "    df_errors[df_index] = df_picklist[df_index]\n",
        "    df_errors = df_errors.set_index(df_index)\n",
        "    df_errors = df_errors.dropna(how='all', axis=0)\n",
        "    df_errors = df_errors.dropna(how='all', axis=1)\n",
        "    df_errors = df_errors.reset_index()\n",
        "\n",
        "    df = df[[df_index, df_status]]\n",
        "    df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "    df_errors = df_errors.set_index(df_index)\n",
        "    df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "    #df_errors = df_errors.reset_index()\n",
        "    return df_errors\n",
        "\n",
        "dic_gen = ['code (duplicate)', 'site status', 'categorization by transmission sys']\n",
        "pick_gen = {\n",
        "    'categorization by transmission sys': ['Macro','Public DAS','Repeater','Transmission']\n",
        "    }\n",
        "df_pick_general_errors = check_picklist_v1(towerdb, tw_index, tw_status, dic_gen, pick_gen)\n",
        "df_pick_general_errors\n",
        "#Pipeline sites without categorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdgc55EAdJZV"
      },
      "source": [
        "Validation for On air sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-cTM_dnU0Gk"
      },
      "source": [
        "picklist_cz = {\n",
        "    'categorization by transmission sys': ['Macro','Public DAS','Repeater','Transmission'],\n",
        "    'categorization by site type':['DAS passive','GBT','RTT'],\n",
        "    'strategic macro sites':['Yes','No'],\n",
        "    'critical sites':['Yes','No'],\n",
        "    'case a core site':['Non Core', 'Case A','Case B'],\n",
        "    'macro site - transmission hub site':['Yes','No'],\n",
        "    'macro site - transmission hub site with/without shelters': ['With shelters','Without shelters','Non Transmission Hub Site'],\n",
        "    'transmission sites': ['No','Yes'],\n",
        "    'room configuration': ['Indoor','Outdoor',\"Indoor & Outdoor\"],\n",
        "    'power supply':['AC','DC','AC & DC','No Power'],\n",
        "    'air conditioning': ['No','Yes; Indoor Air Conditioning','Yes; Indoor Free Air cooling / Free cooling units'],\n",
        "    'active sharing arrangements involving the operator':['MORAN (On VF equipment)','MORAN (On non-VF equipment)','MOCN with Spectrum Pooling','Partial Active-Passive','No Active Sharing'],\n",
        "    'vf-cz demerger phase': ['Cancelled','Phase 1','Phase 2','Skylon BTS'],\n",
        "    'strategic_site_bucket': ['Yes - 0-5%','Yes - 5-10%','Non Strategic'],\n",
        "    'critical site bucket': ['Beyond 10%','Within 10%','Non Critical'],\n",
        "    'subsequent_sharing_arrangement': ['Yes','No'],\n",
        "    'decommissioned_site': ['Yes','No'],\n",
        "    'wip_site': ['Yes','No'],\n",
        "    'bts_site': ['Yes','No'],\n",
        "    'sites_as_metered_estimated': ['Estimated Model','Metered Model']\n",
        "}\n",
        "dic_cols = [tw_index,'categorization by transmission sys',\\\n",
        "            'categorization by site type',\\\n",
        "            'strategic macro sites',\\\n",
        "            'critical sites',\\\n",
        "            'case a core site',\\\n",
        "            'macro site - transmission hub site',\\\n",
        "            'macro site - transmission hub site with/without shelters',\\\n",
        "            'transmission sites',\\\n",
        "            'room configuration',\\\n",
        "            'power supply',\\\n",
        "            'air conditioning',\\\n",
        "            'active sharing arrangements involving the operator',\\\n",
        "            'vf-cz demerger phase',\\\n",
        "            'strategic_site_bucket',\\\n",
        "            'critical site bucket',\\\n",
        "            'subsequent_sharing_arrangement',\\\n",
        "            'decommissioned_site',\\\n",
        "            'wip_site',\\\n",
        "            'bts_site',\\\n",
        "            'sites_as_metered_estimated']\n",
        "df_pick_actives_errors = check_picklist_v1(tw_in_service, tw_index, tw_status, dic_cols, picklist_cz)\n",
        "df_pick_actives_errors\n",
        "#Erros em varias colunas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWLLVcK3cPRZ"
      },
      "source": [
        "Month-on-Month BTS sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "iQJPPtAXZiJr",
        "outputId": "ba7f0e7d-2331-47de-fe00-7d5588eb7203"
      },
      "source": [
        "def check_mom_bts(df_tw, tw_index,status_col, tw_col, df_msa, msa_index, msa_col):\n",
        "\n",
        "    msa_bts = [i for i in df_msa[df_msa[msa_col]=='Yes'][msa_index]]\n",
        "\n",
        "    tw_bts = [i for i in df_tw[df_tw[tw_col]=='Yes'][tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts if i not in tw_bts]\n",
        "    filtered = df_tw.loc[df_tw[tw_index].isin(out_tower_bts)]\n",
        "\n",
        "    return filtered[[tw_index,status_col, tw_col]]    \n",
        "        \n",
        "tw_filtered = towerdb[towerdb['site status']=='In Service']\n",
        "\n",
        "df_mom_bts = check_mom_bts(tw_filtered, tw_index, tw_status, tw_bts, msa, tw_index, tw_bts)\n",
        "df_mom_bts\n",
        "\n",
        "#df_mom_decom = check_mom_bts(tw_filtered, tw_index,tw_status, tw_decom, msa, tw_index, tw_decom)\n",
        "#df_mom_decom\n",
        "# No errors in Both"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code (duplicate)</th>\n",
              "      <th>site status</th>\n",
              "      <th>decommissioned_site</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [code (duplicate), site status, decommissioned_site]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vv5LQ99m5mc"
      },
      "source": [
        "#Read UIP File\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip = pd.read_excel('/content/UserInput_CzechRepublic_20210831.xlsx',sheet_name='SiteLevel',usecols=[0,2,3],skiprows=3).fillna('')\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [i for i in msa[tw_index]]\n",
        "tw_sites = [str(i) for i in towerdb[tw_index]]\n",
        "uip_sites = [str(i) for i in uip['Site_ID']]\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQSv5rvy6Mev"
      },
      "source": [
        "Verificar Errors de Sites Numericos com String\n",
        "Alguns sites estão com Integer ou String no ficheiro anterior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3NGucZF02V4"
      },
      "source": [
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [i for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites['New_Sites'] =  new_sites['New_Sites'].astype(int)\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    \n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uis = [i for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index] if i not in uip_list]\n",
        "    bts_out_uis = pd.DataFrame(bts_out_uis, columns=['Bts_Sites_Out_UIS_File'])\n",
        "    bts_out_uis = pd.merge(bts_out_uis, filtered, how='left', left_on=['Bts_Sites_Out_UIS_File'], right_on=tw_index)\n",
        "    bts_out_uis = bts_out_uis[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    #print(current_date)\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "\n",
        "    return new_sites, bts_out_uis, df_site_bts[[tw_index,status_col, bts_col, bill_col]]\n",
        "\n",
        "new_sites, bts_sites_out_uis, df_bts_demerged = check_new_sites(towerdb, tw_index, tw_bts, tw_bill, tw_status, msa_sites, tw_sites, uip_sites)\n",
        "\n",
        "print(new_sites)\n",
        "print('\\n')\n",
        "print(bts_sites_out_uis)\n",
        "print('\\n')\n",
        "print(df_bts_demerged)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROTHG9-NqPcD"
      },
      "source": [
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, df_msa, msa_index, msa_wip):\n",
        "\n",
        "    wip_msa = [i for i in df_msa[df_msa[msa_wip]=='Yes'][msa_index]]\n",
        "    \n",
        "    tw_wip_sites = [str(i) for i in df_tw[df_tw[wip_tw]=='Yes'][tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, wip_tw, tw_bts]]\n",
        "    \n",
        "    wip_out_tw_list = [i for i in tw_wip_sites if i not in wip_msa]\n",
        "\n",
        "    return tw_wip_site_bts_flagged\n",
        "\n",
        "    # Falta os outros países\n",
        "tw_wip_site_bts_flagged = check_wip(towerdb, tw_index, tw_wip, tw_bts, msa, tw_index, tw_wip)\n",
        "\n",
        "print(tw_wip_site_bts_flagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6HAiaPFrV6F"
      },
      "source": [
        "Validation DOER sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68rdgmyprVgu",
        "outputId": "3e5f25e4-c686-4fc3-a865-d828d136d126"
      },
      "source": [
        "#Validate Decomissioned Sites\n",
        "def check_decommissioned(df,df_index,status_col, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "\n",
        "    return filtered[[df_index,status_col, decom_col, doer_col]]\n",
        "\n",
        "\n",
        "df_decom_errors = check_decommissioned(towerdb,tw_index, tw_status, tw_decom, tw_doer)\n",
        "df_decom_errors\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onf4s48gr6Yw"
      },
      "source": [
        "DOER Valitadions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bsHWICor963"
      },
      "source": [
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    df_tw = df_tw[df_tw[status_col]==status][[tw_index, status_col, date_col]] \n",
        "    #print(df_tw)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[df_tw[date_col].astype('datetime64[ns]') < current_date]\n",
        "            return filtered\n",
        "        else:\n",
        "            #bill_dates = pd.to_datetime(df_tw[date_col], errors==coerrce)\n",
        "            filtered = df_tw[df_tw[date_col]=='']\n",
        "            return filtered\n",
        "    else:\n",
        "        print('\\nNo errors founded!\\n')\n",
        "actives  = towerdb[towerdb[tw_status]=='In Service']\n",
        "#Validate sites in service which has DOER less than current date\n",
        "\n",
        "df_doer_errors = check_tw_bill_doer(actives, tw_index, tw_doer, tw_status, 'In Service', 'doer')\n",
        "df_doer_errors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzT8BJ8NtF9j"
      },
      "source": [
        "TowerDb vs UIS validations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYjPay1Rx7OD"
      },
      "source": [
        "UIS In Month"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80qFRKGovOKf"
      },
      "source": [
        "\"\"\"Check UIP In Month ites matches with Towerdb sites\"\"\"\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [str(i) for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "\n",
        "    uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS In Month not active in TowerDB!'])\n",
        "    #uis_sites_not_in_towerdb['UIS In Month not active in TowerDB!'] =  uis_sites_not_in_towerdb['UIS In Month not active in TowerDB!'].astype(int)\n",
        "    uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS In Month not active in TowerDB!',\\\n",
        "                                    right_on=tw_index)\n",
        "    uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS In Month not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS In Month!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS In Month!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS In Month!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "    decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "    decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                    right_on=tw_index)\n",
        "    decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "\n",
        "    #Check BTS sites\n",
        "    bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "    uip_bts = [i for i in df_uip[df_uip['BTS site applicable charge (Annual)']!='']['Site_ID']]\n",
        "\n",
        "    #if not set(bts_sites).intersection(uip_sites):\n",
        "    bts_sites_out_uip = [i for i in uip_bts if i not in bts_sites]\n",
        "    bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['UIS BTS not in TowerDB(BTS)'])\n",
        "    #bts_sites_out_uip['UIS BTS not in TowerDB(BTS)'] = bts_sites_out_uip['UIS BTS not in TowerDB(BTS)'].astype(int)\n",
        "    bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='UIS BTS not in TowerDB(BTS)',\\\n",
        "                                    right_on=tw_index)\n",
        "    bts_sites_out_uip = bts_sites_out_uip[['UIS BTS not in TowerDB(BTS)', status_tw_col]]\n",
        "\n",
        "    #  Check for UIP critical sites \n",
        "    uip_critical = [i for i in df_uip[df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='']['Site_ID']]\n",
        "    bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "\n",
        "    critical = [i for i in uip_critical if i not in bts_tw_critical]\n",
        "    critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "    critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                right_on=tw_index)\n",
        "    critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "    #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "    return uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "uis_sites_not_in_towerdb,in_service_uis_sites, decomiss_sites_in_uis, bts_sites_out_uis, critical = check_uip_tw(towerdb,tw_index, tw_status, \\\n",
        "                                                                tw_decom, tw_bts,tw_critical, uip, uip_sites)\n",
        "\n",
        "print(uis_sites_not_in_towerdb)\n",
        "print('\\n')\n",
        "print(in_service_uis_sites)\n",
        "print('\\n')\n",
        "print(decomiss_sites_in_uis)\n",
        "print('\\n')\n",
        "print(bts_sites_out_uis)\n",
        "print('\\n')\n",
        "print(critical)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxo8qoPHx2aD"
      },
      "source": [
        "UIS True UP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8UcruIH8ikt"
      },
      "source": [
        "\"\"\"Check UIP In Month ites matches with Towerdb sites\"\"\"\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [str(i) for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "\n",
        "    uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS In Month not active in TowerDB!'])\n",
        "    #uis_sites_not_in_towerdb['UIS In Month not active in TowerDB!'] =  uis_sites_not_in_towerdb['UIS In Month not active in TowerDB!'].astype(int)\n",
        "    uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS In Month not active in TowerDB!',\\\n",
        "                                    right_on=tw_index)\n",
        "    uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS In Month not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS In Month!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS In Month!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS In Month!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "    decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "    decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                    right_on=tw_index)\n",
        "    decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "\n",
        "    #Check BTS sites\n",
        "    bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "    uip_bts = [i for i in df_uip[df_uip['BTS site applicable charge (Annual)']!='']['Site_ID']]\n",
        "\n",
        "    #if not set(bts_sites).intersection(uip_sites):\n",
        "    bts_sites_out_uip = [i for i in uip_bts if i not in bts_sites]\n",
        "    bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['UIS BTS not in TowerDB(BTS)'])\n",
        "    #bts_sites_out_uip['UIS BTS not in TowerDB(BTS)'] = bts_sites_out_uip['UIS BTS not in TowerDB(BTS)'].astype(int)\n",
        "    bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='UIS BTS not in TowerDB(BTS)',\\\n",
        "                                    right_on=tw_index)\n",
        "    bts_sites_out_uip = bts_sites_out_uip[['UIS BTS not in TowerDB(BTS)', status_tw_col]]\n",
        "\n",
        "    #  Check for UIP critical sites \n",
        "    uip_critical = [i for i in df_uip[df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='']['Site_ID']]\n",
        "    bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "\n",
        "    critical = [i for i in uip_critical if i not in bts_tw_critical]\n",
        "    critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "    critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                right_on=tw_index)\n",
        "    critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "    #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "    return uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "#Read UIP File\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip_true = pd.read_excel('/content/UserInput_CzechRepublic_20210630 TrueUp.xlsx',sheet_name='SiteLevel',usecols=[0,2,3],skiprows=3).fillna('')\n",
        "uip_true.columns = uip_names\n",
        "\n",
        "uip_true_sites = [str(i) for i in uip['Site_ID']]\n",
        "\n",
        "uis_sites_not_in_towerdb_tu , in_service_uis_sites_tu, decomiss_sites_in_uis_tu, bts_sites_out_uis_tu, critical_tu = check_uip_tw(towerdb,tw_index, tw_status, \\\n",
        "                                                                tw_decom, tw_bts,tw_critical, uip_true, uip_true_sites)\n",
        "\n",
        "print(uis_sites_not_in_towerdb_tu)\n",
        "print('\\n')\n",
        "print(in_service_uis_sites_tu)\n",
        "print('\\n')\n",
        "print(decomiss_sites_in_uis_tu)\n",
        "print('\\n')\n",
        "print(bts_sites_out_uis_tu)\n",
        "print('\\n')\n",
        "print(critical_tu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKOd5LyjuMVc"
      },
      "source": [
        "Commercial Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BClu4PcvuOEo"
      },
      "source": [
        "def check_diffs_v2(path_current, path_last, sheet='Commercial'):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def highlight_diff(data, color='yellow'):\n",
        "        attr = 'background-color: {}'.format(color)\n",
        "        other = data.xs('Current', axis='columns', level=-1)\n",
        "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                            index=data.index, columns=data.columns)\n",
        "\n",
        "    _actual = pd.read_excel(path_current,sheet_name=sheet).fillna('')\n",
        "    _before = pd.read_excel(path_last,sheet_name=sheet).fillna('')\n",
        "\n",
        "    df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "    df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "    return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "\n",
        "path_current = '/content/UserInput_CzechRepublic_20210831.xlsx'\n",
        "path_last = '/content/UserInput_CzechRepublic_20210731.xlsx'\n",
        "df_comm_errors = check_diffs_v2(path_current, path_last,sheet='Commercial')\n",
        "df_comm_errors\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__UxAcFX7JHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d57dfe-7e0e-43c2-e6bd-2b14a6fd5895"
      },
      "source": [
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "        #lista = []\n",
        "\n",
        "        df.fillna(\"\", inplace=True)\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df\n",
        "\n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    #df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    if not df_commercial_diffs.empty:\n",
        "        return df_commercial_diffs\n",
        "    else:\n",
        "        print('\\nNo Errors founded!')\n",
        "\n",
        "names = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', 'MSA Sites (Phase 1)\\nInput_Value',\\\n",
        "        'PMA Sites (Phase 2)\\nInput_Value','Description/Instruction', 'Frequency of Update']\n",
        "merge_cols = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', \\\n",
        "              'Description/Instruction', 'Frequency of Update']\n",
        "cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Data_Type','Input_Value_1_actual',\\\n",
        "                'Input_Value_1_before','Input_Value_2_actual','Input_Value_2_before','Equal Values','Description/Instruction', 'Frequency of Update']\n",
        "# Check for commercial Values into current UIP File and compare with UIP File before\n",
        "\n",
        "df_com = check_commercial(path_current, path_last,['MSA Sites (Phase 1)\\nInput_Value','PMA Sites (Phase 2)\\nInput_Value'], names, merge_cols, cols_ordered)\n",
        "df_com\n",
        "# No Errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors founded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRJDetTx6r31"
      },
      "source": [
        "def general_log_erros(df_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')  \n",
        "    for lista in df_list:\n",
        "        try:\n",
        "            if not lista[1].empty:\n",
        "                lista[1].to_excel(writer, sheet_name=lista[0], index=False)\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        except:\n",
        "            if lista[1] is not None and lista[1].is_valid():\n",
        "                lista[1].to_excel(writer, sheet_name=lista[0], index=False)\n",
        "                \n",
        "    writer.save() \n",
        "\n",
        "logs = [['Actives Dates Columns Errors', df_dates_errors],\n",
        "['General Picklist Errors', df_pick_general_errors],\n",
        "['Active Picklist Errors', df_pick_actives_errors],\n",
        "['MSA BTS not in TowerDB', df_mom_bts],\n",
        "['New Sites',  new_sites],\n",
        "['BTS Billing trigger Date Errors', df_bts_demerged],\n",
        "['BTS and WIP Flagged Errors',tw_wip_site_bts_flagged],\n",
        "['Decomm Sites Dates Errors', df_decom_errors],\n",
        "['DOER Errors', df_doer_errors],\n",
        "['UIS Sites not active in TowerDB', uis_sites_not_in_towerdb],\n",
        "['TowerDB Sites out of UIS', in_service_uis_sites],\n",
        "['Decom Sites In UIS', decomiss_sites_in_uis],\n",
        "['UIS BTS not in TowerDB(BTS)', bts_sites_out_uis],\n",
        "['Sites Beyond 10% out UIS', critical],\n",
        "['UIS(True Up) not active in TowerDB', uis_sites_not_in_towerdb_tu],\n",
        "['TowerDB\\'s out of UIS(True up)', in_service_uis_sites_tu],\n",
        "['Decom\\'s In UIS(True Up)', decomiss_sites_in_uis_tu],\n",
        "['UIS(True up) not in TowerDB(BTS)', bts_sites_out_uis_tu],\n",
        "['Sites Beyond 10% out UIS(True)', critical_tu],\n",
        "['Comercial Differences', df_comm_errors]]\n",
        "\n",
        "general_log_erros(logs, '/content/Towerdb_CZ_Errors.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmEVTURV2gCK"
      },
      "source": [
        "Create CSV File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxxPNjgByHcw"
      },
      "source": [
        "Script to make TA Input file validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOREPa3GfFb_"
      },
      "source": [
        "#Read TA Input File\n",
        "def read_files(path, sheetname, col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int, cols_amount, bill_cols, \\\n",
        "               format='mix', type_date=\"%d/%m/%Y\"):\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "        \n",
        "    def replace_values_am(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df \n",
        "\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname,header=0, names = col_name, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values_am(df, cols_amount, 0)\n",
        "    for col in cols_amount:\n",
        "        #df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f' {x:,} ')\n",
        "\n",
        "    for col in cols_int:\n",
        "        df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int)\n",
        "    df = df.fillna('')\n",
        "    replace_values_am(df, bill_cols)\n",
        "    date_parser(df, cols_date, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    return df\n",
        "\n",
        "def replace_values_ta(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ']\n",
        "    #lista = []\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        df[column] = df[column].fillna(0)\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "path_ta_input = '/content/TA_Input_CzechRepublic_20210731.csv'\n",
        "col = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type',\\\n",
        "       'Contract start date', 'Contract end date', 'Status of contract',\\\n",
        "       'Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart',\\\n",
        "       'Classification of Tenant', 'Annual amount - billing currency',\\\n",
        "       'Billing Currency', 'Annual Amount in CZK', 'Amount in EUR',\\\n",
        "       'Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "       'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key',\\\n",
        "       'Classification', 'Residual Period in Years', 'Remarks',\\\n",
        "       'Count of unique key', 'Count of contracts', 'Scoping classification',\\\n",
        "       'DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "       'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x',\\\n",
        "       'SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count','Quota of Unique Key','unamed']\n",
        "\n",
        "#ta = pd.read_csv(path_ta_input, header=0, names=col)\n",
        "dates = ['Contract start date', 'Contract end date', 'Update in month']\n",
        "interger = ['Residual Period in Years','Count of unique key', 'Count of contracts', 'Unique Tenant Count',\\\n",
        "            'Quota of Unique Key']\n",
        "amount = ['Annual Amount in CZK', 'Amount in EUR']\n",
        "ta_bill_cols = ['Contract start date','Contract end date',\\\n",
        "                'Counterpart','Classification of Tenant']\n",
        "ta = read_files('/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx', 'Tenant ',col, 0, 0, 'Code', dates, interger,amount, ta_bill_cols)\n",
        "\n",
        "ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "\n",
        "ta = ta.reindex(columns=ta_cols)\n",
        "ta = ta.rename(columns={'unamed': '',\n",
        "                        'unamed_2': ''})\n",
        "\n",
        "#ta.to_excel('/content/TA_Input_CzechRepublic_20210831.xlsx', index=False)\n",
        "ta.to_csv('/content/TA_Input_CzechRepublic_20210831.csv', index=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbxbGMi7F9aL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6cf92cb-39ee-4e46-d2f1-37814acf7a21"
      },
      "source": [
        "ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "len(ta_cols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqMPxE4jijDQ"
      },
      "source": [
        "ta_msa = pd.read_csv('/content/TA_Input_CzechRepublic_20210731.csv')\n",
        "msa_cols = list(ta_msa.columns)\n",
        "ta_cols = list(ta.columns)\n",
        "df_cols = check_columns_received(ta, msa_cols)\n",
        "df_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqSk2BMDCK6W",
        "outputId": "ba7c4578-b3f7-479a-ff25-3c1c954cece8"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index,start_date,end_date):\n",
        "    df[start_date] = pd.to_datetime(df[start_date],errors='coerce')\n",
        "    df[end_date] = pd.to_datetime(df[end_date],errors='coerce')\n",
        "    filtered = df[df[start_date] > df[end_date]]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "\n",
        "df_ta_dates = check_lc_ta_dates(ta,'Code', 'Contract start date', 'Contract end date')\n",
        "df_ta_dates"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4BraJ7gDw5V",
        "outputId": "1b7b78dc-a1b9-4bc1-fc57-a8df925663f9"
      },
      "source": [
        "ta['Classification of Tenant'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MNO     323\n",
              "OTMO    295\n",
              "Name: Classification of Tenant, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYc-9mpk18Ip"
      },
      "source": [
        "Script to read LC Input file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33z-44v_EOl1"
      },
      "source": [
        "def read_files(path, sheetname,col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int, cols_amount, bill_cols, format='mix', type_date=\"%d/%m/%Y\"):\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname,header=0, names = col_name, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_amount, 0)\n",
        "    for col in cols_amount:\n",
        "        #df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].apply(lambda x: '{:0,.2f}'.format(x))\n",
        "\n",
        "    df = df.fillna('')\n",
        "    replace_values(df, bill_cols)\n",
        "    date_parser(df, cols_date, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    return df\n",
        "\n",
        "def replace_values_ta(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', ]\n",
        "    #lista = []\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        df[column] = df[column].fillna(0)\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "col_names = ['Contract ID - NEW', 'FIN ID', 'Contract Crncy', 'Contract Type', 'Freq.', 'Freq. Unit', 'Indexation',\\\n",
        "         'Index upon request', 'Counterpart ID', 'Counterpart', 'LC Amount CZK\\nyearly', 'Amount in EUR yearly (Actual)', \\\n",
        "         'Payment terms Code', 'VAT Subject', '% of VAT', 'Contr. Start date', 'Contr. 1st End', 'Contr. Term End', \\\n",
        "         'Sublease Consession', 'Renewal Option', 'Unique Key', 'Count of Key', 'Count of ContrBct ', \\\n",
        "         'Remarks - Consistency check', 'Residual Period in years', 'Scoping classification', 'DAS sites', \\\n",
        "         'DAS ownership', '31_12_9999', 'Unique Key1', 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021', \\\n",
        "         'Check LC amount', 'Updated Item\\nAmount yearly', 'Contr. 1st End as of 31.05.2021', \\\n",
        "         'Contr. 1st End as of 30.06.2021', 'Check Contr. 1st End', 'Updated Item\\n1st End date', 'Comment', \\\n",
        "         'Comment date', 'Date']\n",
        "\n",
        "#ta = pd.read_csv(path_ta_input, header=0, names=col)\n",
        "dates_lc = ['Contr. Start date', 'Contr. 1st End', 'Contr. 1st End as of 31.05.2021', 'Contr. 1st End as of 30.06.2021']\n",
        "interger_lc = []\n",
        "amount_lc = [\"Amount in EUR yearly (Actual)\", 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021']\n",
        "lc_bill_cols = ['Contract ID - NEW','Counterpart']\n",
        "lc = read_files('/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx', 'Final data_Lease',col_names, 1, 0, 'FIN ID', \\\n",
        "                dates_lc, interger_lc,amount_lc, lc_bill_cols)\n",
        "\n",
        "col_order_lc = [\"Contract ID - NEW\",\"FIN ID\",\"Contract Crncy\",\"Contract Type\",\"Freq.\",\"Freq. Unit\",\"Indexation\",\\\n",
        "                \"Index upon request\",\"Counterpart ID\",\"Counterpart\",\"LC Amount CZK\\nyearly\",\"Amount in EUR yearly (Actual)\",\\\n",
        "                \"Payment terms Code\",\"VAT Subject\",\"% of VAT\",\"Contr. Start date\",\"Contr. 1st End\",\"Contr. Term End\",\\\n",
        "                \"Sublease Consession\",\"Renewal Option\",\"Unique Key\",\"Count of Key\",\"Count of ContrBct \",\\\n",
        "                \"Remarks - Consistency check\",\"Residual Period in years\",\"Scoping classification\",\"DAS sites\",\\\n",
        "                \"DAS ownership\",\"31_12_9999\",\"Unique Key1\",'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021',\\\n",
        "                \"Check LC amount\",\"Updated Item\\nAmount yearly\",'Contr. 1st End as of 31.05.2021','Contr. 1st End as of 30.06.2021',\\\n",
        "                \"Check Contr. 1st End\",\"Updated Item\\n1st End date\",\"Comment\",\"Comment date\",\"Date\"]\n",
        "lc = lc[col_order_lc]\n",
        "lc.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i4SMB8fY8Hs"
      },
      "source": [
        "lc.to_excel('/content/LC_Input_CzechRepublic_20210831.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVsiMGPtbHWo"
      },
      "source": [
        "lc.to_csv('/content/LC_Input_CzechRepublic_20210831.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBLRb-ydh9uM"
      },
      "source": [
        "def change_incorret(df, df_index, col_chg, incorrect_value, new_value):\n",
        "    list_sites = list(df[df[col_chg]==incorrect_value][df_index])\n",
        "    df.loc[df[df_index].isin(list_sites), col_chg] = new_value\n",
        "    print(df.loc[df[df_index].isin(list_sites)][[df_index, col_chg]])\n",
        "\n",
        "change_incorret(lc, \"Contract ID - NEW\", '% of VAT', 0.21,'21%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f85vYdkE1_NL"
      },
      "source": [
        "def check_amounts_lc(df_check, df_index, columns, pattern=','):\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    filtered = df_check[[df_index]]\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        #df_new = pd.merge(df_new, filtered, how='left', left_on='identification - site key', right_on=tw_index)\n",
        "        df_new = df_new.set_index(df_index)\n",
        "        #df_new = df_new[[status_col]+ df_new.columns[:-1].tolist()]\n",
        "        #df_new = df_new[['identification - site key', status_col]]\n",
        "        return df_new\n",
        "    else: \n",
        "        print('\\nNo one columns with incorrect Amount format!\\n') \n",
        "\n",
        "lc_cols = [\"FIN ID\", 'LC Amount CZK\\nyearly']  \n",
        "\n",
        "df_lc_amount = check_amounts_lc(lc, \"FIN ID\", lc_cols)\n",
        "df_lc_amount\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjcRUHSCO0_W",
        "outputId": "84313014-d21d-4f09-8e67-a8104e65d064"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index,start_date,end_date):\n",
        "    df[start_date] = pd.to_datetime(df[start_date],errors='coerce')\n",
        "    df[end_date] = pd.to_datetime(df[end_date],errors='coerce')\n",
        "    filtered = df[df[start_date] > df[end_date]]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "        \n",
        "df_lc_dates = check_lc_ta_dates(lc,\"FIN ID\", \"Contr. Start date\",\"Contr. 1st End\")\n",
        "df_lc_dates\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}