{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cz_validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMveZdmTSjSiwcUMt2w5j5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emerenan/xlsx_validation/blob/main/cz_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGZCe9A8rnZN",
        "outputId": "d709b4f3-5d0d-475f-9366-ed7e1abaa2e0"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDANxxNd5efG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "from unidecode import unidecode\n",
        "\n",
        "def csv_files(path):\n",
        "    \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "    import csv\n",
        "    f = open(path, encoding='windows-1252', errors='ignore')\n",
        "    data = []\n",
        "    for row in csv.reader(f, delimiter=','):\n",
        "        data.append(row)\n",
        "    col = [*data[0]]\n",
        "    data.pop(0)\n",
        "    df = pd.DataFrame(data, columns=col)\n",
        "    return df, col\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index):\n",
        "    \"\"\"\n",
        "    Params:\\n\n",
        "    path: parth of file in the computer.\\n\n",
        "    n_skiprows: Number of rows to delete in the original file,.\\n\n",
        "    columns_to_convert: Columns to convert the data general type. \\n\n",
        "    n_skipcolumn: Columns to skip in the original file. \\n\n",
        "    endrow = pass 0 to read everything, 1 to count entire\n",
        "    columns_order: List of columns names in specific order to pass in the engine.\\n\n",
        "    \"\"\"\n",
        "    if date_format == '-':\n",
        "        format = lambda x: datetime.strptime(x, \"%d-%m-%Y\")\n",
        "    else:\n",
        "        format = lambda x: datetime.strptime(x, \"%d/%m/%Y\")\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows).fillna('')\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    \"\"\"cont = 0\n",
        "    for i in df.iloc[:,site_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[:cont, :]\n",
        "    #df.columns = columns_order\"\"\"\n",
        "    \n",
        "    # convert intery columns to integer \n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].fillna(0)\n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].astype('int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def check_df(df, error_msg):\n",
        "    if df == None or df.empty:\n",
        "        return f'{error_msg}'\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def count_duplicates(lista):\n",
        "    count_dict = {}\n",
        "    for entry in lista:\n",
        "        if entry in count_dict.keys():\n",
        "            count_dict[entry] += 1\n",
        "        else:\n",
        "            count_dict[entry] = 1\n",
        "    \n",
        "    duplicates = {}\n",
        "    for k, v in count_dict.items():\n",
        "        if v > 1:\n",
        "            duplicates[k] = v\n",
        "    return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "def defining_df(df, column_range, number_col):\n",
        "    cont = 0\n",
        "    for i in df.iloc[:,number_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[0:cont, :]\n",
        "    return df\n",
        "\n",
        "#old version\n",
        "def check_columns(table, output_columns):\n",
        "    \"\"\"\n",
        "    Check the total of number of missing columns and the missing columns in passed table.\\n\n",
        "\n",
        "    Params:\\n\n",
        "    table: contain the columns to be check.\\n\n",
        "    output_columns: columns structure at the final file. \n",
        "\n",
        "    Returns:\\n\n",
        "    Number of missing columns and a list that contains the name os missing columns.\n",
        "    \"\"\"\n",
        "    total_received = len(table.columns)\n",
        "    number_missing_columns = 0\n",
        "    missing_columns = []\n",
        "    #Counting of missing columns       \n",
        "    #if country in contries:      \n",
        "    for columns in output_columns:\n",
        "        if columns.lower() not in [labels.lower() for labels in table]:\n",
        "            number_missing_columns +=1\n",
        "            missing_columns.append(columns)\n",
        "    \n",
        "    return total_received, number_missing_columns, missing_columns\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna('', inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "\n",
        "def date_parser(df, columns, t=1, format=1, type_dates='normal'):\n",
        "    t_col = type_dates.lower()\n",
        "    if format == 1:\n",
        "        type_date = \"%d/%m/%Y\"\n",
        "    else:\n",
        "        type_date = \"%d-%m-%Y\"\n",
        "    for column in columns:\n",
        "        if t == 1:\n",
        "            df[column] = pd.to_datetime(df[column], errors='coerce').fillna('')\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "# Refactorar esse codigo para receber todas as colunas num dic\n",
        "# Sendo as keys=columns e values= picklist for each column\n",
        "def check_date_columns(df, df_index,status_col,columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "\n",
        "def check_amounts(df_check, df_index, status_col, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    filtered = df_check[[df_index, status_col]]\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        df_new = pd.merge(df_new, filtered, how='left', left_on='identification - site key', right_on=tw_index)\n",
        "        df_new = df_new.set_index(tw_index)\n",
        "        df_new = df_new[[status_col]+ df_new.columns[:-1].tolist()]\n",
        "        #df_new = df_new[['identification - site key', status_col]]\n",
        "        return df_new\n",
        "    else: \n",
        "        print('\\nNo one columns with incorrect Amount format!\\n')\n",
        "        \n",
        "def check_picklist(df,df_index,df_status, df_cols, picklist_dict):\n",
        "\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        columns = [i.lower() for i in picklist_dict.keys()]\n",
        "        for column in set(columns): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            #print(value)\n",
        "            if not value in set(picklist_dict[column]) or pd.isnull(value):\n",
        "                #print(set(picklist_dict[column]))\n",
        "\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "                else:\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "\n",
        "    #df = df_errors.dropna()   \n",
        "    df_errors = df_errors.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    if len(df_errors)>0:\n",
        "        df = df[[df_index, df_status]]\n",
        "        df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "        df_errors = df_errors.reset_index()\n",
        "    else:\n",
        "        print('\\nNo one Picklist Error Founded!')\n",
        "    return df_errors\n",
        "\n",
        "def check_picklist_V1(df,df_index,df_status,picklist_dict):\n",
        "    log = {}\n",
        "\n",
        "    #picklist_dict={'Categorization by Transmission Sys':['Long-Term Mobile Sites','Macro','Non-Enterprise DAS','Repeater Sites','Public DAS Sites','Transmission']}\n",
        "    for column in picklist_dict:\n",
        "        df_aux=df.copy()\n",
        "        #print(type(picklist_dict[column]))\n",
        "        new=df_aux[column].isin(picklist_dict[column])\n",
        "        #print(new)\n",
        "        indexes=df.index[new == False].tolist()\n",
        "        if column not in log:log[column]=[]\n",
        "        log[column]=log[column]+indexes\n",
        "    newDict = {}\n",
        "\n",
        "    for key,value in log.items():\n",
        "      for val in value:\n",
        "          ID=df.iloc[val][df_index]\n",
        "          if ID in newDict:\n",
        "              v=f'Incorret picklist value: {df.iloc[val][key]}'\n",
        "              if df.iloc[val][key]!=df.iloc[val][key] or pd.isnull(df.iloc[val][key]) or  pd.isna(df.iloc[val][key]) or df.iloc[val][key]=='' or df.iloc[val][key]=='nan':\n",
        "                   v='Blank Value'\n",
        "              newDict[ID][key]=v\n",
        "          else:\n",
        "              v=f'Incorret picklist value: {df.iloc[val][key]}'\n",
        "              if df.iloc[val][key]!=df.iloc[val][key] or pd.isnull(df.iloc[val][key]) or  pd.isna(df.iloc[val][key]) or df.iloc[val][key]=='' or df.iloc[val][key]=='nan':\n",
        "                   v='Blank Value'\n",
        "              newDict[ID] = {'status': df.iloc[val][df_status],key:v}\n",
        "    #print(newDict)\n",
        "    logs = pd.DataFrame.from_dict(newDict, orient='index')\n",
        "    logs.index.name='Site'\n",
        "    logs = logs.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    return logs\n",
        "\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [str(i) for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uis = [str(i) for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index].sort_values() if i not in uip_list]\n",
        "    bts_out_uis = pd.DataFrame(bts_out_uis, columns=['Bts_Sites_Out_UIS_File'])\n",
        "    bts_out_uis = pd.merge(bts_out_uis, filtered, how='left', left_on=['Bts_Sites_Out_UIS_File'], right_on=tw_index)\n",
        "    bts_out_uis = bts_out_uis[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "    if not (new_sites.empty or bts_out_uis.empty or df_site_bts.empty):\n",
        "        return new_sites, bts_out_uis, df_site_bts[[tw_index,status_col, bts_col, bill_col]]\n",
        "    else:\n",
        "        print('\\n No errors founded!\\n')\n",
        "\n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, status_col, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    filtered = df_tw[[tw_index, status_col]]\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    if not df.empty:\n",
        "        df = pd.merge(df, filtered, how='left', left_on=['New Sites'], right_on=tw_index)\n",
        "        df = df[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "        return df\n",
        "    else: \n",
        "        print('\\nNo errors founded!\\n')\n",
        "\n",
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, df_msa, msa_index, msa_wip):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[msa_wip]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "    \n",
        "    tw_wip_sites = df_tw[df_tw[wip_tw]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "    \n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, wip_tw, tw_bts]]\n",
        "    \n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    \n",
        "    if not(len(wip_out_tw_list)==0 or tw_wip_site_bts_flagged.empty):\n",
        "        return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "\n",
        "    # Falta os outros países\n",
        "\n",
        "def check_decommissioned(df,df_index,status_col, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[df_index,status_col, decom_col, doer_col]]\n",
        "    else:\n",
        "        print('\\nNo errors Founded!\\n')\n",
        "    \n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    df_tw = df_tw[df_tw[status_col]==status][[tw_index, status_col, date_col]] \n",
        "    #print(df_tw)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[df_tw[date_col].astype('datetime64[ns]') < current_date]\n",
        "            return filtered\n",
        "        else:\n",
        "            #bill_dates = pd.to_datetime(df_tw[date_col], errors==coerrce)\n",
        "            filtered = df_tw[df_tw[date_col]=='']\n",
        "            return filtered\n",
        "    else:\n",
        "        print('\\nNo errors founded!\\n')\n",
        "\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col,bill_col, status_col, dt_format):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date, format=dt_format)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        df_tw[bill_col] = pd.to_datetime(df_tw[bill_col],errors='coerce', format=dt_format)\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)&\\\n",
        "                         (df_tw[bill_col].astype('datetime64[ns]') < current_date)]\n",
        "        return filtered[[tw_index, status_col, bill_col, doer_col]]  \n",
        "    else:\n",
        "        print('\\nNothing wrong in services sites!\\n')\n",
        "                                                              \n",
        "def check_mom_bts(df_tw, tw_index,status_col, tw_col, df_msa, msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index,status_col, tw_col]]    \n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')      \n",
        "\n",
        "def check_lc_ta_dates(df,tw_index,status_col, start_date,end_date):\n",
        "    filtered = df[df[start_date] > df[end_date]]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index,status_col, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "    t1 = ['pt', 'de', 'cz', 'ie', 'es', 'ro', 'hu']\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    if country.lower() in t1:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIS File!'])\n",
        "        in_service_uip_sites = pd.merge(in_service_uip_sites, filtered, how='left', left_on='Site In service out of UIS File!',\\\n",
        "                                        right_on=tw_index)\n",
        "        in_service_uip_sites = in_service_uip_sites[['Site In service out of UIS File!', status_tw_col]]\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        if decom_col != \"\":\n",
        "            tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "            decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "            decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "        \n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "            bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIPS File', status_tw_col]]\n",
        "            if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "                return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "        else:\n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "            bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "            if not (in_service_uip_sites.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "                return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIS File!'])\n",
        "        in_service_uip_sites = pd.merge(in_service_uip_sites, filtered, how='left', left_on='Site In service out of UIS File!',\\\n",
        "                                        right_on=tw_index)\n",
        "        in_service_uip_sites = in_service_uip_sites[['Site In service out of UIS File!', status_tw_col]]\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "        decomiss_sites_in_uip = []\n",
        "        if set(tw_decomiss).intersection(uip_sites):\n",
        "            decomiss_sites_in_uip = [i for i in tw_decomiss if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(decomiss_sites_in_uip, columns=['Decomissioned Site in UIS File'])\n",
        "        decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip = [i for i in df_uip['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Yes'][tw_index]\n",
        "        critical = []\n",
        "        if set(uip).intersection(bts_tw_critical):\n",
        "            critical = [i for i in bts_tw_critical if i not in uip]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% out in UIS File'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "        if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "            return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "        #lista = []\n",
        "\n",
        "        df.fillna(\"\", inplace=True)\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df\n",
        "\n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    if not df_commercial_diffs.empty:\n",
        "        return df_commercial_diffs\n",
        "    else:\n",
        "        print('\\nNo Errors founded!')\n",
        "\n",
        "def check_diffs_v2(path_current, path_last, cols_order, type_file='Excel', sheet='Commercial'):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def highlight_diff(data, color='yellow'):\n",
        "        attr = 'background-color: {}'.format(color)\n",
        "        other = data.xs('Current', axis='columns', level=-1)\n",
        "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                            index=data.index, columns=data.columns)\n",
        "    type_file = type_file.lower()\n",
        "    #if type_file =='excel':\n",
        "    _actual = pd.read_excel(path_current,sheet_name=sheet).fillna('')\n",
        "    _before = pd.read_excel(path_last,sheet_name=sheet).fillna('')\n",
        "\n",
        "    df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "    df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "    #df_final.style.apply(highlight_diff, axis=None)\n",
        "    if not df_final.empty:\n",
        "        return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "    else:\n",
        "        print('\\nNo differences Founded!\\n')\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() \n",
        "\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        #Ajustado só para CZ_Julho, tirei o encoding\n",
        "        df_NEW = pd.read_csv(path_NEW).fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = fit_df(path_OLD,bill_cols)\n",
        "        df_NEW = fit_df(path_NEW,bill_cols)\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "        status_1 = f'{status_col}_current'\n",
        "        status_2 = f'{status_col}_before'\n",
        "        new_copy = new_copy.set_index('sites')\n",
        "        new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} old_file vs new_file.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXC51sanKFX5"
      },
      "source": [
        "Comparing Files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HMh8Aa46QUv"
      },
      "source": [
        "def find_diffs_between_files_1(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, cols_conv, type_file='mix',status_col='', \\\n",
        "                             kind='tw', dates_cols=[], kind_col='',type_date=\"%d/%m/%Y\", sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col,cols_conv, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col] \n",
        "            #df.columns = lower_str(list(df.columns))\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            for col in cols_conv:\n",
        "                df[col] = df[col].astype(str).apply(unidecode)\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df = df.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "            #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']]\n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    cols_conv = lower_str(cols_conv)\n",
        "    dates_cols = lower_str(dates_cols)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        date_parser(df_OLD, dates_cols,format , type_date)\n",
        "        df_OLD = fit_df(df_OLD,index_col,cols_conv,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols,format , type_date)\n",
        "        df_NEW = fit_df(df_NEW,index_col,cols_conv,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='ISO-8859-1').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        #df_OLD = df_OLD[lower_str(bill_cols)]\n",
        "        df_OLD = fit_df(df_OLD, index_col, cols_conv,kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='ISO-8859-1').fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,cols_conv, kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD) header=0, names=cols_old,\n",
        "        df_OLD = pd.read_csv(path_OLD,encoding='ISO-8859-1').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col,cols_conv, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        #print(df_NEW.columns)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols,format , type_date)\n",
        "        df_NEW = fit_df(df_NEW, index_col,cols_conv, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "bill_col= ['Code (Duplicate)',\\\n",
        "            \"Site Status\",\\\n",
        "            \"Categorization by Transmission Sys\",\\\n",
        "            \"Categorization by Site Type\",\\\n",
        "            'Strategic Macro Sites',\\\n",
        "            'Critical Sites',\\\n",
        "            'Case A Core Site',\\\n",
        "            'Macro Site - Transmission Hub Site',\\\n",
        "            'Macro Site - Transmission Hub Site with/without Shelters',\\\n",
        "            'Transmission Sites',\\\n",
        "            'Room Configuration',\\\n",
        "            'Power Supply',\\\n",
        "            'Air Conditioning',\\\n",
        "            'Active Sharing Arrangements involving the Operator',\\\n",
        "            'VF-CZ Demerger phase',\\\n",
        "            \"Billing Trigger date \",\\\n",
        "            'Strategic_Site_Bucket',\\\n",
        "            'Critical Site Bucket',\\\n",
        "            'Wip_Site',\\\n",
        "            'Bts_Site',\\\n",
        "            'Sites_As_Metered_Estimated',\\\n",
        "            'Subsequent_Sharing_Arrangement',\\\n",
        "            \"First_Active_Sharing_Deployment_Type\",\\\n",
        "            \"First_Active_Sharing_Start_Date\",\\\n",
        "            \"First_Active_Sharing_End_Date\",\\\n",
        "            \"Decommissioned_Site\",\\\n",
        "            \"Transfer_Date_Of_Consent_Required_Sites\",\\\n",
        "            \"Date_Of_Equipment_Removal\"]\n",
        "pathtw = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "pathold = '/content/TowerDB_CzechRepublic_20210731.csv'\n",
        "tw_save = '/content/CZ_TW'\n",
        "tab = 'Tower DB'\n",
        "row = 3\n",
        "old_n = 'twdb_last.csv'\n",
        "new_n = 'twdb_current.xlsx'\n",
        "cols_con = ['Province','Municipality','Address']\n",
        "dates = ['Billing Trigger date ','infrastructure ready (existing)/ to be ready (new)',\\\n",
        "         'First_Active_Sharing_Start_Date',\\\n",
        "         'First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "         'Date_Of_Equipment_Removal']  \n",
        "\n",
        "find_diffs_between_files_1(pathold, pathtw, 'Code (Duplicate)', bill_col,tw_save, old_n,new_n,cols_con,'mix',\\\n",
        "                           status_col=\"Site Status\" , kind='tw', dates_cols=dates , kind_col='', sheetname=tab, skipr = row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "sSFDJzKG0c20",
        "outputId": "7135ba70-20e1-4f82-c179-8ae0975ae204"
      },
      "source": [
        "# Read CZ towerdb files\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index, cols_number, col_dates, format='normal', type_date=\"%d/%m/%Y\"):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ']\n",
        "        #lista = []\n",
        "\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(0)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df\n",
        "\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    date_parser(df, col_dates, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_number)\n",
        "\n",
        "    for col in cols_number:\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "    \n",
        "    df = df.fillna('')\n",
        "\n",
        "    return df\n",
        "\n",
        "path_towerdb = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "towerdb_col = [\"Code (Duplicate)\",\"Site Status\",\"VF - In scope / out of scope (Generalised scoping)\",\"Site in Skylon scope Actual (From Site List Sheet )\",\\\n",
        "               \"Legacy Site Code(Duplicate)\",\"TIMS Site Code\",\"Legacy Site Code\",\"Site Name\",\"Macro Region\",\"Province\",\"Municipality\",\"Inhabitants\",\"Address\",\\\n",
        "               \"Ground Register\",\"Altitude\",\"Latitude\",\"Longitude\",\"Categorization by Inhabitants\",\"Categorization by Transmission Sys\",\\\n",
        "               \"Categorization by Site Type\",\"Categorization by Transmission Sys (subcluster)\",\"Other internal Categorization 1 (Identify ACQ Sites)\",\\\n",
        "               \"Other internal Categorization 2 Energy provider (Eon/ LL)\",\"DAS+Macro\",\"DAS (Yes/ No)\",\"DAS Ownership (Complete/ Partial/ 3rd Party)\",\\\n",
        "               \"Active/ Passive DAS\",\"# of remote units/ radiating points\",\"Type of Structure\",\"Distance highest antenna to ground level\",\"GBT Tower height\",\\\n",
        "               \"POD ID\",\"Energy Consumption LTM (kwh)\",\"Annual Energy cost LTM (Euros)\",\"Infrastructure ready (existing)/ to be ready (new)\",\\\n",
        "               \"Infrastructure to be dismantled by\",\"Radio equipments to be deactivated by\",\"Infrastructure to be shared by\",\"Technology VOD\",\"Fibre / Microwave\",\\\n",
        "               \"Vertical passive structure owner\",\"Room configuration (detailed)\",\"Shelter passive structure ownership\",\"Type of Air Conditioning\",\\\n",
        "               \"Number of cabinets (Full Capacity)\",\"Number of Antenna (Full Capacity)\",\"Number of MW (Full Capacity)\",\"Counterpart\",\"# of Lease Contracts\",\\\n",
        "               \"Current annual lease fees \",\"Current other fees (Maintenance)\",\"Current other fees\",\"(Average) residual duration - Lease contract\",\\\n",
        "               \"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\"# of Tenants Agreements\",\\\n",
        "               \"Current Total Annual Hosting Fees\",\"Tenant (name/ID) MNO1 (Česká telekomunikační infrastruktura a.s.)\",\"Annual Fee per Tenant MNO1\",\\\n",
        "               \"Annual Energy Fee MNO1\",\"Annual Maintenance Fee MNO1\",\"Other Services Fee MNO1\",\"Residual duration MNO1 (Years)\",\\\n",
        "               \"Tenant (name/ID) MNO2 (T-Mobile Czech Republic a.s.)\",\"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\"Annual Maintenance Fee MNO2\",\\\n",
        "               \"Other Services Fee MNO2\",\"Residual duration MNO2 (days)\",\"Tenant (name/ID) MNO3\",\"Annual Fee per Tenant MNO3\",\"Annual Energy Fee MNO3\",\\\n",
        "               \"Annual Maintenance Fee MNO3\",\"Other Services Fee MNO3\",\"Residual duration MNO3\",\"# of OTMOs\",\"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\\\n",
        "               \"Annual Maintenance Fee OTMOs\",\"Other Services Fee OTMOs\",\"Average residual duration (days)\",\"Check\",\"Strategic Macro Sites\",\"Critical Sites\",\\\n",
        "               \"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\"Transmission Sites\",\\\n",
        "               \"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\"VF-CZ Demerger phase\",\\\n",
        "               \"EVO Location [FAR Site ID] \",\"Billing Trigger date \",\"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\\\n",
        "               \"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\"Wip_Site\",\\\n",
        "               \"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"]\n",
        "towerdb_col = lower_str(towerdb_col)\n",
        "dates = ['Billing Trigger date ', \"Infrastructure ready (existing)/ to be ready (new)\", \"Infrastructure to be dismantled by\",\\\n",
        "         'First_Active_Sharing_Start_Date','First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "         'Date_Of_Equipment_Removal']\n",
        "num_parse = [\"Inhabitants\", \"Current annual lease fees \",\"(Average) residual duration - Lease contract\",\\\n",
        "             \"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\\\n",
        "             \"# of Tenants Agreements\",\"Current Total Annual Hosting Fees\", \"Annual Fee per Tenant MNO1\",\\\n",
        "             \"Annual Energy Fee MNO1\", \"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\\\n",
        "             \"Residual duration MNO2 (days)\", \"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\\\n",
        "             \"Other Services Fee OTMOs\",\"Average residual duration (days)\"]\n",
        "tab = 'Tower DB'\n",
        "row = 3\n",
        "towerdb = read_files(path_towerdb,tab, 3, 0,\"Code (Duplicate)\",num_parse,dates)\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "towerdb = towerdb[towerdb_col]\n",
        "\n",
        "path_msa = '/content/TowerDB_CzechRepublic_20210731 (3).csv'\n",
        "msa = pd.read_csv(path_msa,encoding='ISO-8859-1').fillna('')\n",
        "msa_cols = lower_str(list(msa.columns))\n",
        "msa.columns = msa_cols\n",
        "\n",
        "\"\"\"Check columns received\"\"\"\n",
        "df_cols = check_columns_received(towerdb, msa_cols)\n",
        "df_cols\n",
        "#No colum\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column(s) Missing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ï»¿code (duplicate)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tenant (name/id) mno1 (äeskã¡ telekomunikaän...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Column(s) Missing\n",
              "0                                ï»¿code (duplicate)\n",
              "1  tenant (name/id) mno1 (äeskã¡ telekomunikaän..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoqB1yjZ0n1r"
      },
      "source": [
        "towerdb.to_excel('/content/TowerDB_CzechRepublic_20210831.xlsx', index=False)"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKLpXMuub_65"
      },
      "source": [
        "towerdb.to_csv('/content/TowerDB_CzechRepublic_20210831.csv', index=False)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhdRKjcRBC4b"
      },
      "source": [
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = \"code (duplicate)\"\n",
        "tw_doer = \"date_of_equipment_removal\"\n",
        "tw_status = \"site status\"\n",
        "tw_bts = 'bts_site'\n",
        "tw_bill = \"billing trigger date \"\n",
        "tw_wip = 'wip_site'\n",
        "tw_decom = 'decommissioned_site'\n",
        "tw_critical = 'critical sites'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "2BXkWi_-4P3v",
        "outputId": "ba841b5f-a6a3-46f9-edde-5dadd323c37b"
      },
      "source": [
        "path_msa = '/content/TowerDB_CzechRepublic_20210731 (3).csv'\n",
        "msa = pd.read_csv(path_msa,encoding='ISO-8859-1').fillna('')\n",
        "cols_con = ['Province','Municipality','Address']\n",
        "for col in cols_con:\n",
        "   msa[col] = msa[col].astype(str).apply(unidecode)\n",
        "msa = msa.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "msa.columns = lower_str(towerdb_col)\n",
        "msa.head(1)\n",
        "#msa.columns = msa_cols\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code (duplicate)</th>\n",
              "      <th>site status</th>\n",
              "      <th>vf - in scope / out of scope (generalised scoping)</th>\n",
              "      <th>site in skylon scope actual (from site list sheet )</th>\n",
              "      <th>legacy site code(duplicate)</th>\n",
              "      <th>tims site code</th>\n",
              "      <th>legacy site code</th>\n",
              "      <th>site name</th>\n",
              "      <th>macro region</th>\n",
              "      <th>province</th>\n",
              "      <th>municipality</th>\n",
              "      <th>inhabitants</th>\n",
              "      <th>address</th>\n",
              "      <th>ground register</th>\n",
              "      <th>altitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>categorization by inhabitants</th>\n",
              "      <th>categorization by transmission sys</th>\n",
              "      <th>categorization by site type</th>\n",
              "      <th>categorization by transmission sys (subcluster)</th>\n",
              "      <th>other internal categorization 1 (identify acq sites)</th>\n",
              "      <th>other internal categorization 2 energy provider (eon/ ll)</th>\n",
              "      <th>das+macro</th>\n",
              "      <th>das (yes/ no)</th>\n",
              "      <th>das ownership (complete/ partial/ 3rd party)</th>\n",
              "      <th>active/ passive das</th>\n",
              "      <th># of remote units/ radiating points</th>\n",
              "      <th>type of structure</th>\n",
              "      <th>distance highest antenna to ground level</th>\n",
              "      <th>gbt tower height</th>\n",
              "      <th>pod id</th>\n",
              "      <th>energy consumption ltm (kwh)</th>\n",
              "      <th>annual energy cost ltm (euros)</th>\n",
              "      <th>infrastructure ready (existing)/ to be ready (new)</th>\n",
              "      <th>infrastructure to be dismantled by</th>\n",
              "      <th>radio equipments to be deactivated by</th>\n",
              "      <th>infrastructure to be shared by</th>\n",
              "      <th>technology vod</th>\n",
              "      <th>fibre / microwave</th>\n",
              "      <th>...</th>\n",
              "      <th>other services fee mno2</th>\n",
              "      <th>residual duration mno2 (days)</th>\n",
              "      <th>tenant (name/id) mno3</th>\n",
              "      <th>annual fee per tenant mno3</th>\n",
              "      <th>annual energy fee mno3</th>\n",
              "      <th>annual maintenance fee mno3</th>\n",
              "      <th>other services fee mno3</th>\n",
              "      <th>residual duration mno3</th>\n",
              "      <th># of otmos</th>\n",
              "      <th>annual fee from otmos</th>\n",
              "      <th>annual energy fee from otmos</th>\n",
              "      <th>annual maintenance fee otmos</th>\n",
              "      <th>other services fee otmos</th>\n",
              "      <th>average residual duration (days)</th>\n",
              "      <th>check</th>\n",
              "      <th>strategic macro sites</th>\n",
              "      <th>critical sites</th>\n",
              "      <th>case a core site</th>\n",
              "      <th>macro site - transmission hub site</th>\n",
              "      <th>macro site - transmission hub site with/without shelters</th>\n",
              "      <th>transmission sites</th>\n",
              "      <th>room configuration</th>\n",
              "      <th>power supply</th>\n",
              "      <th>air conditioning</th>\n",
              "      <th>active sharing arrangements involving the operator</th>\n",
              "      <th>vf-cz demerger phase</th>\n",
              "      <th>evo location [far site id]</th>\n",
              "      <th>billing trigger date</th>\n",
              "      <th>strategic_site_bucket</th>\n",
              "      <th>critical site bucket</th>\n",
              "      <th>subsequent_sharing_arrangement</th>\n",
              "      <th>first_active_sharing_deployment_type</th>\n",
              "      <th>first_active_sharing_start_date</th>\n",
              "      <th>first_active_sharing_end_date</th>\n",
              "      <th>decommissioned_site</th>\n",
              "      <th>wip_site</th>\n",
              "      <th>bts_site</th>\n",
              "      <th>transfer_date_of_consent_required_sites</th>\n",
              "      <th>sites_as_metered_estimated</th>\n",
              "      <th>date_of_equipment_removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BADA1G</td>\n",
              "      <td>In Service</td>\n",
              "      <td>In-scope</td>\n",
              "      <td>Skylon</td>\n",
              "      <td>BADA1G</td>\n",
              "      <td>CZ-TIMS-41750</td>\n",
              "      <td>BADA1G</td>\n",
              "      <td>BKADA</td>\n",
              "      <td>East</td>\n",
              "      <td>JihomoravskA 1/2  kraj</td>\n",
              "      <td>Blansko</td>\n",
              "      <td>4,590</td>\n",
              "      <td>Adamov, RonovskA! 4/443, 67904</td>\n",
              "      <td>600041</td>\n",
              "      <td>320</td>\n",
              "      <td>49.2917</td>\n",
              "      <td>16.6627</td>\n",
              "      <td>Suburban</td>\n",
              "      <td>Macro</td>\n",
              "      <td>RTT</td>\n",
              "      <td>Standard</td>\n",
              "      <td>No</td>\n",
              "      <td>Power Company-LV</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>Rooftop structure</td>\n",
              "      <td>30.5</td>\n",
              "      <td>-</td>\n",
              "      <td>859182400200612435</td>\n",
              "      <td>11441</td>\n",
              "      <td>2073.185878</td>\n",
              "      <td>19/06/2000</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>2G; 4G; 5G</td>\n",
              "      <td>Microwave</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Non Core</td>\n",
              "      <td>No</td>\n",
              "      <td>Non Transmission Hub Site</td>\n",
              "      <td>No</td>\n",
              "      <td>Outdoor</td>\n",
              "      <td>DC</td>\n",
              "      <td>No</td>\n",
              "      <td>No Active Sharing</td>\n",
              "      <td>Phase 2</td>\n",
              "      <td>CZ62BADA1G</td>\n",
              "      <td></td>\n",
              "      <td>Non Strategic</td>\n",
              "      <td>Non Critical</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>31/03/2023</td>\n",
              "      <td>Estimated Model</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  code (duplicate)  ... date_of_equipment_removal\n",
              "0           BADA1G  ...                          \n",
              "\n",
              "[1 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT75KHTjUyMe"
      },
      "source": [
        "# Check Dates Format (dd-mm-YYYY)\n",
        "\"\"\"Não deve haver NAN ou NAT na coluna, fazer fillna('') antes\"\"\"\n",
        "def check_date_columns(df, df_index,status_col,bts_col, wip_col, columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col,bts_col, wip_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col,bts_col, wip_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "\n",
        "def date_parser(df, columns, t=1, format=1, type_dates='normal'):\n",
        "    t_col = type_dates.lower()\n",
        "    if format == 1:\n",
        "        type_date = \"%d/%m/%Y\"\n",
        "    else:\n",
        "        type_date = \"%d-%m-%Y\"\n",
        "    for column in columns:\n",
        "        if t == 1:\n",
        "            df[column] = pd.to_datetime(df[column], errors='coerce').fillna('')\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "dates_cols = [tw_index,'Billing Trigger date ', 'First_Active_Sharing_Start_Date',\\\n",
        "              'First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "              'Date_Of_Equipment_Removal'] \n",
        "dates_cols = lower_str(dates_cols)\n",
        "\n",
        "dates = ['Billing Trigger date ', 'First_Active_Sharing_Start_Date',\\\n",
        "              'First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "              'Date_Of_Equipment_Removal']           \n",
        "dates = lower_str(dates)\n",
        "\n",
        "#date_parser(towerdb, lower_str(dates), 2, 1, 'mixed')\n",
        "#tw_in_service = tw_in_service[dates_cols]\n",
        "tw_in_service = towerdb[towerdb['site status']=='In Service']\n",
        "df_dates_errors = check_date_columns(tw_in_service,tw_index, tw_status, tw_bts, tw_wip, lower_str(dates_cols), 2)\n",
        "df_dates_errors\n",
        "#Errors em varias colunas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtUVfvVPdP76"
      },
      "source": [
        "Validaton for general sites picklist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUXH4ckBaL4Q"
      },
      "source": [
        "dic_gen = [tw_index,'categorization by transmission sys']\n",
        "pick_gen = {\n",
        "    'categorization by transmission sys': ['Macro','Public DAS','Repeater','Transmission']\n",
        "    }\n",
        "df_pick_general_errors = check_picklist(towerdb, tw_index, tw_status, dic_gen, pick_gen)\n",
        "df_pick_general_errors\n",
        "#Pipeline sites without categorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdgc55EAdJZV"
      },
      "source": [
        "Validation for On air sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMKhNSaiWHnI"
      },
      "source": [
        "sites = list(towerdb[towerdb['air conditioning']=='Yes; Indoor Free air cooling / Free cooling units'][tw_index])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT7nbdJzTFXm"
      },
      "source": [
        "def change_incorret(df, df_index, col_chg, incorrect_value, new_value):\n",
        "    list_sites = list(df[df[col_chg]==incorrect_value][tw_index])\n",
        "    df.loc[df[df_index].isin(list_sites), col_chg] = new_value\n",
        "    print(df.loc[df[df_index].isin(list_sites), col_chg])\n",
        "\n",
        "change_incorret(towerdb, tw_index, 'subsequent_sharing_arrangement', 'NO',\\\n",
        "                'No')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-cTM_dnU0Gk"
      },
      "source": [
        "picklist_cz = {\n",
        "    'categorization by transmission sys': ['Macro','Public DAS','Repeater','Transmission'],\n",
        "    'categorization by site type':['DAS passive','GBT','RTT'],\n",
        "    'strategic macro sites':['Yes','No'],\n",
        "    'critical sites':['Yes','No'],\n",
        "    'case a core site':['Non Core', 'Case A','Case B'],\n",
        "    'macro site - transmission hub site':['Yes','No'],\n",
        "    'macro site - transmission hub site with/without shelters': ['With shelters','Without shelters','Non Transmission Hub Site'],\n",
        "    'transmission sites': ['No','Yes'],\n",
        "    'room configuration': ['Indoor','Outdoor',\"Indoor & Outdoor\"],\n",
        "    'power supply':['AC','DC','AC & DC','No Power'],\n",
        "    'air conditioning': ['No','Yes; Indoor Air Conditioning','Yes; Indoor Free Air cooling / Free cooling units'],\n",
        "    'active sharing arrangements involving the operator':['MORAN (On VF equipment)','MORAN (On non-VF equipment)','MOCN with Spectrum Pooling','Partial Active-Passive','No Active Sharing'],\n",
        "    'vf-cz demerger phase': ['Cancelled','Phase 1','Phase 2','Skylon BTS'],\n",
        "    'strategic_site_bucket': ['Yes - 0-5%','Yes - 5-10%','Non Strategic'],\n",
        "    'critical site bucket': ['Beyond 10%','Within 10%','Non Critical'],\n",
        "    'subsequent_sharing_arrangement': ['Yes','No'],\n",
        "    'decommissioned_site': ['Yes','No'],\n",
        "    'wip_site': ['Yes','No'],\n",
        "    'bts_site': ['Yes','No'],\n",
        "    'sites_as_metered_estimated': ['Estimated Model','Metered Model']\n",
        "}\n",
        "dic_cols = [tw_index,'categorization by transmission sys',\\\n",
        "            'categorization by site type',\\\n",
        "            'strategic macro sites',\\\n",
        "            'critical sites',\\\n",
        "            'case a core site',\\\n",
        "            'macro site - transmission hub site',\\\n",
        "            'macro site - transmission hub site with/without shelters',\\\n",
        "            'transmission sites',\\\n",
        "            'room configuration',\\\n",
        "            'power supply',\\\n",
        "            'air conditioning',\\\n",
        "            'active sharing arrangements involving the operator',\\\n",
        "            'vf-cz demerger phase',\\\n",
        "            'strategic_site_bucket',\\\n",
        "            'critical site bucket',\\\n",
        "            'subsequent_sharing_arrangement',\\\n",
        "            'decommissioned_site',\\\n",
        "            'wip_site',\\\n",
        "            'bts_site',\\\n",
        "            'sites_as_metered_estimated']\n",
        "df_pick_actives_errors = check_picklist(tw_in_service, tw_index, tw_status, dic_cols, picklist_cz)\n",
        "df_pick_actives_errors\n",
        "#Erros em varias colunas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbonEwO4ZW93"
      },
      "source": [
        "Replace - Verificar real necessidade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OijUpqyrZWds"
      },
      "source": [
        "#check specific columns to change 'N/A', 0, and '-' to blank values \n",
        "def replace_values_v1(df, columns, value=\"\"):\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", '-',0, '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    #df.fillna(0, inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        df[column] = df[column].fillna(0)\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "towerdb_bill_cols = ['Code (Duplicate)', 'Categorization by Transmission Sys', 'Categorization by Site Type',\"Strategic Macro Sites\",\\\n",
        "           \"Critical Sites\",\"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\\\n",
        "           \"Transmission Sites\",\"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\\\n",
        "           \"VF-CZ Demerger phase\",\"Billing Trigger date \",\"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\\\n",
        "           \"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\\\n",
        "           \"Wip_Site\",\"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"]\n",
        "towerdb_bill_cols = lower_str(towerdb_bill_cols)\n",
        " \n",
        "replace_values_v1(towerdb, towerdb_bill_cols, value=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXXM5Q0HAwrh"
      },
      "source": [
        "df = towerdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWLLVcK3cPRZ"
      },
      "source": [
        "Month-on-Month BTS sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQJPPtAXZiJr",
        "outputId": "49444112-8d57-4ccc-a88c-736dd0a42193"
      },
      "source": [
        "def check_mom_bts(df_tw, tw_index,status_col, tw_col, df_msa, msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index,status_col, tw_col]]    \n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n') \n",
        "        \n",
        "tw_filtered = towerdb[towerdb['site status']=='In Service']\n",
        "\n",
        "df_mom_bts = check_mom_bts(tw_filtered, tw_index, tw_status, tw_bts, msa, tw_index, tw_bts)\n",
        "df_mom_bts\n",
        "\n",
        "df_mom_decom = check_mom_bts(tw_filtered, tw_index,tw_status, tw_decom, msa, tw_index, tw_decom)\n",
        "df_mom_decom\n",
        "# No errors in Both"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n",
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vv5LQ99m5mc"
      },
      "source": [
        "#Read UIP File\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip = pd.read_excel('/content/UserInput_CzechRepublic_20210630 TrueUp.xlsx',sheet_name='SiteLevel',usecols=[0,2,3],skiprows=3)\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [i for i in msa[tw_index]]\n",
        "tw_sites = [i for i in towerdb[tw_index]]\n",
        "uip_sites = [i for i in uip['Site_ID']]\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY-DtFVHtAJx"
      },
      "source": [
        "Verificar Errors de Sites Numericos com String"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQSv5rvy6Mev"
      },
      "source": [
        "Verificar Errors de Sites Numericos com String"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3NGucZF02V4"
      },
      "source": [
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [i for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uis = [i for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index] if i not in uip_list]\n",
        "    bts_out_uis = pd.DataFrame(bts_out_uis, columns=['Bts_Sites_Out_UIS_File'])\n",
        "    bts_out_uis = pd.merge(bts_out_uis, filtered, how='left', left_on=['Bts_Sites_Out_UIS_File'], right_on=tw_index)\n",
        "    bts_out_uis = bts_out_uis[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    #print(current_date)\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "\n",
        "    return new_sites, bts_out_uis, df_site_bts[[tw_index,status_col, bts_col, bill_col]]\n",
        "\n",
        "new_sites, bts_sites_out_uis, df_bts_demerged = check_new_sites(towerdb, tw_index, tw_bts, tw_bill, tw_status, msa_sites, tw_sites, uip_sites)\n"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zJvPiIH5d6A"
      },
      "source": [
        "towerdb[towerdb[tw_index]==750210]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKSKGy21oJpp"
      },
      "source": [
        "new_sites"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCFe9lJmoLSm"
      },
      "source": [
        "# 13 sites\n",
        "bts_sites_out_uis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROTHG9-NqPcD",
        "outputId": "4693913f-215d-4ec5-ef6b-deab74ad7627"
      },
      "source": [
        "df_wip_errors = check_wip(towerdb, tw_index, tw_wip, tw_bts, msa, tw_index, tw_wip)\n",
        "df_wip_errors"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6HAiaPFrV6F"
      },
      "source": [
        "Validation DOER sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68rdgmyprVgu",
        "outputId": "3e5f25e4-c686-4fc3-a865-d828d136d126"
      },
      "source": [
        "#Validate Decomissioned Sites\n",
        "def check_decommissioned(df,df_index,status_col, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[df_index,status_col, decom_col, doer_col]]\n",
        "    else:\n",
        "        print('\\nNo errors Founded!\\n')\n",
        "\n",
        "df_decom_errors = check_decommissioned(towerdb,tw_index, tw_status, tw_decom, tw_doer)\n",
        "df_decom_errors"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onf4s48gr6Yw"
      },
      "source": [
        "DOER Valitadions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "_bsHWICor963",
        "outputId": "8880e113-434b-4723-aa95-2b1e2ae4343a"
      },
      "source": [
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    df_tw = df_tw[df_tw[status_col]==status][[tw_index, status_col, date_col]] \n",
        "    #print(df_tw)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[df_tw[date_col].astype('datetime64[ns]') < current_date]\n",
        "            return filtered\n",
        "        else:\n",
        "            #bill_dates = pd.to_datetime(df_tw[date_col], errors==coerrce)\n",
        "            filtered = df_tw[df_tw[date_col]=='']\n",
        "            return filtered\n",
        "    else:\n",
        "        print('\\nNo errors founded!\\n')\n",
        "actives  = towerdb[towerdb[tw_status]=='In Service']\n",
        "#Validate sites in service which has DOER less than current date\n",
        "\n",
        "df_doer_errors = check_tw_bill_doer(actives, tw_index, tw_doer, tw_status, 'In Service', 'doer')\n",
        "df_doer_errors"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code (duplicate)</th>\n",
              "      <th>site status</th>\n",
              "      <th>date_of_equipment_removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3900</th>\n",
              "      <td>40710X</td>\n",
              "      <td>In Service</td>\n",
              "      <td>30/08/2020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     code (duplicate) site status date_of_equipment_removal\n",
              "3900           40710X  In Service                30/08/2020"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzT8BJ8NtF9j"
      },
      "source": [
        "TowerDb vs UIS validations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYjPay1Rx7OD"
      },
      "source": [
        "UIS In Month"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80qFRKGovOKf"
      },
      "source": [
        "\"\"\"Check UIP In Month ites matches with Towerdb sites\"\"\"\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    uis_sites_not_in_towerdb = []\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "    if uis_sites_not_in_towerdb:\n",
        "        uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS In Month not active in TowerDB!'])\n",
        "        uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS In Month not active in TowerDB!',\\\n",
        "                                        right_on=tw_index)\n",
        "        uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS In Month not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS In Month!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS In Month!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS In Month!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    if decom_col != \"\":\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "        decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "    \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        \n",
        "        critical = []\n",
        "        if len(bts_tw_critical) > 0:\n",
        "            if set(uip_critical).intersection(bts_tw_critical):\n",
        "                critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "                #print(critical)\n",
        "        if critical:\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIPS File', status_tw_col]]\n",
        "        #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "        return uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        critical = []\n",
        "        if len(uip_critical) > 0:\n",
        "            critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "        #if not (in_service_uip_sites.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "        return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "\n",
        "uis_sites_not_in_towerdb,in_service_uis_sites, decomiss_sites_in_uis, bts_sites_out_uis, critical = check_uip_tw(towerdb,tw_index, tw_status, \\\n",
        "                                                                tw_decom, tw_bts,tw_critical, uip, uip_sites, 'cz')\n",
        "#No errors"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxo8qoPHx2aD"
      },
      "source": [
        "UIS True UP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZoz18kWtQx-"
      },
      "source": [
        "\"\"\"Check UIP sites matches with Towerdb sites\"\"\"\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    uis_sites_not_in_towerdb = []\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "    if uis_sites_not_in_towerdb:\n",
        "        uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS Sites True Up not active in TowerDB!'])\n",
        "        uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS Sites True Up not active in TowerDB!',\\\n",
        "                                        right_on=tw_index)\n",
        "        uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS Sites True Up not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS True Up Sites!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS True Up Sites!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS True Up Sites!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    if decom_col != \"\":\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "        decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "    \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        \n",
        "        critical = []\n",
        "        if len(bts_tw_critical) > 0:\n",
        "            if set(uip_critical).intersection(bts_tw_critical):\n",
        "                critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "                #print(critical)\n",
        "        if critical:\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIPS File', status_tw_col]]\n",
        "        #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "        return uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        critical = []\n",
        "        if len(uip_critical) > 0:\n",
        "            if set(uip_critical).intersection(bts_tw_critical):\n",
        "                critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "        #if not (in_service_uip_sites.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "        return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "\n",
        "\n",
        "uis_sites_not_in_towerdb,in_service_uis_sites, decomiss_sites_in_uis, bts_sites_out_uis, critical = check_uip_tw(towerdb,tw_index, tw_status, \\\n",
        "                                                                tw_decom, tw_bts,tw_critical, uip, uip_sites, 'cz')\n",
        "#No errors"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKOd5LyjuMVc"
      },
      "source": [
        "Commercial Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BClu4PcvuOEo"
      },
      "source": [
        "def check_diffs_v2(path_current, path_last, sheet='Commercial'):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def highlight_diff(data, color='yellow'):\n",
        "        attr = 'background-color: {}'.format(color)\n",
        "        other = data.xs('Current', axis='columns', level=-1)\n",
        "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                            index=data.index, columns=data.columns)\n",
        "    #type_file = type_file.lower()\n",
        "    #if type_file =='excel':\n",
        "    _actual = pd.read_excel(path_current,sheet_name=sheet).fillna('')\n",
        "    _before = pd.read_excel(path_last,sheet_name=sheet).fillna('')\n",
        "\n",
        "    df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "    df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "    #df_final.style.apply(highlight_diff, axis=None)\n",
        "    if not df_final.empty:\n",
        "        return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "    else:\n",
        "        print('\\nNo differences Founded!\\n')\n",
        "path_current = '/content/UserInput_CzechRepublic_20210831.xlsx'\n",
        "path_last = '/content/UserInput_CzechRepublic_20210731.xlsx'\n",
        "df_comm_errors = check_diffs_v2(path_current, path_last,sheet='Commercial')\n",
        "df_comm_errors\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__UxAcFX7JHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d57dfe-7e0e-43c2-e6bd-2b14a6fd5895"
      },
      "source": [
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "        #lista = []\n",
        "\n",
        "        df.fillna(\"\", inplace=True)\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df\n",
        "\n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    #df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    if not df_commercial_diffs.empty:\n",
        "        return df_commercial_diffs\n",
        "    else:\n",
        "        print('\\nNo Errors founded!')\n",
        "\n",
        "names = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', 'MSA Sites (Phase 1)\\nInput_Value',\\\n",
        "        'PMA Sites (Phase 2)\\nInput_Value','Description/Instruction', 'Frequency of Update']\n",
        "merge_cols = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', \\\n",
        "              'Description/Instruction', 'Frequency of Update']\n",
        "cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Data_Type','Input_Value_1_actual',\\\n",
        "                'Input_Value_1_before','Input_Value_2_actual','Input_Value_2_before''Equal Values','Description/Instruction', 'Frequency of Update']\n",
        "# Check for commercial Values into current UIP File and compare with UIP File before\n",
        "\n",
        "df_com = check_commercial(path_current, path_last,['MSA Sites (Phase 1)\\nInput_Value','PMA Sites (Phase 2)\\nInput_Value'], names, merge_cols, cols_ordered)\n",
        "df_com\n",
        "# No Errors"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors founded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRJDetTx6r31"
      },
      "source": [
        "dfs = [df_dates_errors, df_pick_general_errors, df_pick_actives_errors, df_doer_errors]\n",
        "names=['actives dates erros', 'Picklist General Errors', 'Actives Picklist Errors', 'In Service Site DOER(Past)']\n",
        "\n",
        "general_log_erros(dfs, names, '/content/Towerdb_August_Errors.xlsx')"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmEVTURV2gCK"
      },
      "source": [
        "Create CSV File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxxPNjgByHcw"
      },
      "source": [
        "Script to make TA Input file validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBVMpAXSBds4"
      },
      "source": [
        "def find_diffs_between_files_1(path_OLD, path_NEW, index_col, bill_cols,path_save, old_name,new_name, cols_conv, type_file='mix',\\\n",
        "                               status_col='', kind='tw', dates_cols=[], kind_col='',type_date=\"%d/%m/%Y\", sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col,cols_conv, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col] \n",
        "            #df.columns = lower_str(list(df.columns))\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            if cols_conv:\n",
        "                for col in cols_conv:\n",
        "                    df[col] = df[col].astype(str).apply(unidecode)\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df = df.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "            #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']]\n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    cols_conv = lower_str(cols_conv)\n",
        "    dates_cols = lower_str(dates_cols)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        date_parser(df_OLD, dates_cols,format , type_date)\n",
        "        df_OLD = fit_df(df_OLD,index_col,cols_conv,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols,format , type_date)\n",
        "        df_NEW = fit_df(df_NEW,index_col,cols_conv,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='ISO-8859-1').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        #df_OLD = df_OLD[lower_str(bill_cols)]\n",
        "        df_OLD = fit_df(df_OLD, index_col, cols_conv,kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='ISO-8859-1').fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,cols_conv, kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD) header=0, names=cols_old,\n",
        "        ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=ta_cols, encoding='ISO-8859-1').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        print(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col,cols_conv, kind, kind_col)\n",
        "        \n",
        "        col = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type',\\\n",
        "                'Contract start date', 'Contract end date', 'Status of contract',\\\n",
        "                'Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart',\\\n",
        "                'Classification of Tenant', 'Annual amount - billing currency',\\\n",
        "                'Billing Currency', 'Annual Amount in CZK', 'Amount in EUR',\\\n",
        "                'Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "                'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key',\\\n",
        "                'Classification', 'Residual Period in Years', 'Remarks',\\\n",
        "                'Count of unique key', 'Count of contracts', 'Scoping classification',\\\n",
        "                'DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "                'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x',\\\n",
        "                'SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count','Quota of Unique Key','unamed']\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, header=0, names = col, skiprows = skipr)\n",
        "        #print(df_NEW.columns)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        print(df_NEW.head(1))\n",
        "        #df_NEW = df_NEW.reindex(columns=ta_cols)\n",
        "        print(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.reindex(columns=list(df_OLD.columns))\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "        date_parser(df_NEW, dates_cols,format , type_date)\n",
        "        df_NEW = fit_df(df_NEW, index_col,cols_conv, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    df_all.to_excel('/content/excel.xlsx', index=False)\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "ta_cols = ['Tenant Agreement ID - NEW', 'Code','Contract start date', 'Contract end date','Counterpart',\\\n",
        "           'Classification of Tenant',]\n",
        "path_ta_input = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "sheet = 'Tenant '\n",
        "ta_old = '/content/TA_Input_CzechRepublic_20210731.csv'\n",
        "ta_save = '/content/CZ_TA'\n",
        "old_ta = 'TA_old.csv'\n",
        "new_ta = 'TA_new.xlsx'\n",
        "#find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, \"\", ta_save,'csv', 'ta')\n",
        "\n",
        "\n",
        "cols_con_ta = ['Province','Municipality','Address']\n",
        "dates = ['Contract start date', 'Contract end date', 'Update in month']\n",
        "\n",
        "find_diffs_between_files_1(ta_old, path_ta_input, 'Code', ta_cols, ta_save, old_ta,new_ta, [],'mix',\\\n",
        "                           status_col=\"\", kind='ta', dates_cols=dates , kind_col='Site Name', sheetname=sheet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qXbp6GbntD4"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', kind_col='', k='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        #Ajustado só para CZ_Julho, tirei o encoding\n",
        "        df_NEW = pd.read_csv(path_NEW).fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    else:\n",
        "        ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "        ta_cols = lower_str(ta_cols)\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=ta_cols, encoding='latin')\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #df_OLD['sites'] = [site if type(site) != str else len(df_OLD[index_col])+i for i, site in enumerate(df_OLD[index_col])]\n",
        "        #df_OLD = df_OLD.sort_values(by='sites')\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD['site name']+df_OLD['count of contracts'].astype(str)+df_OLD['service type'].astype(str)\n",
        "        \n",
        "\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "\n",
        "        col = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type',\\\n",
        "                'Contract start date', 'Contract end date', 'Status of contract',\\\n",
        "                'Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart',\\\n",
        "                'Classification of Tenant', 'Annual amount - billing currency',\\\n",
        "                'Billing Currency', 'Annual Amount in CZK', 'Amount in EUR',\\\n",
        "                'Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "                'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key',\\\n",
        "                'Classification', 'Residual Period in Years', 'Remarks',\\\n",
        "                'Count of unique key', 'Count of contracts', 'Scoping classification',\\\n",
        "                'DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "                'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x',\\\n",
        "                'SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count','Quota of Unique Key','unamed']\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,header=0, names=col, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.reindex(columns=ta_cols)\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW['sites'] = df_NEW[index_col].astype(str)+df_NEW[kind_col].astype(str)+df_NEW['site name']+df_NEW['count of contracts'].astype(str)+df_NEW['service type'].astype(str)\n",
        "        #print([i for i in list(df_NEW['sites']) if list(df_NEW['sites']).count(i)>1])\n",
        "\n",
        "     \n",
        "        #print(df_NEW.info())\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "\n",
        "        try:\n",
        "            df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        except:\n",
        "            print('falha')\n",
        "        #print(list(df_NEW.columns)==(df_OLD.columns))\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "        status_1 = f'{status_col}_current'\n",
        "        status_2 = f'{status_col}_before'\n",
        "        new_copy = new_copy.set_index('sites')\n",
        "        new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} old_file vs new_file.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                             end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "\n",
        "    #print(len(newRows))\n",
        "    for site in new_copy[index_col]:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, \\\n",
        "                             status_col='', path_save=ta_save, type_file='mix',kind='ta',kind_col='tenant agreement id - new',k='service type', sheetname=sheet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzQjvvD7MRTb"
      },
      "source": [
        "#ta.groupby(ta['Code'].apply(type) != str).apply(lambda i: i.sort_values('Code')).reset_index(drop = True)\n",
        "sites = []\n",
        "for i in ta['Code']:\n",
        "    if type(i) == str:\n",
        "        sites.append(i)\n",
        "    else:\n",
        "        sites.append(int(i))\n",
        "ta['sites'] = sites\n",
        "#ta.groupby(ta['sites'].apply(type) != str).apply(lambda i: i.sort_values('sites')).reset_index(drop = True)"
      ],
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "zOREPa3GfFb_",
        "outputId": "c42d1de2-7a69-4e2a-edb7-81efbf19d3a6"
      },
      "source": [
        "#Read TA Input File\n",
        "def read_files(path, sheetname,col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int, cols_amount, bill_cols, \\\n",
        "               format='mix', type_date=\"%d/%m/%Y\"):\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname,header=0, names = col_name, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_amount, 0)\n",
        "    for col in cols_amount:\n",
        "        #df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "    for col in cols_int:\n",
        "        df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int)\n",
        "    df = df.fillna('')\n",
        "    replace_values(df, bill_cols)\n",
        "    date_parser(df, cols_date, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    return df\n",
        "\n",
        "def replace_values_ta(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ']\n",
        "    #lista = []\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        df[column] = df[column].fillna(0)\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "col = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type',\\\n",
        "       'Contract start date', 'Contract end date', 'Status of contract',\\\n",
        "       'Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart',\\\n",
        "       'Classification of Tenant', 'Annual amount - billing currency',\\\n",
        "       'Billing Currency', 'Annual Amount in CZK', 'Amount in EUR',\\\n",
        "       'Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "       'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key',\\\n",
        "       'Classification', 'Residual Period in Years', 'Remarks',\\\n",
        "       'Count of unique key', 'Count of contracts', 'Scoping classification',\\\n",
        "       'DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "       'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x',\\\n",
        "       'SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count','Quota of Unique Key','unamed']\n",
        "\n",
        "#ta = pd.read_csv(path_ta_input, header=0, names=col)\n",
        "dates = ['Contract start date', 'Contract end date', 'Update in month']\n",
        "interger = ['Residual Period in Years','Count of unique key', 'Count of contracts', 'Unique Tenant Count',\\\n",
        "            'Quota of Unique Key']\n",
        "amount = ['Annual Amount in CZK', 'Amount in EUR']\n",
        "ta_bill_cols = ['Tenant Agreement ID - NEW','Contract start date','Contract end date',\\\n",
        "                'Counterpart','Classification of Tenant']\n",
        "ta = read_files('/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx', 'Tenant ',col, 0, 0, 'Code', dates, interger,amount, ta_bill_cols)\n",
        "\n",
        "ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "\n",
        "ta = ta.reindex(columns=ta_cols)\n",
        "\n",
        "ta.head(1)\n",
        "\n"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tenant Agreement ID - NEW</th>\n",
              "      <th>Code</th>\n",
              "      <th>Site Name</th>\n",
              "      <th>Service Type</th>\n",
              "      <th>Contract start date</th>\n",
              "      <th>Contract end date</th>\n",
              "      <th>Status of contract</th>\n",
              "      <th>Renewal Option</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Counterpart ID</th>\n",
              "      <th>Counterpart</th>\n",
              "      <th>Classification of Tenant</th>\n",
              "      <th>Annual amount - billing currency</th>\n",
              "      <th>Billing Currency</th>\n",
              "      <th>Annual Amount in CZK</th>\n",
              "      <th>Amount in EUR</th>\n",
              "      <th>Terms of Payment</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Indexed YES/NO</th>\n",
              "      <th>Index</th>\n",
              "      <th>Percentage</th>\n",
              "      <th>VAT Subject YES/NO</th>\n",
              "      <th>Percentage (VAT)</th>\n",
              "      <th>Unique Key</th>\n",
              "      <th>Classification</th>\n",
              "      <th>Residual Period in Years</th>\n",
              "      <th>Remarks</th>\n",
              "      <th>Count of unique key</th>\n",
              "      <th>Count of contracts</th>\n",
              "      <th>Scoping classification</th>\n",
              "      <th>DAS sites</th>\n",
              "      <th>DAS ownership</th>\n",
              "      <th>31-Mar-21</th>\n",
              "      <th>Update in month</th>\n",
              "      <th>Updated Item</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Counterpart ID_1</th>\n",
              "      <th>Counterpart_1</th>\n",
              "      <th>x</th>\n",
              "      <th>SiteType</th>\n",
              "      <th>1.028</th>\n",
              "      <th>Unique Tenant</th>\n",
              "      <th>Unique Tenant Count</th>\n",
              "      <th>unamed</th>\n",
              "      <th>unamed_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2500000008</td>\n",
              "      <td>150910</td>\n",
              "      <td>LIUSI</td>\n",
              "      <td>Lease</td>\n",
              "      <td>10/09/2020</td>\n",
              "      <td>31/12/9999</td>\n",
              "      <td>effective contract</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>400004777</td>\n",
              "      <td>a-net Liberec</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>100.0</td>\n",
              "      <td>CZK</td>\n",
              "      <td>100</td>\n",
              "      <td>3</td>\n",
              "      <td>30 days deferred payment</td>\n",
              "      <td>yearly</td>\n",
              "      <td>NO</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>YES</td>\n",
              "      <td>0.21</td>\n",
              "      <td>150910|400004777</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>8094</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>In-Scope - Skylon BTS</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>01/04/2020</td>\n",
              "      <td>new contract</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>handover protocol signed 10.9.2020</td>\n",
              "      <td></td>\n",
              "      <td>RTT</td>\n",
              "      <td></td>\n",
              "      <td>150910|a-net Liberec</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Tenant Agreement ID - NEW    Code  ... unamed unamed_2\n",
              "0                2500000008  150910  ...             NaN\n",
              "\n",
              "[1 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW2ncZB4-5vB"
      },
      "source": [
        "ta['Tenant Agreement ID - NEW'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrmelbSTYn0_"
      },
      "source": [
        "ta.to_excel('/content/TA_Input_CzechRepublic_20210831.xlsx', index=False)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1iihpLhb4VW"
      },
      "source": [
        "ta.to_csv('/content/TA_Input_CzechRepublic_20210831.csv', index=False)"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqMPxE4jijDQ"
      },
      "source": [
        "ta_msa = pd.read_csv('/content/TA_Input_CzechRepublic_20210731.csv', encoding='latin2')\n",
        "msa_cols = list(ta_msa.columns)\n",
        "ta_cols = list(ta.columns)\n",
        "df_cols = check_columns_received(ta, msa_cols)\n",
        "df_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqSk2BMDCK6W",
        "outputId": "ba7c4578-b3f7-479a-ff25-3c1c954cece8"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index,start_date,end_date):\n",
        "    df[start_date] = pd.to_datetime(df[start_date],errors='coerce')\n",
        "    df[end_date] = pd.to_datetime(df[end_date],errors='coerce')\n",
        "    filtered = df[df[start_date] > df[end_date]]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "\n",
        "df_ta_dates = check_lc_ta_dates(ta,'Code', 'Contract start date', 'Contract end date')\n",
        "df_ta_dates"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4BraJ7gDw5V",
        "outputId": "1b7b78dc-a1b9-4bc1-fc57-a8df925663f9"
      },
      "source": [
        "ta['Classification of Tenant'].value_counts()"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MNO     323\n",
              "OTMO    295\n",
              "Name: Classification of Tenant, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYc-9mpk18Ip"
      },
      "source": [
        "Script to read LC Input file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "33z-44v_EOl1",
        "outputId": "35b3b693-ac95-45ae-df09-387ec567c3a8"
      },
      "source": [
        "def read_files(path, sheetname,col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int, cols_amount, bill_cols, format='mix', type_date=\"%d/%m/%Y\"):\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname,header=0, names = col_name, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_amount, 0)\n",
        "    for col in cols_amount:\n",
        "        #df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "\n",
        "    df = df.fillna('')\n",
        "    replace_values(df, bill_cols)\n",
        "    date_parser(df, cols_date, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    return df\n",
        "\n",
        "def replace_values_ta(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ']\n",
        "    #lista = []\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        df[column] = df[column].fillna(0)\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "col_names = ['Contract ID - NEW', 'FIN ID', 'Contract Crncy', 'Contract Type', 'Freq.', 'Freq. Unit', 'Indexation',\\\n",
        "         'Index upon request', 'Counterpart ID', 'Counterpart', 'LC Amount CZK\\nyearly', 'Amount in EUR yearly (Actual)', \\\n",
        "         'Payment terms Code', 'VAT Subject', '% of VAT', 'Contr. Start date', 'Contr. 1st End', 'Contr. Term End', \\\n",
        "         'Sublease Consession', 'Renewal Option', 'Unique Key', 'Count of Key', 'Count of ContrBct ', \\\n",
        "         'Remarks - Consistency check', 'Residual Period in years', 'Scoping classification', 'DAS sites', \\\n",
        "         'DAS ownership', '31_12_9999', 'Unique Key1', 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021', \\\n",
        "         'Check LC amount', 'Updated Item\\nAmount yearly', 'Contr. 1st End as of 31.05.2021', \\\n",
        "         'Contr. 1st End as of 30.06.2021', 'Check Contr. 1st End', 'Updated Item\\n1st End date', 'Comment', \\\n",
        "         'Comment date', 'Date']\n",
        "\n",
        "#ta = pd.read_csv(path_ta_input, header=0, names=col)\n",
        "dates_lc = ['Contr. Start date', 'Contr. 1st End', 'Contr. 1st End as of 31.05.2021', 'Contr. 1st End as of 30.06.2021']\n",
        "interger_lc = []\n",
        "amount_lc = ['LC Amount CZK\\nyearly']\n",
        "lc_bill_cols = ['Contract ID - NEW','Counterpart']\n",
        "lc = read_files('/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx', 'Final data_Lease',col_names, 1, 0, 'FIN ID', \\\n",
        "                dates_lc, interger_lc,amount_lc, lc_bill_cols)\n",
        "\n",
        "col_order_lc = [\"Contract ID - NEW\",\"FIN ID\",\"Contract Crncy\",\"Contract Type\",\"Freq.\",\"Freq. Unit\",\"Indexation\",\\\n",
        "                \"Index upon request\",\"Counterpart ID\",\"Counterpart\",\"LC Amount CZK\\nyearly\",\"Amount in EUR yearly (Actual)\",\\\n",
        "                \"Payment terms Code\",\"VAT Subject\",\"% of VAT\",\"Contr. Start date\",\"Contr. 1st End\",\"Contr. Term End\",\\\n",
        "                \"Sublease Consession\",\"Renewal Option\",\"Unique Key\",\"Count of Key\",\"Count of ContrBct \",\\\n",
        "                \"Remarks - Consistency check\",\"Residual Period in years\",\"Scoping classification\",\"DAS sites\",\\\n",
        "                \"DAS ownership\",\"31_12_9999\",\"Unique Key1\",'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021',\\\n",
        "                \"Check LC amount\",\"Updated Item\\nAmount yearly\",'Contr. 1st End as of 31.05.2021','Contr. 1st End as of 30.06.2021',\\\n",
        "                \"Check Contr. 1st End\",\"Updated Item\\n1st End date\",\"Comment\",\"Comment date\",\"Date\"]\n",
        "lc = lc[col_order_lc]\n",
        "lc.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Contract ID - NEW</th>\n",
              "      <th>FIN ID</th>\n",
              "      <th>Contract Crncy</th>\n",
              "      <th>Contract Type</th>\n",
              "      <th>Freq.</th>\n",
              "      <th>Freq. Unit</th>\n",
              "      <th>Indexation</th>\n",
              "      <th>Index upon request</th>\n",
              "      <th>Counterpart ID</th>\n",
              "      <th>Counterpart</th>\n",
              "      <th>LC Amount CZK\\nyearly</th>\n",
              "      <th>Amount in EUR yearly (Actual)</th>\n",
              "      <th>Payment terms Code</th>\n",
              "      <th>VAT Subject</th>\n",
              "      <th>% of VAT</th>\n",
              "      <th>Contr. Start date</th>\n",
              "      <th>Contr. 1st End</th>\n",
              "      <th>Contr. Term End</th>\n",
              "      <th>Sublease Consession</th>\n",
              "      <th>Renewal Option</th>\n",
              "      <th>Unique Key</th>\n",
              "      <th>Count of Key</th>\n",
              "      <th>Count of ContrBct</th>\n",
              "      <th>Remarks - Consistency check</th>\n",
              "      <th>Residual Period in years</th>\n",
              "      <th>Scoping classification</th>\n",
              "      <th>DAS sites</th>\n",
              "      <th>DAS ownership</th>\n",
              "      <th>31_12_9999</th>\n",
              "      <th>Unique Key1</th>\n",
              "      <th>LC amount as of 31.05.2021</th>\n",
              "      <th>LC amount as of 30.06.2021</th>\n",
              "      <th>Check LC amount</th>\n",
              "      <th>Updated Item\\nAmount yearly</th>\n",
              "      <th>Contr. 1st End as of 31.05.2021</th>\n",
              "      <th>Contr. 1st End as of 30.06.2021</th>\n",
              "      <th>Check Contr. 1st End</th>\n",
              "      <th>Updated Item\\n1st End date</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Comment date</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1100000000</td>\n",
              "      <td>111530</td>\n",
              "      <td>CZK</td>\n",
              "      <td>Lease</td>\n",
              "      <td>3</td>\n",
              "      <td>Months</td>\n",
              "      <td>Free - Modifiable</td>\n",
              "      <td>yes</td>\n",
              "      <td>800222850</td>\n",
              "      <td>Sabadinová Marie</td>\n",
              "      <td>90,176</td>\n",
              "      <td>3281.549127</td>\n",
              "      <td>Due 14days from invoice receipt date</td>\n",
              "      <td>no</td>\n",
              "      <td></td>\n",
              "      <td>28/06/2002</td>\n",
              "      <td>28/06/2022</td>\n",
              "      <td></td>\n",
              "      <td>Yes, but announcement needed</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1100000000|800222850</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mapped</td>\n",
              "      <td>2</td>\n",
              "      <td>In-scope</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>T00</td>\n",
              "      <td>1100000000|800222850|T00</td>\n",
              "      <td>90176.97</td>\n",
              "      <td>90176.97</td>\n",
              "      <td>0.0</td>\n",
              "      <td>JULY - no change, OK</td>\n",
              "      <td>28/06/2022</td>\n",
              "      <td>28/06/2022</td>\n",
              "      <td>0</td>\n",
              "      <td>JULY - no change, OK</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1100000001</td>\n",
              "      <td>111470</td>\n",
              "      <td>CZK</td>\n",
              "      <td>Lease</td>\n",
              "      <td>3</td>\n",
              "      <td>Months</td>\n",
              "      <td>Free - Modifiable</td>\n",
              "      <td>yes</td>\n",
              "      <td>800199147</td>\n",
              "      <td>SPOLECENSTVI VLASTNIKU JEDNOTEK DOMU C.P. 589,...</td>\n",
              "      <td>88,819</td>\n",
              "      <td>3232.135371</td>\n",
              "      <td>Due 14days from invoice receipt date</td>\n",
              "      <td>yes</td>\n",
              "      <td>0.21</td>\n",
              "      <td>25/09/2002</td>\n",
              "      <td>31/12/2025</td>\n",
              "      <td></td>\n",
              "      <td>Yes, but announcement needed</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1100000001|800199147</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mapped</td>\n",
              "      <td>5</td>\n",
              "      <td>In-scope</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>T00</td>\n",
              "      <td>1100000001|800199147|T00</td>\n",
              "      <td>88819.08</td>\n",
              "      <td>88819.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>JULY - no change, OK</td>\n",
              "      <td>31/12/2025</td>\n",
              "      <td>31/12/2025</td>\n",
              "      <td>0</td>\n",
              "      <td>JULY - no change, OK</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Contract ID - NEW  FIN ID Contract Crncy  ... Comment Comment date Date\n",
              "0         1100000000  111530            CZK  ...                          \n",
              "1         1100000001  111470            CZK  ...                          \n",
              "\n",
              "[2 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsLYlT0QKY8g",
        "outputId": "a393528d-2a2e-4b0f-b51d-274986cecceb"
      },
      "source": [
        "lc['LC Amount CZK\\nyearly'].value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50,000     115\n",
              "30,000     106\n",
              "60,000     106\n",
              "40,000      94\n",
              "80,000      92\n",
              "          ... \n",
              "89,930       1\n",
              "122,984      1\n",
              "49,000       1\n",
              "400,000      1\n",
              "54,073       1\n",
              "Name: LC Amount CZK\\nyearly, Length: 2562, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i4SMB8fY8Hs"
      },
      "source": [
        "lc.to_excel('/content/LC_Input_CzechRepublic_20210831.xlsx', index=False)"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVsiMGPtbHWo"
      },
      "source": [
        "lc.to_csv('/content/LC_Input_CzechRepublic_20210831.csv', index=False)"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f85vYdkE1_NL"
      },
      "source": [
        "def check_amounts_lc(df_check, df_index, columns, pattern=','):\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    filtered = df_check[[df_index]]\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        #df_new = pd.merge(df_new, filtered, how='left', left_on='identification - site key', right_on=tw_index)\n",
        "        df_new = df_new.set_index(df_index)\n",
        "        #df_new = df_new[[status_col]+ df_new.columns[:-1].tolist()]\n",
        "        #df_new = df_new[['identification - site key', status_col]]\n",
        "        return df_new\n",
        "    else: \n",
        "        print('\\nNo one columns with incorrect Amount format!\\n') \n",
        "\n",
        "lc_cols = [\"FIN ID\", 'LC Amount CZK\\nyearly']  \n",
        "\n",
        "df_lc_amount = check_amounts_lc(lc, \"FIN ID\", lc_cols)\n",
        "df_lc_amount\n",
        "# No errors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjcRUHSCO0_W",
        "outputId": "84313014-d21d-4f09-8e67-a8104e65d064"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index,start_date,end_date):\n",
        "    df[start_date] = pd.to_datetime(df[start_date],errors='coerce')\n",
        "    df[end_date] = pd.to_datetime(df[end_date],errors='coerce')\n",
        "    filtered = df[df[start_date] > df[end_date]]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "        \n",
        "df_lc_dates = check_lc_ta_dates(lc,\"FIN ID\", \"Contr. Start date\",\"Contr. 1st End\")\n",
        "df_lc_dates\n"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9YiQwfjCUR7"
      },
      "source": [
        "lc_cols = ['FIN ID', 'Counterpart', 'LC Amount CZK\\nyearly','Contr. Start date', 'Contr. 1st End']\n",
        "path_lc_input = '/content/LC_Input_CzechRepublic_20210731.csv'\n",
        "lc_old = '/content/LC_Input_CzechRepublic_20210630.csv'\n",
        "lc_save = '/content/CZ_LC'\n",
        "find_diffs_between_files(lc_old, path_lc_input, 'FIN ID', lc_cols, \"\", lc_save,'csv', 'lc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "q4gE5Z66WFC1",
        "outputId": "23833546-c5a0-40c9-d5f1-302e19a5d418"
      },
      "source": [
        "def find_diffs_between_files_lc(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', kind_col='', k='', am_cols = [],\\\n",
        "                             cols_int=[], cols_date=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "     \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "    \n",
        "    def date_parser(df, column,format, type_date):\n",
        "        if format == 'mix':\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "            return df[column]\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    am_cols = lower_str(am_cols)\n",
        "    cols_int = lower_str(cols_int)\n",
        "    cols_date = lower_str(cols_date)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        #Ajustado só para CZ_Julho, tirei o encoding\n",
        "        df_NEW = pd.read_csv(path_NEW).fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    else:\n",
        "        col_order_lc = [\"Contract ID - NEW\",\"FIN ID\",\"Contract Crncy\",\"Contract Type\",\"Freq.\",\"Freq. Unit\",\"Indexation\",\\\n",
        "                        \"Index upon request\",\"Counterpart ID\",\"Counterpart\",\"LC Amount CZK\\nyearly\",\"Amount in EUR yearly (Actual)\",\\\n",
        "                        \"Payment terms Code\",\"VAT Subject\",\"% of VAT\",\"Contr. Start date\",\"Contr. 1st End\",\"Contr. Term End\",\\\n",
        "                        \"Sublease Consession\",\"Renewal Option\",\"Unique Key\",\"Count of Key\",\"Count of ContrBct \",\\\n",
        "                        \"Remarks - Consistency check\",\"Residual Period in years\",\"Scoping classification\",\"DAS sites\",\\\n",
        "                        \"DAS ownership\",\"31_12_9999\",\"Unique Key1\",'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021',\\\n",
        "                        \"Check LC amount\",\"Updated Item\\nAmount yearly\",'Contr. 1st End as of 31.05.2021','Contr. 1st End as of 30.06.2021',\\\n",
        "                        \"Check Contr. 1st End\",\"Updated Item\\n1st End date\",\"Comment\",\"Comment date\",\"Date\"]\n",
        "        col_order_lc = lower_str(col_order_lc)\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=col_order_lc)\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD = df_OLD.sort_values(by='contract id - new')\n",
        "        \n",
        "        lista = []\n",
        "        df_OLD['sites'] = df_OLD['contract id - new'].copy()\n",
        "        for i in range(len(df_OLD['contract id - new'])):\n",
        "            if str(df_OLD.iloc[i]['contract id - new']) in lista:            \n",
        "                df_OLD.iloc[i]['sites'] = str(df_OLD.iloc[i]['contract id - new'])+\"_\"+str(lista.count(str(df_OLD.iloc[i]['contract id - new'])))\n",
        "            else:\n",
        "                df_OLD.iloc[i]['sites'] = str(df_OLD.iloc[i]['contract id - new'])+\"_0\"    \n",
        "            lista.append(str(df_OLD.iloc[i]['contract id - new']))   \n",
        "\n",
        "       \n",
        "\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD[k].astype(str)+df_OLD['count of contrbct '].astype(str)+df_OLD[\"count of key\"].astype(str)+df_OLD[\"counterpart\"].astype(str)\n",
        "        df_OLD = df_OLD.fillna('')\n",
        "\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        #df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "\n",
        "        col_names = ['Contract ID - NEW', 'FIN ID', 'Contract Crncy', 'Contract Type', 'Freq.', 'Freq. Unit', 'Indexation',\\\n",
        "                'Index upon request', 'Counterpart ID', 'Counterpart', 'LC Amount CZK\\nyearly', 'Amount in EUR yearly (Actual)', \\\n",
        "                'Payment terms Code', 'VAT Subject', '% of VAT', 'Contr. Start date', 'Contr. 1st End', 'Contr. Term End', \\\n",
        "                'Sublease Consession', 'Renewal Option', 'Unique Key', 'Count of Key', 'Count of ContrBct ', \\\n",
        "                'Remarks - Consistency check', 'Residual Period in years', 'Scoping classification', 'DAS sites', \\\n",
        "                'DAS ownership', '31_12_9999', 'Unique Key1', 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021', \\\n",
        "                'Check LC amount', 'Updated Item\\nAmount yearly', 'Contr. 1st End as of 31.05.2021', \\\n",
        "                'Contr. 1st End as of 30.06.2021', 'Check Contr. 1st End', 'Updated Item\\n1st End date', 'Comment', \\\n",
        "                'Comment date', 'Date']\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,header=0, names=col_names, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.reindex(columns=col_order_lc)\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW = df_NEW.sort_values(by='contract id - new')\n",
        "\n",
        "        df_NEW['sites'] = df_NEW[index_col].copy()\n",
        "        df_NEW['sites'] = df_NEW['sites'].astype(str)\n",
        "        df_NEW['sites'] = np.where(df_NEW['sites'].duplicated(keep=False), \n",
        "                      df_NEW['sites'] + df_NEW.groupby('sites').cumcount().add(1).astype(str),\n",
        "                      df_NEW['sites'])\n",
        "        print(df_NEW['sites'])\n",
        "        #df_NEW['sites'] = [str(site)+\"_\"+str(i) for i, site in enumerate(df_NEW['contract id - new'])]\n",
        "\n",
        "        #df_NEW['sites'] = df_NEW[index_col].astype(str)+df_NEW[kind_col].astype(str)+df_NEW[k].astype(str)+df_NEW['count of contrbct '].astype(str)+df_NEW[\"counterpart\"].astype(str)+df_NEW[\"counterpart\"].astype(str)\n",
        "        \n",
        "        df_NEW = replace_values(df_NEW, am_cols, 0)\n",
        "        for col in am_cols:\n",
        "            #df[col] = df[col].fillna(0)\n",
        "            df_NEW[col] = df_NEW[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "\n",
        "        df_NEW = replace_values(df_NEW,bill_cols)\n",
        "        for i in cols_date:\n",
        "            df_NEW[i] = date_parser(df_NEW, i, 'mix', \"%d/%m/%Y\")\n",
        "        df_NEW = df_NEW.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "        #print(df_NEW.info())\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "\n",
        "        try:\n",
        "            df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        except:\n",
        "            print('falha')\n",
        "        #print(list(df_NEW.columns)==(df_OLD.columns))\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} old_file vs new_file.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                             end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "\n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "lc_col = ['FIN ID', 'Counterpart', 'LC Amount CZK\\nyearly','Contr. Start date', 'Contr. 1st End']\n",
        "path_lc_input = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "lc_old = '/content/LC_Input_CzechRepublic_20210731.csv'\n",
        "lc_save = '/content/CZ_LC'\n",
        "ta_tab = 'Final data_Lease'\n",
        "dates_lc = ['Contr. Start date', 'Contr. 1st End', 'Contr. 1st End as of 31.05.2021', 'Contr. 1st End as of 30.06.2021']\n",
        "interger_lc = []\n",
        "amount_lc = ['LC Amount CZK\\nyearly']\n",
        "lc_bill_cols = ['Contract ID - NEW','Counterpart']\n",
        "\"\"\"find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, \\\n",
        "                         status_col='', path_save=ta_save, type_file='mix',kind='ta',\\\n",
        "                         kind_col='tenant agreement id - new',k='service type',am_cols = amount_ta,\\\n",
        "                         cols_int=interger, cols_date=dates, sheetname=sheet)\"\"\"\n",
        "find_diffs_between_files_lc(lc_old, path_lc_input, 'FIN ID', lc_col,status_col='', path_save=lc_save,type_file='mix', kind='ta',\\\n",
        "                            kind_col = 'contract id - new', k='contract type',am_cols=amount_lc,\\\n",
        "                           cols_int=[], cols_date=dates_lc, sheetname=ta_tab, skipr=1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:79: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0        111530\n",
            "1        111470\n",
            "2        211800\n",
            "3       1918801\n",
            "4       1918802\n",
            "         ...   \n",
            "4671    ZSNA1G4\n",
            "4672     300230\n",
            "4673    7501002\n",
            "4674    2290102\n",
            "4675     241690\n",
            "Name: sites, Length: 4676, dtype: object\n",
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-69830e6c4c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0mlc_bill_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Contract ID - NEW'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Counterpart'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\"\"\"find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols,                          status_col='', path_save=ta_save, type_file='mix',kind='ta',                         kind_col='tenant agreement id - new',k='service type',am_cols = amount_ta,                         cols_int=interger, cols_date=dates, sheetname=sheet)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m \u001b[0mfind_diffs_between_files_lc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_lc_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FIN ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlc_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstatus_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlc_save\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ta'\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mkind_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'contract id - new'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'contract type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mam_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamount_lc\u001b[0m\u001b[0;34m,\u001b[0m                           \u001b[0mcols_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdates_lc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheetname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mta_tab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-69830e6c4c68>\u001b[0m in \u001b[0;36mfind_diffs_between_files_lc\u001b[0;34m(path_OLD, path_NEW, index_col, bill_cols, status_col, path_save, type_file, kind, kind_col, k, am_cols, cols_int, cols_date, sheetname, skipr, skipc)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_NEW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mdroppedRows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mnew_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_copy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_OLD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mnew_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_copy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7749\u001b[0m             \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7750\u001b[0m             \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7751\u001b[0;31m             \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7752\u001b[0m         )\n\u001b[1;32m   7753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(self, inplace)\u001b[0m\n\u001b[1;32m   5232\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5236\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5214\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5216\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5203\u001b[0m         \"\"\"\n\u001b[1;32m   5204\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5205\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5207\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5213\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5214\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    986\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1907\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         merged_blocks = _merge_blocks(\n\u001b[0;32m-> 1909\u001b[0;31m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m         )\n\u001b[1;32m   1911\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[1;32m   1929\u001b[0m         \u001b[0;31m# combination of those slices is a slice, too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06oxtFVo2dvO"
      },
      "source": [
        "\"\"\"Verify before, cause you need to convert all values in cols for string format to check\"\"\"\n",
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna('', inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "\n",
        "\"\"\"Verify before, cause you need to convert all values in cols for string format to check\"\"\"\n",
        "def date_parser(df, columns, t=1, format=1, type_dates='normal'):\n",
        "    t_col = type_dates.lower()\n",
        "    if format == 1:\n",
        "        type_date = \"%d/%m/%Y\"\n",
        "    else:\n",
        "        type_date = \"%d-%m-%Y\"\n",
        "    for column in columns:\n",
        "        if t == 1:\n",
        "            df[column] = pd.to_datetime(df[column], errors='coerce').fillna('')\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "#Columns to parser\n",
        "col_order = [\"towerdb owner\", \"identification - site key\", \"identification - code\", \"identification - fl\",\\\n",
        "             \"position - site name\", \"identification - other mno fl\", \"position - macro region\", \\\n",
        "             \"position - region\", \"position - municipality\", \"position - address\", \"position - latitude\", \\\n",
        "             \"position - longitude\", \"category - categorization by transmission sys (subcluster)\", \\\n",
        "             \"category - categorization by site type\", \"column1\", \"category - categorisation by inhabitants\", \\\n",
        "             \"other - ownership\", \"other - status\", \"column2\", \"column3\", \"column4\", \"column5\", \"column6\", \\\n",
        "             \"column7\", \"column8\", \"column9\", \"column10\", \"lease contract - current annual lease fee\", \\\n",
        "             \"lease contract - lease contract comment\", \"column11\", \"column12\", \"column13\", \"column14\", \\\n",
        "             \"tenants - anchor\", \"tenants - vf - ran sharing\", \"tenants - vf - passive\", \\\n",
        "             \"tenants - wind - ran sharing\", \"tenants - wind - passive\", \"tenants - cosmote - passive\", \\\n",
        "             \"tenants - ran sharing tenants\", \"tenants - passive tenants\", \"tenants - non-mno tenants\", \\\n",
        "             \"tenants - non-mno tenant name\", \"tenants - total tenants\", \"column15\", \\\n",
        "             \"ee - no. of 24hr generators\", \"ee - no. of standby generators\", \"kpi - sites\", \"kpi - tenants\", \\\n",
        "             \"msa - bts/replacement\", \"msa - bts commitment site\", \"column16\", \\\n",
        "             \"infrastructure type - technology\", \"infrastructure type - fibre / microwave\", \\\n",
        "             \"infrastructure type - floor space\", \"infrastructure type - tower height\", \"column17\",\\\n",
        "             \"notified-97 (gr use only)\", \"changes (gr use only)\", \"msa - billing trigger stop date\", \\\n",
        "             \"other - indoor vs outdoor\", \"skylon - category\", \"msa - standard configuration\", \\\n",
        "             \"msa - construction type\", \"msa - hub sites total mw diameter\", \"bp - consolidated classification\",\\\n",
        "             \"msa - critical site\", \"sensitive - due to\", \"sensitive - description\", \"sensitive - department\", \\\n",
        "             \"notified - department\", \"notified - due to\", \"msa - sensitive\", \"msa - notified\", \\\n",
        "             \"notified - status\", \"notified - details\", \"msa - rectifier\", \"msa - battery\", \"msa - aircon\", \\\n",
        "             \"msa - revenue generating\", \"msa - date of decommissioning\", \"flag indicating bts site\",\\\n",
        "             \"msa - billing trigger date\", \"msa - dg-shelter\", \"msa - critical site in excess of the 15.5% cap\",\\\n",
        "             \"msa - form of active sharing\", \"msa - start date for active sharing arrangement\",\\\n",
        "             \"msa - end date for active sharing arrangement\", \"column18\", \"msa - rfai date\", \\\n",
        "             \"msa - site acceptance date\", \"msa - sub-lease reason\", \"msa - sub-lease applies\", \\\n",
        "             \"msa - date of equipment removal (vf)\", \"msa - date of equipment removal (wh)\"]\n",
        "\n",
        "path_tw = \"/content/GR_TowerDB Jun'21 FINAL (Billing version).xlsx\"\n",
        "sheet_tw = 'GR_TowerDB'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "\n",
        "towerdb = read_files(path_tw, sheet_tw, skipr, skipc, 'Identification - Site Key')\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "towerdb = towerdb.reindex(columns = lower_str(col_order))\n",
        "\n",
        "\n",
        "towerdb[tw_bts] = new_format(towerdb, tw_bts)\n",
        "\n",
        "dates=['msa - start date for active sharing arrangement','msa - date of decommissioning',\\\n",
        "       'msa - end date for active sharing arrangement', 'msa - rfai date', 'msa - site acceptance date',\\\n",
        "       tw_doer_vf, tw_doer_wh]\n",
        "date_parser(towerdb, [tw_bill, 'msa - billing trigger stop date'],2, 1,'normal')\n",
        "date_parser(towerdb, dates,1, 1, 'mixed')\n",
        "\n",
        "\"\"\"Datas a verificar no prx mês\"\"\"\n",
        "#towerdb.loc[(towerdb[tw_bill]=='09/09/9999')&(towerdb[tw_bts]=='No'), tw_bts] = 'Yes'\n",
        "\n",
        "# Para o caracter A (alpha)\n",
        "towerdb[tw_index] = towerdb[tw_index].apply(unidecode)\n",
        "\n",
        "#Sempre fazer o replace depois de todas as modificações\n",
        "towerdb = replace_values(towerdb, lower_str(col_order))\n",
        "#towerdb = towerdb.round(2)\n",
        "#O Ficheiro da Grecia dá error na conversão dos caracteres  quando\n",
        "towerdb.to_csv('/content/TowerDB_Greece_20210831.csv', index=False, encoding='cp1258')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}