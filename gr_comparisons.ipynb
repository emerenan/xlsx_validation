{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gr_comparisons.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwgTDG++12aS+4/EhC4KdI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND1EC9odYOQG",
        "outputId": "1c2297ee-fb92-4775-f9b4-2008008d84d3"
      },
      "source": [
        "!pip install unidecode\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "def new_format(df, bts_col):\n",
        "    new_format = []\n",
        "    for i in df[bts_col]:\n",
        "        if i == 'BTS':\n",
        "            new_format.append('Yes')\n",
        "        elif i == 'Legacy':\n",
        "            new_format.append('No')\n",
        "        else:\n",
        "            new_format.append('')\n",
        "    return new_format\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index, date_cols):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df.columns = lower_str(list(df.columns))\n",
        "    \n",
        "    #Create empty columns\n",
        "    for i in range(1, 19):\n",
        "        df[f'column{i}'] = [np.nan for i in range(df.shape[0])]\n",
        "    \n",
        "    # Parsing Dates\n",
        "    for date_col in date_cols:\n",
        "        lista = []\n",
        "        df[date_col] = df[date_col].replace([' '], '')\n",
        "        df[date_col] = df[date_col].fillna('')\n",
        "        for i in df[date_col]:\n",
        "            dates_format = re.match(r'\\d{2}\\/\\d{2}\\/\\d{4}', str(i)) or re.match(r'\\d{2}\\-\\d{2}\\-\\d{4}', str(i))\n",
        "            not_date_format = not re.match(r'\\d{2}\\/\\d{2}\\/\\d{4}', str(i)) or not re.match(r'\\d{2}\\-\\d{2}\\-\\d{4}', str(i))\n",
        "            if i !='' and not_date_format:\n",
        "                #print(towerdb[towerdb[date_col]==i][['identification - site key', date_col]])\n",
        "                lista.append(f'{i:%d/%m/%Y}')\n",
        "            \n",
        "            elif dates_format:\n",
        "                lista.append(i)\n",
        "            else:\n",
        "                lista.append('')\n",
        "        df[date_col] = lista\n",
        "\n",
        "    return df\n",
        "\n",
        "guideline = [\"towerdb owner\", \"identification - site key\", \"identification - code\", \"identification - fl\",\\\n",
        "             \"position - site name\", \"identification - other mno fl\", \"position - macro region\", \\\n",
        "             \"position - region\", \"position - municipality\", \"position - address\", \"position - latitude\", \\\n",
        "             \"position - longitude\", \"category - categorization by transmission sys (subcluster)\", \\\n",
        "             \"category - categorization by site type\", \"column1\", \"category - categorisation by inhabitants\", \\\n",
        "             \"other - ownership\", \"other - status\", \"column2\", \"column3\", \"column4\", \"column5\", \"column6\", \\\n",
        "             \"column7\", \"column8\", \"column9\", \"column10\", \"lease contract - current annual lease fee\", \\\n",
        "             \"lease contract - lease contract comment\", \"column11\", \"column12\", \"column13\", \"column14\", \\\n",
        "             \"tenants - anchor\", \"tenants - vf - ran sharing\", \"tenants - vf - passive\", \\\n",
        "             \"tenants - wind - ran sharing\", \"tenants - wind - passive\", \"tenants - cosmote - passive\", \\\n",
        "             \"tenants - ran sharing tenants\", \"tenants - passive tenants\", \"tenants - non-mno tenants\", \\\n",
        "             \"tenants - non-mno tenant name\", \"tenants - total tenants\", \"column15\", \\\n",
        "             \"ee - no. of 24hr generators\", \"ee - no. of standby generators\", \"kpi - sites\", \"kpi - tenants\", \\\n",
        "             \"msa - bts/replacement\", \"msa - bts commitment site\", \"column16\", \\\n",
        "             \"infrastructure type - technology\", \"infrastructure type - fibre / microwave\", \\\n",
        "             \"infrastructure type - floor space\", \"infrastructure type - tower height\", \"column17\",\\\n",
        "             \"notified-97 (gr use only)\", \"changes (gr use only)\", \"msa - billing trigger stop date\", \\\n",
        "             \"other - indoor vs outdoor\", \"skylon - category\", \"msa - standard configuration\", \\\n",
        "             \"msa - construction type\", \"msa - hub sites total mw diameter\", \"bp - consolidated classification\",\\\n",
        "             \"msa - critical site\", \"sensitive - due to\", \"sensitive - description\", \"sensitive - department\", \\\n",
        "             \"notified - department\", \"notified - due to\", \"msa - sensitive\", \"msa - notified\", \\\n",
        "             \"notified - status\", \"notified - details\", \"msa - rectifier\", \"msa - battery\", \"msa - aircon\", \\\n",
        "             \"msa - revenue generating\", \"msa - date of decommissioning\", \"flag indicating bts site\",\\\n",
        "             \"msa - billing trigger date\", \"msa - dg-shelter\", \"msa - critical site in excess of the 15.5% cap\",\\\n",
        "             \"msa - form of active sharing\", \"msa - start date for active sharing arrangement\",\\\n",
        "             \"msa - end date for active sharing arrangement\", \"column18\", \"msa - rfai date\", \\\n",
        "             \"msa - site acceptance date\", \"msa - sub-lease reason\", \"msa - sub-lease applies\", \\\n",
        "             \"msa - date of equipment removal (vf)\", \"msa - date of equipment removal (wh)\"]\n",
        "guideline = list(map(lambda x: re.sub(r'^\\s+|\\s+$', '', str(x.lower())), guideline))\n",
        "\n",
        "date_columns = ['msa - start date for active sharing arrangement', 'msa - rfai date', 'msa - site acceptance date', 'msa - billing trigger date', \n",
        "            'msa - date of decommissioning', 'msa - date of equipment removal (vf)', 'msa - date of equipment removal (wh)', 'msa - billing trigger stop date',\n",
        "            'msa - end date for active sharing arrangement']\n",
        "\n",
        "bill_cols = ['TowerDB owner','Identification - Site Key','Category - Categorisation by inhabitants','Other - Ownership','Other - Status',\\\n",
        "             'Lease Contract - Current annual lease fee','Tenants - VF - Passive','Tenants - Wind - Passive','Tenants - Cosmote - Passive',\\\n",
        "             'Tenants - Passive tenants','EE - No. of 24Hr generators','EE - No. of standby generators','Other - Indoor vs Outdoor',\\\n",
        "             'MSA - Standard Configuration','MSA - Construction Type','BP - Consolidated classification','MSA - Critical Site','MSA - Rectifier',\\\n",
        "             'MSA - Battery','MSA - Aircon','MSA - Billing Trigger Date','MSA - DG-Shelter','MSA - Critical Site in excess of the 15.5% cap',\\\n",
        "             'MSA - Form of Active Sharing','MSA - Start Date for Active Sharing Arrangement','MSA - Sub-Lease Applies','MSA - Date of Equipment Removal (VF)',\\\n",
        "             'MSA - Date of Equipment Removal (WH)']\n",
        "bill_cols = list(map(lambda x: re.sub(r'^\\s+|\\s+$', '', str(x.lower())), bill_cols))\n",
        "\n",
        "path_tw = \"/content/GR_TowerDB Jun'21 FINAL (Billing version).xlsx\"\n",
        "sheet_tw = 'GR_TowerDB'\n",
        "\n",
        "towerdb = read_files(path_tw, sheet_tw, 0, 0, 'Identification - Site Key', date_columns)\n",
        "towerdb.rename(columns={'msa - legacy/bts': \"flag indicating bts site\"}, inplace=True)\n",
        "towerdb.columns = list(map(lambda x: re.sub(r'^\\s+|\\s+$', '', str(x.lower())), towerdb.columns.to_list()))\n",
        "towerdb = towerdb[guideline]\n",
        "towerdb['flag indicating bts site'] = new_format(towerdb, 'flag indicating bts site')\n",
        "towerdb['tenants - non-mno tenants'] = [int(i) if not pd.isnull(i) else '' for i in towerdb['tenants - non-mno tenants']]\n",
        "towedb = towerdb.fillna('')\n",
        "\n",
        "from unidecode import unidecode\n",
        "df_old = pd.read_csv('/content/TowerDB_Greece_20210731.csv', encoding='windows-1252').fillna('')\n",
        "df_old.columns = list(map(lambda x: re.sub(r'^\\s+|\\s+$', '', str(x.lower())), df_old.columns.to_list()))\n",
        "#towerdb = towerdb.reindex(columns = lower_str(col_order))\n",
        "df_old['identification - site key'] = df_old['identification - site key'].apply(unidecode)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CjT09FfaiWj",
        "outputId": "ed7d851b-0ef2-46ca-df7d-253d17e48039"
      },
      "source": [
        "def comparison(df_OLD, df_NEW,index_col,bill_cols, path_save, old_name, new_name, status='', kind='tw',):\n",
        "    # Perform Diff\n",
        "    from unidecode import unidecode\n",
        "    import pandas as pd\n",
        "    from unidecode import unidecode\n",
        "    import numpy as np\n",
        "    import datetime as dt\n",
        "    from datetime import datetime\n",
        "    import re\n",
        "    from openpyxl import Workbook, styles\n",
        "    from openpyxl.styles import PatternFill, Font\n",
        "    from openpyxl.styles.differential import DifferentialStyle\n",
        "    from openpyxl.formatting.rule import Rule\n",
        "\n",
        "    def fit_df(df, index_col):\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    df_OLD = fit_df(df_OLD, index_col)\n",
        "    df_NEW = fit_df(df_NEW, index_col)\n",
        "\n",
        "    new_copy = df_NEW.copy()\n",
        "    new_copy[index_col] = new_copy[index_col].apply(unidecode)\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = list(df_OLD.columns)\n",
        "    cols_NEW = list(df_NEW.columns)\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "    #print(sharedCols)\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "            #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'Old value:{value_OLD} > New Value:{value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "\n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "        status_1 = f'{status}_current'\n",
        "        status_2 = f'{status}_before'\n",
        "        new_copy = new_copy.set_index('sites')\n",
        "        new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format \n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "to_save_tw = '/content/GR_TW'\n",
        "old_n = 'July.csv'\n",
        "new_n = \"August.xlsx\"\n",
        "index = 'Identification - Site Key'\n",
        "\n",
        "comparison(df_old, towerdb, 'identification - site key',bill_cols, to_save_tw, old_n, new_n, 'other - status')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['90642-Α64201', '90750-Α75002']\n",
            "Dropped Rows: ['1135-113501', '1448-144801', '90642-A64201', '90750-A75002', '91250-D25002']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-jywipCHF6e"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col]\n",
        "\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'Old value:{value_OLD} > New Value:{value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old,  engine='python').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new,  engine='python').fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python', encoding='windows-1252').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "    \n",
        "bill_cols = ['TowerDB owner',\\\n",
        "             'Identification - Site Key',\\\n",
        "             'Category - Categorisation by inhabitants',\\\n",
        "             'Other - Ownership',\\\n",
        "             'Other - Status',\\\n",
        "             'Lease Contract - Current annual lease fee',\\\n",
        "             'Tenants - VF - Passive',\\\n",
        "             'Tenants - Wind - Passive',\\\n",
        "             'Tenants - Cosmote - Passive',\\\n",
        "             'Tenants - Passive tenants',\\\n",
        "             'EE - No. of 24Hr generators',\\\n",
        "             'EE - No. of standby generators',\\\n",
        "             'Other - Indoor vs Outdoor',\\\n",
        "             'MSA - Standard Configuration',\\\n",
        "             'MSA - Construction Type',\\\n",
        "             'BP - Consolidated classification',\\\n",
        "             'MSA - Critical Site',\\\n",
        "             'MSA - Rectifier',\\\n",
        "             'MSA - Battery',\\\n",
        "             'MSA - Aircon',\\\n",
        "             'MSA - Billing Trigger Date',\\\n",
        "             'MSA - DG-Shelter',\\\n",
        "             'MSA - Critical Site in excess of the 15.5% cap',\\\n",
        "             'MSA - Form of Active Sharing',\\\n",
        "             'MSA - Start Date for Active Sharing Arrangement',\\\n",
        "             'MSA - End Date for Active Sharing Arrangement',\\\n",
        "             'MSA - Sub-Lease Applies',\\\n",
        "             'MSA - Date of Equipment Removal (VF)',\\\n",
        "             'MSA - Date of Equipment Removal (WH)']\n",
        "\n",
        "in_month = '/content/TowerDB_Greece_20210731_renan.csv'\n",
        "true_up = '/content/TowerDB_Greece_20210630.csv'\n",
        "old_na = 'TowerDB_Greece_20210630.csv'\n",
        "new_na = \"TowerDB_Greece_20210731_renan.csv\"\n",
        "to_save = '/content/GR_TW'\n",
        "index = 'Identification - Site Key'\n",
        "find_diffs_between_files(true_up, in_month, index, bill_cols, to_save, old_na,new_na,'csv',status_col = 'Other - Status',kind='tw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28XhASgfFrrU"
      },
      "source": [
        "## UIS Comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exR02yWyF1Rk"
      },
      "source": [
        "# Wind "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGvXyH7uFp5A"
      },
      "source": [
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    import pandas as pd\n",
        "    \n",
        "    import numpy as np\n",
        "    import datetime as dt\n",
        "    from datetime import datetime\n",
        "    import re\n",
        "    from openpyxl import Workbook, styles\n",
        "    from openpyxl.styles import PatternFill, Font\n",
        "    from openpyxl.styles.differential import DifferentialStyle\n",
        "    from openpyxl.formatting.rule import Rule\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col]\n",
        "\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old,  engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new).fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "uis_new = '/content/UserInput_Greece_WH_20210831.xlsx'\n",
        "uis_old = '/content/UserInput_Greece_WH_20210731.xlsx'\n",
        "sheet = 'SiteLevel'\n",
        "uis_index = 'Site_ID (Alphanumeric or Numeric)'\n",
        "to_uis = '/content/GR_UIS_WH'\n",
        "old_uis = '20210731.xlsx'\n",
        "new_uis = '20210831.xlsx'\n",
        "bill = []\n",
        "find_diffs_between_files(uis_old, uis_new, uis_index, bill, to_uis, old_uis,new_uis,'excel',status_col='',\\\n",
        "                         kind='',kind_col='', sheetname=sheet, skipr=2, skipc=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT5j1hhiGWLx"
      },
      "source": [
        "# Vodafone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoQSw3jOGXpQ",
        "outputId": "d77f4c32-a74e-43be-a006-f167e7dad1a3"
      },
      "source": [
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    import pandas as pd\n",
        "    \n",
        "    import numpy as np\n",
        "    import datetime as dt\n",
        "    from datetime import datetime\n",
        "    import re\n",
        "    from openpyxl import Workbook, styles\n",
        "    from openpyxl.styles import PatternFill, Font\n",
        "    from openpyxl.styles.differential import DifferentialStyle\n",
        "    from openpyxl.formatting.rule import Rule\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col]\n",
        "\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old,  engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new).fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "uis_new = '/content/UserInput_Greece_VF_20210831.xlsx'\n",
        "uis_old = '/content/UserInput_Greece_VF_20210731.xlsx'\n",
        "sheet = 'SiteLevel'\n",
        "uis_index = 'Site_ID (Alphanumeric or Numeric)'\n",
        "to_uis = '/content/GR_UIS_VF'\n",
        "old_uis = '20210731.xlsx'\n",
        "new_uis = '20210831.xlsx'\n",
        "bill = []\n",
        "find_diffs_between_files(uis_old, uis_new, uis_index, bill, to_uis, old_uis,new_uis,'excel',status_col='',\\\n",
        "                         kind='',kind_col='', sheetname=sheet, skipr=2, skipc=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "\n",
            "New Rows:     ['1154-3010163', '124-1000124', '154-1000154', '156-1000156', '157-1000157', '177-1000177', '2134-1002134', '2193-1002193', '2316-1002316', '240-1000240', '244-1000244', '2648-2001218', '274-1000274', '279-1000279', '2913-2001217', '305-1000305', '306-2001304', '4986-3015543', '5531-3012398', '674-1000674', '867-1000867']\n",
            "Dropped Rows: ['1027-3010382', '108-1000108', '1334-3007858', '146-1000146', '150-1000150', '2227-1002227', '224-2001202', '229-1000229', '2294-1002294', '2396-1002396', '391-2001302', '45-1000045', '4964-2001062', '5291-3011000', '5337-3015543', '5619-3010594', '6260-1006260', '695-1000695', '809-1000809', '817-2001118']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Iwn6_CqjQZi",
        "outputId": "81acc4f0-570a-4e9f-c21f-ebc57d0bc3f2"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    import pandas as pd\n",
        "    from unidecode import unidecode\n",
        "    import numpy as np\n",
        "    import datetime as dt\n",
        "    from datetime import datetime\n",
        "    import re\n",
        "    from openpyxl import Workbook, styles\n",
        "    from openpyxl.styles import PatternFill, Font\n",
        "    from openpyxl.styles.differential import DifferentialStyle\n",
        "    from openpyxl.formatting.rule import Rule\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col]\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,index_col, status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        new_copy[index_col] = new_copy[index_col].apply(unidecode)\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'Old value:{value_OLD} > New Value:{value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old,  engine='python').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new,  engine='python').fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, engine='python').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        #towerdb = towerdb.reindex(columns = lower_str(col_order))\n",
        "        df_OLD[index_col] = df_OLD[index_col].apply(unidecode)\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        " \n",
        "        \"\"\"\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python', encoding='windows-1252').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\"\"\"\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW,index_col, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "bill_cols = ['TowerDB owner',\\\n",
        "             'Identification - Site Key',\\\n",
        "             'Category - Categorisation by inhabitants',\\\n",
        "             'Other - Ownership',\\\n",
        "             'Other - Status',\\\n",
        "             'Lease Contract - Current annual lease fee',\\\n",
        "             'Tenants - VF - Passive',\\\n",
        "             'Tenants - Wind - Passive',\\\n",
        "             'Tenants - Cosmote - Passive',\\\n",
        "             'Tenants - Passive tenants',\\\n",
        "             'EE - No. of 24Hr generators',\\\n",
        "             'EE - No. of standby generators',\\\n",
        "             'Other - Indoor vs Outdoor',\\\n",
        "             'MSA - Standard Configuration',\\\n",
        "             'MSA - Construction Type',\\\n",
        "             'BP - Consolidated classification',\\\n",
        "             'MSA - Critical Site',\\\n",
        "             'MSA - Rectifier',\\\n",
        "             'MSA - Battery',\\\n",
        "             'MSA - Aircon',\\\n",
        "             'MSA - Billing Trigger Date',\\\n",
        "             'MSA - DG-Shelter',\\\n",
        "             'MSA - Critical Site in excess of the 15.5% cap',\\\n",
        "             'MSA - Form of Active Sharing',\\\n",
        "             'MSA - Start Date for Active Sharing Arrangement',\\\n",
        "             'MSA - End Date for Active Sharing Arrangement',\\\n",
        "             'MSA - Sub-Lease Applies',\\\n",
        "             'MSA - Date of Equipment Removal (VF)',\\\n",
        "             'MSA - Date of Equipment Removal (WH)']\n",
        "             \n",
        "path_tw = \"/content/GR_TowerDB Jun'21 FINAL (Billing version).xlsx\"\n",
        "msa = \"/content/TowerDB_Greece_20210731.csv\"\n",
        "sheet_tw = 'GR_TowerDB'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "to_save = '/content/GR_TW'\n",
        "old_n = 'July.csv'\n",
        "new_n = \"August.xlsx\"\n",
        "index = 'Identification - Site Key'\n",
        "\n",
        "find_diffs_between_files(msa, path_tw, index, bill_cols, to_save, old_n,new_n,'mix',\\\n",
        "                         status_col = 'Other - Status', kind='tw',kind_col='',sheetname=sheet_tw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['90642-Α64201', '90750-Α75002']\n",
            "Dropped Rows: ['1135-113501', '1448-144801', '90642-A64201', '90750-A75002', '91250-D25002']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}