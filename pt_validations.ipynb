{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pt_validations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P_G8dsjMrO1"
      },
      "source": [
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "\n",
        "def csv_files(path):\n",
        "    \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "    import csv\n",
        "    f = open(path, encoding='windows-1252', errors='ignore')\n",
        "    data = []\n",
        "    for row in csv.reader(f, delimiter=','):\n",
        "        data.append(row)\n",
        "    col = [*data[0]]\n",
        "    data.pop(0)\n",
        "    df = pd.DataFrame(data, columns=col)\n",
        "    return df, col\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index):\n",
        "    \"\"\"\n",
        "    Params:\\n\n",
        "    path: parth of file in the computer.\\n\n",
        "    n_skiprows: Number of rows to delete in the original file,.\\n\n",
        "    columns_to_convert: Columns to convert the data general type. \\n\n",
        "    n_skipcolumn: Columns to skip in the original file. \\n\n",
        "    endrow = pass 0 to read everything, 1 to count entire\n",
        "    columns_order: List of columns names in specific order to pass in the engine.\\n\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    \"\"\"cont = 0\n",
        "    for i in df.iloc[:,site_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[:cont, :]\n",
        "    #df.columns = columns_order\"\"\"\n",
        "    \n",
        "    # convert intery columns to integer \n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].fillna(0)\n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].astype('int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def check_df(df, error_msg):\n",
        "    if df == None or df.empty:\n",
        "        return f'{error_msg}'\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def count_duplicates(lista):\n",
        "    count_dict = {}\n",
        "    for entry in lista:\n",
        "        if entry in count_dict.keys():\n",
        "            count_dict[entry] += 1\n",
        "        else:\n",
        "            count_dict[entry] = 1\n",
        "    \n",
        "    duplicates = {}\n",
        "    for k, v in count_dict.items():\n",
        "        if v > 1:\n",
        "            duplicates[k] = v\n",
        "    return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "def defining_df(df, column_range, number_col):\n",
        "    cont = 0\n",
        "    for i in df.iloc[:,number_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[0:cont, :]\n",
        "    return df\n",
        "\n",
        "#old version\n",
        "def check_columns(table, output_columns):\n",
        "    \"\"\"\n",
        "    Check the total of number of missing columns and the missing columns in passed table.\\n\n",
        "\n",
        "    Params:\\n\n",
        "    table: contain the columns to be check.\\n\n",
        "    output_columns: columns structure at the final file. \n",
        "\n",
        "    Returns:\\n\n",
        "    Number of missing columns and a list that contains the name os missing columns.\n",
        "    \"\"\"\n",
        "    \"\"\"\"\n",
        "    countries = ['DE':{'towerDB':[],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':[]}\\,\n",
        "                 'HU',\n",
        "                 'IE',\n",
        "                 'RO',\n",
        "                 'PT',\n",
        "                 'ES',\n",
        "                 'CZ': {'towerDB':[\"Code (Duplicate)\",\"Site Status\",\"VF - In scope / out of scope (Generalised scoping)\",\"Site in Skylon scope Actual (From Site List Sheet )\",\"Legacy Site Code(Duplicate)\",\"TIMS Site Code\",\"Legacy Site Code\",\"Site Name\",\"Macro Region\",\"Province\",\"Municipality\",\"Inhabitants\",\"Address\",\"Ground Register\",\"Altitude\",\"Latitude\",\"Longitude\",\"Categorization by Inhabitants\",\"Categorization by Transmission Sys\",\"Categorization by Site Type\",\"Categorization by Transmission Sys (subcluster)\",\"Other internal Categorization 1 (Identify ACQ Sites)\",\"Other internal Categorization 2 Energy provider (Eon/ LL)\",\"DAS+Macro\",\"DAS (Yes/ No)\",\"DAS Ownership (Complete/ Partial/ 3rd Party)\",\"Active/ Passive DAS\",\"# of remote units/ radiating points\",\"Type of Structure\",\"Distance highest antenna to ground level\",\"GBT Tower height\",\"POD ID\",\"Energy Consumption LTM (kwh)\",\"Annual Energy cost LTM (Euros)\",\"Infrastructure ready (existing)/ to be ready (new)\",\"Infrastructure to be dismantled by\",\"Radio equipments to be deactivated by\",\"Infrastructure to be shared by\",\"Technology VOD\",\"Fibre / Microwave\",\"Vertical passive structure owner\",\"Room configuration (detailed)\",\"Shelter passive structure ownership\",\"Type of Air Conditioning\",\"Number of cabinets (Full Capacity)\",\"Number of Antenna (Full Capacity)\",\"Number of MW (Full Capacity)\",\"Counterpart\",\"# of Lease Contracts\",\"Current annual lease fees \",\"Current other fees (Maintenance)\",\"Current other fees\",\"(Average) residual duration - Lease contract\",\"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\"# of Tenants Agreements\",\"Current Total Annual Hosting Fees\",\"Tenant (name/ID) MNO1 (Česká telekomunikační infrastruktura a.s.)\",\"Annual Fee per Tenant MNO1\",\"Annual Energy Fee MNO1\",\"Annual Maintenance Fee MNO1\",\"Other Services Fee MNO1\",\"Residual duration MNO1 (Years)\",\"Tenant (name/ID) MNO2 (T-Mobile Czech Republic a.s.)\",\"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\"Annual Maintenance Fee MNO2\",\"Other Services Fee MNO2\",\"Residual duration MNO2 (days)\",\"Tenant (name/ID) MNO3\",\"Annual Fee per Tenant MNO3\",\"Annual Energy Fee MNO3\",\"Annual Maintenance Fee MNO3\",\"Other Services Fee MNO3\",\"Residual duration MNO3\",\"# of OTMOs\",\"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\"Annual Maintenance Fee OTMOs\",\"Other Services Fee OTMOs\",\"Average residual duration (days)\",\"Check\",\"Strategic Macro Sites\",\"Critical Sites\",\"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\"Transmission Sites\",\"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\"VF-CZ Demerger phase\",\"EVO Location [FAR Site ID] \",\"Billing Trigger date \",\\\n",
        "                 \"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\"Wip_Site\",\"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', 'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', 'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index','Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', 'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', 'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month','Updated Item', 'Comment', 'Counterpart_extra_1', 'Counterpart_extra_2', 'x','SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count', 'not_def_1']}\\,\n",
        "                 }\n",
        "                 'GR']\n",
        "                 \"\"\"\n",
        "    total_received = len(table.columns)\n",
        "    number_missing_columns = 0\n",
        "    missing_columns = []\n",
        "    #Counting of missing columns       \n",
        "    #if country in contries:      \n",
        "    for columns in output_columns:\n",
        "        if columns.lower() not in [labels.lower() for labels in table]:\n",
        "            number_missing_columns +=1\n",
        "            missing_columns.append(columns)\n",
        "    \n",
        "    return total_received, number_missing_columns, missing_columns\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    \"\"\"\n",
        "    for i in bill_cols:\n",
        "        if i not in twdb_col:\n",
        "            col_miss.append(i)\"\"\"\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", 0, '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "      \n",
        "def date_parser(df, columns, format, type_dates='normal'):\n",
        "    t_col = type_dates.lower()\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Refactorar esse codigo para receber todas as colunas num dic\n",
        "# Sendo as keys=columns e values= picklist for each column\n",
        "def check_date_columns(df, df_index,status_col,columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "\n",
        "\n",
        "def check_amounts(df_check, df_index, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "        \n",
        "def check_picklist(df,df_index,df_status, df_cols, picklist_dict):\n",
        "\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        columns = [i.lower() for i in picklist_dict.keys()]\n",
        "        for column in set(columns): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            if not value in set(picklist_dict[column]) or pd.isnull(value):\n",
        "                #print(set(picklist_dict[column]))\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "                else:\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "\n",
        "    #df = df_errors.dropna()   \n",
        "    df_errors = df_errors.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    if len(df_errors)>0:\n",
        "        df = df[[df_index, df_status]]\n",
        "        df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "        df_errors = df_errors.reset_index()\n",
        "    else:\n",
        "        print('\\nNo one Picklist Error Founded!')\n",
        "    return df_errors\n",
        "\n",
        "def check_picklist_3(df,df_index, picklist_dict):\n",
        "    log = {}\n",
        "    for column in picklist_dict:\n",
        "        df_aux = df.copy()\n",
        "        new = df_aux[column].isin(picklist_dict[column])\n",
        "        #new = df_aux[column].apply(lambda x: x in picklist_dict[column])\n",
        "        # Aceita somento os valores que não estão na picklist\n",
        "        indexes = df.index[new == False].tolist()\n",
        "        #if len(indexes)>0:\n",
        "        if column not in log:log[column]=[]\n",
        "        log[column]=log[column]+indexes\n",
        "    #print(log.keys())\n",
        "\n",
        "    newDict ={}\n",
        "    df1 = pd.DataFrame()\n",
        "    for key,value in log.items():\n",
        "        for val in value:  \n",
        "            ID=df.iloc[val][df_index]\n",
        "            if ID in newDict:\n",
        "                newDict[ID].append(key)\n",
        "            else:\n",
        "                newDict[ID] = [key]\n",
        "        \n",
        "    logs = pd.DataFrame.from_dict(newDict, orient='index')\n",
        "    return logs\n",
        "\n",
        "#check on air foi trocado pelo check_picklist\n",
        "def check_on_air_sites(df, df_index, df_cols):\n",
        "\n",
        "    df_check = df[df_cols]\n",
        "    df_check['sites'] = df_check[df_index]\n",
        "    df_check = df_check.set_index('sites')\n",
        "    #df_check = replace_values(df_check, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in df_cols: \n",
        "            if df_check.loc[site,df_index] not in df_errors.index:\n",
        "                df_errors.loc[site,df_index] = df_check.loc[site,df_index]\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "            else:\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    df = df_errors[df_errors.iloc[:,1:]]\n",
        "    df = df.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    #df = df_errors.dropna(how='all', axis=1)   \n",
        "    return df\n",
        "\n",
        "\n",
        "    \"\"\"Restruturar o script em CZ, DE, PT\"\"\"\n",
        "\n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    return df\n",
        "\n",
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[wip_tw]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, wip_tw, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "    \"\"\"Reestrurar os script em PT, DE, CZ\"\"\"\n",
        "    # Falta os outros países\n",
        "\n",
        "def check_decommissioned(df,df_index, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    return filtered[[df_index, decom_col, doer_col]]\n",
        "    \"\"\"Ajustar para CZ, DE, PT\"\"\"\n",
        "    \n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]') < current_date)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "        else:\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]').empty)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "    \"\"\"Ajustar para CZ, DE, PT, IE\"\"\"\n",
        "\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col, status_col):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)]\n",
        "        return filtered[[tw_index, status_col, doer_col]]  \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "                                                              \n",
        "def check_mom_bts(df_tw, tw_index, tw_col, df_msa,msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    return filtered[[tw_index, tw_col]]         \n",
        "\n",
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "    df[start_date] = pd.to_datetime(df[start_date])\n",
        "    df[end_date] = pd.to_datetime(df[end_date], errors='coerce')\n",
        "    filtered = df.loc[pd.to_datetime(df[start_date]) > df[end_date], [tw_index, start_date,end_date]]\n",
        "    print(filtered)\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "    t1 = ['pt', 'de', 'cz', 'ie', 'es', 'ro', 'hu']\n",
        "    if country.lower() in t1:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        if decom_col != \"\":\n",
        "            tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "            decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "\n",
        "        \n",
        "            return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "        else:\n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "            return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "        decomiss_sites_in_uip = []\n",
        "        if set(tw_decomiss).intersection(uip_sites):\n",
        "            decomiss_sites_in_uip = [i for i in tw_decomiss if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(decomiss_sites_in_uip, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip = [i for i in df_uip['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Yes'][tw_index]\n",
        "        critical = []\n",
        "        if set(uip).intersection(bts_tw_critical):\n",
        "            critical = [i for i in bts_tw_critical if i not in uip]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% out in UIP File'])\n",
        "\n",
        "        return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    return df_commercial_diffs\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() \n",
        "\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col]\n",
        "\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old,  engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new).fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMT9CgnZXy5M"
      },
      "source": [
        "Compare oldest and newest files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCmJ7wRvSvM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f1ae490-9698-4087-d89b-e35b04588a93"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', \n",
        "                             status_col='', kind='tw', dates=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col):\n",
        "        #print(df.head(1))\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "        t_col = type_dates.lower()\n",
        "        if format == 1:\n",
        "            type_date = \"%d/%m/%Y\"\n",
        "        else:\n",
        "            type_date = \"%d-%m-%Y\"\n",
        "        for column in lower_str(columns):\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new).fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,encoding='latin1').fillna('')\n",
        "        cols_old = lower_str(list(df_OLD.columns))\n",
        "        df_OLD.columns = cols_old\n",
        "        df_OLD = fit_df(df_OLD, index_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        for i in dates:\n",
        "            df_NEW[i] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) \\\n",
        "                         and not isinstance(date_obj, str) else date_obj for date_obj in df_NEW[i]]\n",
        "            df_NEW[i] = df_NEW[i].astype(str)\n",
        "            \n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW[index_col] = df_NEW[index_col].astype(str)\n",
        "        df_NEW['sites'] = df_NEW[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        #df_NEW = df_NEW[cols_old]\n",
        "\n",
        "        #df_NEW = fit_df(df_NEW, index_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "bill_cols = [\"Site Code\",\\\n",
        "             \"Macro Region\",\\\n",
        "             \"Categorization by Transmission Sys\",\\\n",
        "             \"Categorization by Site Type\",\\\n",
        "             \"Core site type\",\\\n",
        "             \"Transmission Hub Site (YES/NO)\",\\\n",
        "             \"Transmission Hub Site (inc. with/without Shelters)\",\\\n",
        "             \"Transmission Site (inc. with/without Shelters)\",\\\n",
        "             \"Room configuration\",\\\n",
        "             \"Climate Control (YES/NO)\",\\\n",
        "             \"Power Supply \",\\\n",
        "             \"Strategic Site (YES/NO)\",\\\n",
        "             \"Critical Site (YES/NO)\",\\\n",
        "             \"Is the Site a WIP site\",\\\n",
        "             \"\tNOS shared site (Yes/No)\",\\\n",
        "             \"\tMEO shared site (Yes/No)\",\\\n",
        "             \"BTS Sites (Yes/No)\",\\\n",
        "             \"Sites_As_Metered_Estimated\",\\\n",
        "             \"Strategic_Site_Bucket\",\\\n",
        "             \"Critical_Site_Beyond_10\",\\\n",
        "             \"First_Active_Sharing_Deployment_Type \",\\\n",
        "             \"First_Active_Sharing_Start_Date\",\\\n",
        "             \"First_Active_Sharing_End_Date \",\\\n",
        "             \"Subsequent_Sharing_Arrangement\",\\\n",
        "             \"Legacy_Site_Agreement_Terminated(Yes/NO)\",\\\n",
        "             \"Decommissioned Sites(True/false)\"]\n",
        "\n",
        "#'Billing Trigger Date',\\\n",
        "path = '/content/Vantage Towers_PT TowerDB Jun21_20210720.xlsx'\n",
        "msa = '/content/TowerDB_Portugal_20210731.csv'\n",
        "sheet = 'Final Delivery'\n",
        "skiprows = 6\n",
        "skipcolumns = 1\n",
        "to_save = '/content/PT_TW'\n",
        "old = 'TowerDB_Portugal_20210731.csv'\n",
        "new = \"TowerDB_Portugal_Jun21_20210720.xlsx\"\n",
        "dates_tw = ['infrastructure ready (existing)/ to be ready (new)', 'infrastructure to be dismantled by', \n",
        "            'date when vodafone active equipment is removed', 'infrastructure to be shared by']\n",
        "#(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='', kind='tw', sheetname='', skipr=0, skipc=0)\n",
        "find_diffs_between_files(msa, path, 'Site Code', bill_cols, to_save, old, new,type_file='mix',status_col='Status',kind='tw',\\\n",
        "                         dates=dates_tw, sheetname=sheet, skipr=6, skipc=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     []\n",
            "Dropped Rows: []\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATFavsLYAEt4"
      },
      "source": [
        "def find_diffs_between_files_uis(path_OLD, path_NEW, index_col, \\\n",
        "                             path_save, old_name, new_name, type_file='mix', sheetname='', skipr=0, skipc=0):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df):\n",
        "\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        #df = df[uis_cols]\n",
        "        #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    #uis_cols = lower_str(uis_cols)\n",
        "    #index_col = index_col.lower()\n",
        "    # type_file = type_file.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD, sheet_name = sheetname,  skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = df_OLD.drop([0,0])\n",
        "        df_OLD = fit_df(df_OLD)\n",
        "\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,  skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = df_NEW.drop([0,0]) \n",
        "        df_NEW = fit_df(df_NEW)\n",
        "\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "    else:\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "    print(sharedCols)\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0,thresh=len(new_copy.columns[1:]))\n",
        "    new_copy = new_copy.dropna(axis=1)\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - [{old_name}] vs [{new_name}].xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "col_uis = []\n",
        "       \n",
        "pathuis_n = ''\n",
        "pathuis_o = '/content/UserInput_Portugal_20210731.xlsx'\n",
        "sheetuis='SiteLevel'\n",
        "skiprows=2\n",
        "to_save = '/content/HU_UIS'\n",
        "\n",
        "old_n = 'UIS - 20210630_true_up.xlsx'\n",
        "new_n = 'UIS - 20210831.xlsx'\n",
        "\n",
        "find_diffs_between_files_uis(pathuis_o, pathuis_n, 'Site_ID (Alphanumeric or Numeric)', \\\n",
        "                        to_save, old_n, new_n, type_file='excel', sheetname=sheetuis, skipr=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NdgiVFXH-Ll"
      },
      "source": [
        "Entiry Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ8aFiPcS0Gb"
      },
      "source": [
        "Adding new columns for UIP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pOg9Tt-PyXi"
      },
      "source": [
        "Change to openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPpfgkGvAbFb"
      },
      "source": [
        "# Lendo o Ficheiro de input\n",
        "path_uip='/content/UserInput_Portugal_20210731.xlsx'\n",
        "sheet = 'SiteLevel'\n",
        "skiprows = 6\n",
        "skipcolumns = 1\n",
        "site_level = pd.read_excel(path_uip,sheet, header=[0,1,2])\n",
        "head_1 = pd.MultiIndex.from_product([[''],['Numeric (in months)'],['Delay in Site Modification Projects', 'Delay in BTS Projects']])\n",
        "df_1 = pd.DataFrame(columns=head_1)\n",
        "\n",
        "head_2 = pd.MultiIndex.from_product([[''],['Numeric (in €)'],['Excess of Upgrade Capital Expenditure over Threshold']])\n",
        "df_2 = pd.DataFrame(columns=head_2)\n",
        "\n",
        "site_level = pd.concat([site_level, df_1, df_2], axis=1)\n",
        "site_level.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDutpsYT5OPC"
      },
      "source": [
        "cols_ordered = [\"Site Code\",\"Site Name\",\"EVO Site ID\",\"Macro Region\",\"Region\",\"Province\",\"Municipality\",\"Nr. Inhabitants\",\"Address\",\\\n",
        "                \"Altitude\",\"Latitude\",\"Longitude\",\"Ground Register\",\"Categorization by inhabitants\",\\\n",
        "                \"Categorization by Transmission Sys\",\"Categorization by Transmission Sys (sub-cluster)\",\\\n",
        "                \"Categorization by Site Type\",\"Other internal categorisation I\",\"Categorisation by connectivity\",\\\n",
        "                \"Technology VOD\",\"Technology VOD: Auxiliar\",\"Fibre / Microwave\",\"Type of Structure\",\"Tower Height (m)\",\\\n",
        "                \"Floor space\",\"Floor space (availability)\",\"Status\",'Billing Trigger Date',\\\n",
        "                \"Infrastructure to be dismantled by\",\"Date when Vodafone active equipment is removed\",\\\n",
        "                \"Infrastructure to be shared by\",\"Core site type\",\"Transmission Hub Site (YES/NO)\",\\\n",
        "                \"Transmission Hub Site (inc. with/without Shelters)\",\"Transmission Site (inc. with/without Shelters)\",\\\n",
        "                \"Room configuration\",\"An Indoor + Outdoor Site\",\"Climate Control (YES/NO)\",\"Air Conditioning (YES/NO)\",\\\n",
        "                \"Free-Air Cooling (YES/NO)\",\"Power Supply \",\"Active/Passive DAS\",\"Strategic Site (YES/NO)\",\"Critical Site (YES/NO)\",\\\n",
        "                \"\tIf a Non-Critical and non-Transmission Hub Site, Category 1 or Category 2\",\"Is the Site a WIP site\",\\\n",
        "                \"Indicative completion date?\",\"Capital expenditure incurred to the MSA Effective Date\",\\\n",
        "                \"Indicative capital expenditure required to complete the site build\",\"Vertical Infrastructure \",\"Site Type\",\\\n",
        "                \"Technology_VOD\",\"Fibre/Microwave\",\"Power Supply\",\"# Antennas\",\"# Minilinks\",\\\n",
        "                \"\tDetails of Operator Equipment installed at the Site\",\"Site Configuration\",\"\tNOS shared site (Yes/No)\",\\\n",
        "                \"\tMEO shared site (Yes/No)\",\\\n",
        "                \"Details of any Active Sharing involving Operator applicable in respect of the Site (and, where applicable, the type of Active Sharing);\",\\\n",
        "                \"\tDetails of the access arrangements applicable at each Site (including any access restrictions and applicable public access requirements).\",\\\n",
        "                \"Sites under Hawkins arrangements (Yes/No)\",\"Hawkins Sites (Yes/No)\",\"NOS Crossed Site (Yes/No)\",\"BTS Sites (Yes/No)\",\\\n",
        "                \"Energy Provider \",\"POD ID\",\"Annual Energy Consumption (kWh)\",\"Annual Energy cost (€)\",\\\n",
        "                \"Total Energy Cost (Energy provider + Landlord)\",\"# Lease Contracts\",\"Countepart\",\"Total Annual Lease (€)\",\\\n",
        "                \"Current annual lease fees (€)\",\"Current energy lease fees (€)\",\"Current annual other fees (€)\",\\\n",
        "                \"Annual Lease fees + Annual Other fees\",\"Sub-Lease\",\"(Latest) residual duration until expiring date\",\\\n",
        "                \"Maturity Cluster - Expiring date\",\"(Latest) residual duration until expiring date after renewal\",\\\n",
        "                \"Maturity Cluster - Expiring date after renewal\",\"Type of lease contract\",\"Reason for no lease fee\",\\\n",
        "                \"Comments to Lease Contracts\",\"NOS (1/0)\",\"Total Revenues NOS (€)\",\"Annual Hosting fee NOS (€)\",\\\n",
        "                \"Annual Energy fee NOS (€)\",\"Annual Maintenance fee NOS (€)\",\"Annual Other Services fee NOS (€)\",\\\n",
        "                \"Tenancy duration until expiring date NOS\",\"Maturity Clusters NOS\",\"MEO (1/0)\",\"Total Revenues MEO (€)\",\\\n",
        "                \"Annual Hosting fee MEO (€)\",\"Annual Energy fee MEO (€)\",\"Annual Maintenance fee MEO (€)\",\\\n",
        "                \"Annual Other Services fee MEO (€)\",\"Tenancy duration until expiring date MEO\",\"Maturity Clusters MEO\",\\\n",
        "                \"OTMOs (1/0)\",\"Total Revenues OTMOs (€)\",\"Annual Hosting fee OTMOs (€)\",\"Annual Energy fee OTMOs (€)\",\\\n",
        "                \"Annual Maintenance fee OTMOs (€)\",\"Annual Other Services fee OTMOs (€)\",\"Tenancy duration until expiring date OTMOs\",\\\n",
        "                \"Maturity Clusters OTMOs\",\"Total # of 3rd Party Tenants\",\"Name of 3rd Party Tenants\",\"Type of Sharing\",\\\n",
        "                \"Total Hosting Fee & Services from 3rd Party Tenants (€)\",\"Annual Hosting fee from 3rd Party Tenants (€)\",\\\n",
        "                \"Annual Energy fee from 3rd Party Tenants (€)\",\"Annual Maintenance fee from 3rd Party Tenants (€)\",\\\n",
        "                \"Other Services fee from 3rd Party Tenants (€)\",\"Weighted Average tenancy duration until expiring date\",\\\n",
        "                \"Total # of Tenants\",\"Macro Cluster Tenancy\",\"Comments to Tenant Agreements\",\"Macro Cluster Type of contract\",\\\n",
        "                \"Easement (Servitù di passaggio)\",\"Turistic Sites\",\"Sites_As_Metered_Estimated\",\"Strategic_Site_Bucket\",\\\n",
        "                \"Critical_Site_Beyond_10\",\"First_Active_Sharing_Deployment_Type \",\"First_Active_Sharing_Start_Date\",\\\n",
        "                \"First_Active_Sharing_End_Date \",\"Subsequent_Sharing_Arrangement\",\"Legacy_Site_Agreement_Terminated(Yes/NO)\",\\\n",
        "                \"Decommissioned Sites(True/false)\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EHzXea8aDin",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "fe706502-17d1-4667-838d-ef46e2a654bd"
      },
      "source": [
        "# Lendo o Ficheiro de input\n",
        "path ='/content/Vantage Towers_PT TowerDB Jun21_20210720.xlsx'\n",
        "sheetname = 'Final Delivery'\n",
        "skiprows = 6\n",
        "skipcolumns = 1\n",
        "\n",
        "towerdb = read_files(path, sheetname, skiprows, skipcolumns, \"Site Code\")\n",
        "towerdb.rename(columns={'Billing Trigger Stop Date from May21 onwards': 'date_of_equipment_removal'}, inplace=True)\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "#towerdb = towerdb[lower_str(cols_ordered)]\n",
        "\n",
        "\"\"\"First Step Check All columns received\"\"\"\n",
        "path_msa = '/content/TowerDB_Portugal_20210731.csv'\n",
        "df,cols_msa = csv_files(path_msa)\n",
        "df_msa = pd.read_csv(path_msa, names=cols_msa, encoding='latin')\n",
        "df_msa.columns = lower_str(list(df_msa.columns))\n",
        "#df_msa = df_msa[lower_str(cols_ordered)]\n",
        "\n",
        "# pegar as colunas de billing\n",
        "bill_cols = ['Site Code','Macro Region','Categorization by Transmission Sys','Billing Trigger Date',\\\n",
        "             \"Infrastructure to be dismantled by\",'Date when Vodafone active equipment is removed','Core site type','Transmission Hub Site (YES/NO)',\\\n",
        "             'Transmission Hub Site (inc. with/without Shelters)','Transmission Site (inc. with/without Shelters)',\\\n",
        "             'Room configuration','Climate Control (YES/NO)','Power Supply ','Strategic Site (YES/NO)','Critical Site (YES/NO)',\\\n",
        "             'Is the Site a WIP site','\tNOS shared site (Yes/No)','\tMEO shared site (Yes/No)','BTS Sites (Yes/No)',\\\n",
        "             'Total Annual Lease (€)','Annual Lease fees + Annual Other fees','Sites_As_Metered_Estimated','Strategic_Site_Bucket',\\\n",
        "             'Critical_Site_Beyond_10','First_Active_Sharing_Deployment_Type ','First_Active_Sharing_Start_Date',\\\n",
        "             'First_Active_Sharing_End_Date ','Subsequent_Sharing_Arrangement','Legacy_Site_Agreement_Terminated(Yes/NO)',\\\n",
        "             'Decommissioned Sites(True/false)']\n",
        "\n",
        "df_col_missing = check_columns_received(towerdb, list(df_msa.columns))\n",
        "df_col_missing\n",
        "# date_of_equipment_removal e infrastructure ready (existing)/ to be ready... não serão mais usadas\n",
        "#infrastructure ready (existing)/ to be ready... passou a ser o billing trigger date\n",
        "towerdb = towerdb[list(df_msa.columns)]\n",
        "\n",
        "for i in ['province', 'municipality', 'address', 'site name']:\n",
        "    towerdb[i] = towerdb[i].apply(unidecode)\n",
        "\n",
        "ints = ['floor space', 'tower height (m)', 'floor space (availability)', 'annual energy consumption (kwh)' ]\n",
        "for i in ints:\n",
        "    towerdb[i] = towerdb[i].fillna(0)\n",
        "    towerdb[i] = list(map(int, towerdb[i]))\n",
        "    \n",
        "dates_tw = ['infrastructure ready (existing)/ to be ready (new)', 'infrastructure to be dismantled by', 'date when vodafone active equipment is removed',\n",
        "            'infrastructure to be shared by', 'indicative completion date?', 'date_of_equipment_removal']\n",
        "for i in dates_tw:\n",
        "    towerdb[i] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in towerdb[i]]\n",
        "\n",
        "sites = [i for i in towerdb[towerdb['climate control (yes/no)']=='Yes; Indoor Air Conditioning and Free Air cooling / Free cooling units']['site code']]\n",
        "towerdb.loc[towerdb['site code'].isin(sites), 'climate control (yes/no)'] = 'Yes; Indoor Air Conditioning'\n",
        "\n",
        "sites = [i for i in towerdb[towerdb['climate control (yes/no)']=='Yes; Indoor Free Air cooling / Free cooling units']['site code']]\n",
        "towerdb.loc[towerdb['site code'].isin(sites), 'climate control (yes/no)'] = 'Yes; Indoor Air Conditioning'\n",
        "\n",
        "towerdb = towerdb.fillna('')\n",
        "\n",
        "towerdb.head(2)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site code</th>\n",
              "      <th>site name</th>\n",
              "      <th>evo site id</th>\n",
              "      <th>macro region</th>\n",
              "      <th>region</th>\n",
              "      <th>province</th>\n",
              "      <th>municipality</th>\n",
              "      <th>nr. inhabitants</th>\n",
              "      <th>address</th>\n",
              "      <th>altitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>ground register</th>\n",
              "      <th>categorization by inhabitants</th>\n",
              "      <th>categorization by transmission sys</th>\n",
              "      <th>categorization by transmission sys (sub-cluster)</th>\n",
              "      <th>categorization by site type</th>\n",
              "      <th>other internal categorisation i</th>\n",
              "      <th>categorisation by connectivity</th>\n",
              "      <th>technology vod</th>\n",
              "      <th>technology vod: auxiliar</th>\n",
              "      <th>fibre / microwave</th>\n",
              "      <th>type of structure</th>\n",
              "      <th>tower height (m)</th>\n",
              "      <th>floor space</th>\n",
              "      <th>floor space (availability)</th>\n",
              "      <th>status</th>\n",
              "      <th>infrastructure ready (existing)/ to be ready (new)</th>\n",
              "      <th>infrastructure to be dismantled by</th>\n",
              "      <th>date when vodafone active equipment is removed</th>\n",
              "      <th>infrastructure to be shared by</th>\n",
              "      <th>core site type</th>\n",
              "      <th>transmission hub site (yes/no)</th>\n",
              "      <th>transmission hub site (inc. with/without shelters)</th>\n",
              "      <th>transmission site (inc. with/without shelters)</th>\n",
              "      <th>room configuration</th>\n",
              "      <th>an indoor + outdoor site</th>\n",
              "      <th>climate control (yes/no)</th>\n",
              "      <th>air conditioning (yes/no)</th>\n",
              "      <th>free-air cooling (yes/no)</th>\n",
              "      <th>...</th>\n",
              "      <th>total revenues meo (€)</th>\n",
              "      <th>annual hosting fee meo (€)</th>\n",
              "      <th>annual energy fee meo (€)</th>\n",
              "      <th>annual maintenance fee meo (€)</th>\n",
              "      <th>annual other services fee meo (€)</th>\n",
              "      <th>tenancy duration until expiring date meo</th>\n",
              "      <th>maturity clusters meo</th>\n",
              "      <th>otmos (1/0)</th>\n",
              "      <th>total revenues otmos (€)</th>\n",
              "      <th>annual hosting fee otmos (€)</th>\n",
              "      <th>annual energy fee otmos (€)</th>\n",
              "      <th>annual maintenance fee otmos (€)</th>\n",
              "      <th>annual other services fee otmos (€)</th>\n",
              "      <th>tenancy duration until expiring date otmos</th>\n",
              "      <th>maturity clusters otmos</th>\n",
              "      <th>total # of 3rd party tenants</th>\n",
              "      <th>name of 3rd party tenants</th>\n",
              "      <th>type of sharing</th>\n",
              "      <th>total hosting fee &amp; services from 3rd party tenants (€)</th>\n",
              "      <th>annual hosting fee from 3rd party tenants (€)</th>\n",
              "      <th>annual energy fee from 3rd party tenants (€)</th>\n",
              "      <th>annual maintenance fee from 3rd party tenants (€)</th>\n",
              "      <th>other services fee from 3rd party tenants (€)</th>\n",
              "      <th>weighted average tenancy duration until expiring date</th>\n",
              "      <th>total # of tenants</th>\n",
              "      <th>macro cluster tenancy</th>\n",
              "      <th>comments to tenant agreements</th>\n",
              "      <th>macro cluster type of contract</th>\n",
              "      <th>easement (servitù di passaggio)</th>\n",
              "      <th>turistic sites</th>\n",
              "      <th>sites_as_metered_estimated</th>\n",
              "      <th>strategic_site_bucket</th>\n",
              "      <th>critical_site_beyond_10</th>\n",
              "      <th>first_active_sharing_deployment_type</th>\n",
              "      <th>first_active_sharing_start_date</th>\n",
              "      <th>first_active_sharing_end_date</th>\n",
              "      <th>subsequent_sharing_arrangement</th>\n",
              "      <th>legacy_site_agreement_terminated(yes/no)</th>\n",
              "      <th>decommissioned sites(true/false)</th>\n",
              "      <th>date_of_equipment_removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>BRAGA</td>\n",
              "      <td>PT11000001</td>\n",
              "      <td>CONTINENT</td>\n",
              "      <td>NORTE</td>\n",
              "      <td>Braga</td>\n",
              "      <td>Braga</td>\n",
              "      <td>182679</td>\n",
              "      <td>Travessa Escola, 4705-474 Esporões</td>\n",
              "      <td>518</td>\n",
              "      <td>41.516600</td>\n",
              "      <td>-8.395440</td>\n",
              "      <td></td>\n",
              "      <td>Urban</td>\n",
              "      <td>Macro</td>\n",
              "      <td>Standard</td>\n",
              "      <td>GBT</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>2G; 3G; 4G</td>\n",
              "      <td></td>\n",
              "      <td>Fibre</td>\n",
              "      <td>Lattice</td>\n",
              "      <td>51</td>\n",
              "      <td>240</td>\n",
              "      <td>223</td>\n",
              "      <td>In Service</td>\n",
              "      <td>01/01/1992</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Non Core</td>\n",
              "      <td>Yes</td>\n",
              "      <td>With shelters</td>\n",
              "      <td>Non Transmission Site</td>\n",
              "      <td>Indoor</td>\n",
              "      <td>NO</td>\n",
              "      <td>Yes; Indoor Air Conditioning</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>VF Only</td>\n",
              "      <td></td>\n",
              "      <td>Leased Area - VF Only</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Estimated Model</td>\n",
              "      <td>Non Strategic</td>\n",
              "      <td>Within 10%</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>RECEZINHOS</td>\n",
              "      <td>PT13000002</td>\n",
              "      <td>CONTINENT</td>\n",
              "      <td>NORTE</td>\n",
              "      <td>Porto</td>\n",
              "      <td>Penafiel</td>\n",
              "      <td>69772</td>\n",
              "      <td>Cruzes -Croca -  Penafiel, 4560-871 Siforno</td>\n",
              "      <td>411</td>\n",
              "      <td>41.218686</td>\n",
              "      <td>-8.229177</td>\n",
              "      <td></td>\n",
              "      <td>Urban</td>\n",
              "      <td>Macro</td>\n",
              "      <td>Standard</td>\n",
              "      <td>GBT</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>2G; 3G; 4G</td>\n",
              "      <td></td>\n",
              "      <td>Fibre</td>\n",
              "      <td>Monopole</td>\n",
              "      <td>26</td>\n",
              "      <td>80</td>\n",
              "      <td>71</td>\n",
              "      <td>In Service</td>\n",
              "      <td>09/02/1992</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Non Core</td>\n",
              "      <td>No</td>\n",
              "      <td>Non Transmission Hub Site</td>\n",
              "      <td>Non Transmission Site</td>\n",
              "      <td>Indoor</td>\n",
              "      <td>NO</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>VF Only</td>\n",
              "      <td></td>\n",
              "      <td>Leased Area - VF Only</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Estimated Model</td>\n",
              "      <td>Non Strategic</td>\n",
              "      <td>Non Critical</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 135 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   site code  ... date_of_equipment_removal\n",
              "0          1  ...                          \n",
              "1          2  ...                          \n",
              "\n",
              "[2 rows x 135 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTzhYwvI1naf"
      },
      "source": [
        "from unidecode import unidecode\n",
        "\n",
        "for i in ['province', 'municipality', 'address', 'site name']:\n",
        "    towerdb[i] = towerdb[i].apply(unidecode)\n",
        "towerdb.to_csv('/content/TowerDB_Portugal_20210831.csv', index=False)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n14BMZhNxKSQ"
      },
      "source": [
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = 'site code'\n",
        "tw_doer = 'date_of_equipment_removal'\n",
        "tw_dismentled = \"infrastructure to be dismantled by\"\n",
        "tw_status = 'status'\n",
        "tw_bts = 'bts sites (yes/no)'\n",
        "tw_bill ='billing trigger date'\n",
        "tw_wip = 'is the site a wip site'\n",
        "tw_decom = 'decommissioned sites(true/false)'\n",
        "tw_critical = 'critical site(yes/no)'\n",
        "tw_beyond = 'critical_site_beyond_10'\n",
        "\n",
        "msa_index ='site code'\n",
        "msa_doer = 'date_of_equipment_removal'\n",
        "msa_status = 'status'\n",
        "msa_bts = 'bts sites (yes/no)'\n",
        "msa_bill = 'billing trigger date'\n",
        "msa_wip = 'is the site a wip site'\n",
        "msa_decom = 'decommissioned sites(true/false)'\n",
        "msa_critical = 'critical site(yes/no)'\n",
        "msa_beyond = 'critical_site_beyond_10'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWiD-z2aORoM"
      },
      "source": [
        "Condição extra para retornar no log\n",
        "Only Portugal\n",
        "\n",
        "Fazer o check da contagem de Hawkins Sites. Se "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uARs8Ae0NB-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a41414c-fd94-4036-8c6b-30ca768a1539"
      },
      "source": [
        "def check_hawkins(df, tw_index, tw_hawkins):\n",
        "    cont = len(towerdb[~(towerdb[tw_hawkins]=='No') & ~(towerdb[tw_hawkins]=='')][tw_index])\n",
        "    sites = towerdb[~(towerdb[tw_hawkins]=='No') & ~(towerdb[tw_hawkins]=='')][tw_index]\n",
        "    if not cont == 480:\n",
        "        #Retornar a coluna toda e informar que está com error pois é maior que o esperado\n",
        "        df = df.loc[df[tw_index].isin(sites)]\n",
        "        return df[[tw_index, tw_hawkins]]\n",
        "    else:\n",
        "        print('There are 480 sites.')\n",
        "\n",
        "tw_haw = 'hawkins sites (yes/no)'\n",
        "df_hawkins = check_hawkins(towerdb, tw_index, tw_haw)\n",
        "df_hawkins\n",
        "# Missing Hawking Sites"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 480 sites.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XXnSPr6lJCs"
      },
      "source": [
        "Create CSV Renan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuGhB5VS9Lik"
      },
      "source": [
        "#Columns to parser\n",
        "#infra = towerdb['infrastructure to be shared by'].astype('datetime64')\n",
        "dates=[tw_bill, 'infrastructure to be shared by']\n",
        "date_parser(towerdb, dates, \"%d/%m/%Y\", 'no')\n",
        "\n",
        "#Sempre fazer o replace depois de todas as modificações\n",
        "towerdb = replace_values(towerdb, lower_str(cols_ordered))\n",
        "\n",
        "towerdb.round(2).to_csv('/content/TowerDB_Portugal_20210731.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbMRsZOe836i"
      },
      "source": [
        "CSV (In Month and True Up) Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDwpd1lQ9DEe"
      },
      "source": [
        "path_tw_in = '/content/TowerDB_Portugal_20210731.csv'\n",
        "df_in, cols_in = csv_files(path_tw_in)\n",
        "cols_in = lower_str(cols_in)\n",
        "twdb_month = pd.read_csv(path_tw_in, header=0, names=cols_in, engine='python',encoding='latin1')\n",
        "#twdb_month.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sTm_serUYne"
      },
      "source": [
        "cols_ordered = lower_str(cols_ordered)\n",
        "towerdb = replace_values(towerdb, cols_ordered)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F4LIaH4OWCb"
      },
      "source": [
        "\"\"\"You need to convert all values in cols for string format to check\"\"\"\n",
        "# Columns to functions\n",
        "dates_bill = [tw_index, tw_bill]\n",
        "dates_doer = [tw_index, tw_doer]\n",
        "#Columns to parser\n",
        "bill=[tw_bill]\n",
        "doer=[tw_doer]\n",
        "\n",
        "#Columns to parser\n",
        "dates=[tw_bill, tw_doer_vf, tw_doer_wh]\n",
        "date_parser(actives, dates, \"%d/%m/%Y\")\n",
        "\n",
        "actives_1 = towerdb[towerdb['on air / active']=='In Service']\n",
        "no_actives_1 = towerdb[~(towerdb[tw_status]=='In Service')]\n",
        "\n",
        "# date_parser(actives_1, bill, \"%d/%m/%Y\", 'no')  não precisar checkar o bill dos sites in service\n",
        "date_parser(no_actives_1, doer, \"%d/%m/%Y\", 'mixed')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q29CPZjVlEj"
      },
      "source": [
        "towerdb.columns.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3p4_BMXhQ6n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "outputId": "117dc941-62e1-4b90-e2ea-a5341d6710ed"
      },
      "source": [
        "def check_date_columns(df, df_index,status_col,columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            #df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "\"\"\"Second Check Date Format dd-mm-YYYY\"\"\"\n",
        "date_cols = ['Site Code', 'infrastructure ready (existing)/ to be ready (new)', 'Date when Vodafone active equipment is removed', \\\n",
        "             'First_Active_Sharing_Start_Date', 'First_Active_Sharing_End_Date ', 'Subsequent_Sharing_Arrangement']\n",
        "date_cols = lower_str(date_cols)\n",
        "dates = ['infrastructure ready (existing)/ to be ready (new)', 'Date when Vodafone active equipment is removed', \\\n",
        "             'First_Active_Sharing_Start_Date', 'First_Active_Sharing_End_Date ', 'Subsequent_Sharing_Arrangement']\n",
        "dates = lower_str(dates)\n",
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "date_parser(actives, dates, \"%d/%m/%Y\", 'no')             \n",
        "#Check dates of column 'Date_Of_Equipment_Removal' Pg 7\n",
        "\"\"\"Alterar o tipo da data para string para checkar\"\"\"\n",
        "dftw_dates_errors = check_date_columns(actives, tw_index, tw_status, date_cols,2)\n",
        "dftw_dates_errors \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:167: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:168: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site code</th>\n",
              "      <th>date when vodafone active equipment is removed</th>\n",
              "      <th>first_active_sharing_start_date</th>\n",
              "      <th>first_active_sharing_end_date</th>\n",
              "      <th>subsequent_sharing_arrangement</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3545</th>\n",
              "      <td>63083</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3546</th>\n",
              "      <td>63856</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3547</th>\n",
              "      <td>67476</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3548</th>\n",
              "      <td>67540</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3549</th>\n",
              "      <td>67575</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3550 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      site code  ...      status\n",
              "0             1  ...  In Service\n",
              "1             2  ...  In Service\n",
              "2             3  ...  In Service\n",
              "3             4  ...  In Service\n",
              "4             5  ...  In Service\n",
              "...         ...  ...         ...\n",
              "3545      63083  ...  In Service\n",
              "3546      63856  ...  In Service\n",
              "3547      67476  ...  In Service\n",
              "3548      67540  ...  In Service\n",
              "3549      67575  ...  In Service\n",
              "\n",
              "[3550 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7jeNnbahN4L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "ed34f480-63e4-4926-8a35-87a12a9522cc"
      },
      "source": [
        "picklist_cols = ['Site Code', 'Categorization by Transmission Sys', 'Core site type', 'Transmission Hub Site (YES/NO)',\\\n",
        "                 'Transmission Hub Site (inc. with/without Shelters)','Transmission Site (inc. with/without Shelters)',\\\n",
        "                 'Room configuration', 'Climate Control (YES/NO)', 'Power Supply ', 'Strategic Site (YES/NO)',\\\n",
        "                 'Critical Site (YES/NO)', 'Is the Site a WIP site', '\tNOS shared site (Yes/No)',\\\n",
        "                 '\tMEO shared site (Yes/No)', 'BTS Sites (Yes/No)', 'Sites_As_Metered_Estimated', 'Strategic_Site_Bucket', \\\n",
        "                 'Critical_Site_Beyond_10', 'Legacy_Site_Agreement_Terminated(Yes/NO)', 'Decommissioned Sites(True/false)']\n",
        "\n",
        "picklists = {\n",
        "    'categorization by transmission sys': ['Macro','Macro+DAS', 'Public DAS','Repeater','Transmission'],\n",
        "    'core site type': ['Non Core', 'Case A', 'Case B'],\n",
        "    'transmission hub site (yes/no)': ['Yes','No'], \n",
        "    'transmission hub site (inc. with/without shelters)': ['Non Transmission Hub Site', 'With shelters', 'Without shelters'],\n",
        "    'transmission site (inc. with/without shelters)': ['Non Transmission Site', 'Without shelters'],\n",
        "    'room configuration': ['Outdoor', 'Indoor'],\n",
        "    'climate control (yes/no)': ['Yes; Indoor Air Conditioning','', 'No'], \n",
        "    'power supply ': ['AC','DC'],\n",
        "    'strategic site (yes/no)': ['Yes','No'],\n",
        "    'critical site (yes/no)': ['Yes','No'],\n",
        "    'is the site a wip site': ['Yes','No'],\n",
        "    '\tnos shared site (yes/no)': ['Yes','No'],\n",
        "    '\tmeo shared site (yes/no)': ['Yes','No'],\n",
        "    'bts sites (yes/no)': ['Yes','No'],\n",
        "    'sites_as_metered_estimated': ['Estimated Model','Metered Model'],\n",
        "    'strategic_site_bucket': ['Yes - 0-5%','Non Strategic'],\n",
        "    'critical_site_beyond_10': ['Beyond 10%','Within 10%','Non Critical'],\n",
        "    'legacy_site_agreement_terminated(yes/no)': ['Yes','No'],\n",
        "    'decommissioned sites(true/false)': ['Yes','No']}\n",
        "\n",
        "picklist_cols = lower_str(picklist_cols)\n",
        "\n",
        "df_picklist= check_picklist(towerdb,tw_index,tw_status, picklist_cols, picklists)\n",
        "\n",
        "#df_picklist_errors = check_picklist_3(tw ,twdb_index, picklists)\n",
        "#Tem errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:281: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site code</th>\n",
              "      <th>status</th>\n",
              "      <th>room configuration</th>\n",
              "      <th>climate control (yes/no)</th>\n",
              "      <th>power supply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Incorret picklist value: Yes; Indoor Air Condi...</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Incorret picklist value: Yes; Indoor Air Condi...</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Incorret picklist value: Yes; Indoor Air Condi...</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Incorret picklist value: Yes; Indoor Air Condi...</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>45</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Incorret picklist value: Yes; Indoor Air Condi...</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>57558</td>\n",
              "      <td>WIP</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>57716</td>\n",
              "      <td>Pipeline</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>63858</td>\n",
              "      <td>Pipeline</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>68058</td>\n",
              "      <td>Ordered</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>75881</td>\n",
              "      <td>WIP</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>291 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     site code  ... power supply \n",
              "0            4  ...           Ok!\n",
              "1           24  ...           Ok!\n",
              "2           33  ...           Ok!\n",
              "3           41  ...           Ok!\n",
              "4           45  ...           Ok!\n",
              "..         ...  ...           ...\n",
              "286      57558  ...   Blank Value\n",
              "287      57716  ...   Blank Value\n",
              "288      63858  ...   Blank Value\n",
              "289      68058  ...           Ok!\n",
              "290      75881  ...   Blank Value\n",
              "\n",
              "[291 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8epQusu1hKth"
      },
      "source": [
        "def check_on_air_sites(df, df_index, df_cols):\n",
        "\n",
        "    df_check = df[df_cols]\n",
        "    df_check['sites'] = df_check[df_index]\n",
        "    df_check = df_check.set_index('sites')\n",
        "    #df_check = replace_values(df_check, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df_check[df_index]:\n",
        "        for column in df_cols: \n",
        "            if df_check.loc[site,df_index] not in df_errors.index:\n",
        "                df_errors.loc[site,df_index] = df_check.loc[site,df_index]\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "            else:\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    df_errors = df_errors[df_errors.iloc[:,1:]]\n",
        "    df_errors = df_errors.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    #df = df_errors.dropna(how='all', axis=1)   \n",
        "    return df_errors\n",
        "# On the air sites which have blank values\n",
        "on_air_columns = ['Site Code', 'Categorization by Transmission Sys','Categorization by Site Type',\\\n",
        "                  'Sites_As_Metered_Estimated', 'Infrastructure ready (existing)/ to be ready (new)', \\\n",
        "                  'Climate Control (YES/NO)', 'Strategic Site (YES/NO)', 'Critical Site (YES/NO)',\\\n",
        "                  'Is the Site a WIP site', 'Power Supply ', 'Strategic_Site_Bucket', \\\n",
        "                  'Critical_Site_Beyond_10','Legacy_Site_Agreement_Terminated(Yes/NO)', \\\n",
        "                  'Decommissioned Sites(True/false)']\n",
        "on_air_columns = lower_str(on_air_columns)\n",
        "#tw_on  = replace_values(towerdb, on_air_columns, 0)\n",
        "df_on_air = check_on_air_sites(actives, tw_index, on_air_columns)\n",
        "df_on_air\n",
        "#tem error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLkJHrRchGY9"
      },
      "source": [
        "\"\"\"New Sites Check\"\"\"\n",
        "# Read msa files to match site code information from towerdb file\n",
        "\n",
        "# Read UIS\n",
        "uip = pd.read_excel('/content/UserInput_Portugal_20210831.xlsx', sheet_name='SiteLevel', usecols=[0,1,2], skiprows=2).fillna('')\n",
        "uip_col = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip.columns = lower_str(uip_col)\n",
        "msa_sites = [i for i in df_msa[msa_index]]\n",
        "tow_sites = [str(i) for i in towerdb[tw_index]]\n",
        "uip_sites = [str(i) for i in uip['site_id']]\n",
        "\n",
        "# return 2 df, first are the new sites with no BTS flags marked\n",
        "# Second is the sites that are demerged dates than current date\n",
        "#check_new_sites(towerdb, msa_sites, tow_sites, uip_sites, country)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWOe_WrHX3fL"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [i for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uis = [i for i in df_towerdb[(df_towerdb[bts_col]=='Yes')&(df_towerdb[status_col]=='In Service')][tw_index] if i not in uip_list]\n",
        "    bts_out_uis = pd.DataFrame(bts_out_uis, columns=['Bts_Sites_Out_UIS_File'])\n",
        "    bts_out_uis = pd.merge(bts_out_uis, filtered, how='left', left_on=['Bts_Sites_Out_UIS_File'], right_on=tw_index)\n",
        "    bts_out_uis = bts_out_uis[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "    #if not (new_sites.empty or bts_out_uis.empty or df_site_bts.empty):\n",
        "    return new_sites, bts_out_uis, df_site_bts[[tw_index,status_col, bts_col, bill_col]]\n",
        "\n",
        "\n",
        "new_sites, bts_out_uis, df_site_bts = check_new_sites(towerdb,tw_index,tw_bts,tw_bill,tw_status, msa_sites, tow_sites, uip_sites)\n",
        "#No one errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1SMDACzhD7f"
      },
      "source": [
        "\"\"\" Wip Sites Check\"\"\"\n",
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, tw_status, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[wip_tw]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, tw_status,wip_tw, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "wip_out_tw_list, df_wip_and_bts_flagged= check_wip(towerdb,tw_index, tw_wip,tw_bts, tw_status, df_msa,msa_index, msa_wip)\n",
        "wip_out_tw_list\n",
        "#Tem erro no df_wip_and_bts_flagged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OTQALnSEZf0c",
        "outputId": "4713a173-ac95-4128-9137-bb41b2c48037"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site code</th>\n",
              "      <th>status</th>\n",
              "      <th>is the site a wip site</th>\n",
              "      <th>bts sites (yes/no)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2774</th>\n",
              "      <td>8485</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3445</th>\n",
              "      <td>52256</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3513</th>\n",
              "      <td>55181</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3514</th>\n",
              "      <td>55258</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3516</th>\n",
              "      <td>55476</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3523</th>\n",
              "      <td>63002</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3524</th>\n",
              "      <td>63003</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3525</th>\n",
              "      <td>63004</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3526</th>\n",
              "      <td>63005</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3527</th>\n",
              "      <td>63006</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3529</th>\n",
              "      <td>63008</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3530</th>\n",
              "      <td>63009</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3531</th>\n",
              "      <td>63010</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3533</th>\n",
              "      <td>63012</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3534</th>\n",
              "      <td>63013</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3535</th>\n",
              "      <td>63014</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3536</th>\n",
              "      <td>63015</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3538</th>\n",
              "      <td>63017</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3539</th>\n",
              "      <td>63018</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3542</th>\n",
              "      <td>63022</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3543</th>\n",
              "      <td>63025</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3544</th>\n",
              "      <td>63026</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3545</th>\n",
              "      <td>63027</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3547</th>\n",
              "      <td>63029</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3548</th>\n",
              "      <td>63030</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3549</th>\n",
              "      <td>63031</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3552</th>\n",
              "      <td>63035</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3553</th>\n",
              "      <td>63036</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3554</th>\n",
              "      <td>63037</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3556</th>\n",
              "      <td>63039</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3557</th>\n",
              "      <td>63040</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3559</th>\n",
              "      <td>63044</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3560</th>\n",
              "      <td>63045</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3562</th>\n",
              "      <td>63083</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3563</th>\n",
              "      <td>63856</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3565</th>\n",
              "      <td>67476</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3566</th>\n",
              "      <td>67540</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3567</th>\n",
              "      <td>67575</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      site code      status is the site a wip site bts sites (yes/no)\n",
              "2774       8485  In Service                    Yes                Yes\n",
              "3445      52256  In Service                    Yes                Yes\n",
              "3513      55181  In Service                    Yes                Yes\n",
              "3514      55258  In Service                    Yes                Yes\n",
              "3516      55476  In Service                    Yes                Yes\n",
              "3523      63002  In Service                    Yes                Yes\n",
              "3524      63003  In Service                    Yes                Yes\n",
              "3525      63004  In Service                    Yes                Yes\n",
              "3526      63005  In Service                    Yes                Yes\n",
              "3527      63006  In Service                    Yes                Yes\n",
              "3529      63008  In Service                    Yes                Yes\n",
              "3530      63009  In Service                    Yes                Yes\n",
              "3531      63010  In Service                    Yes                Yes\n",
              "3533      63012  In Service                    Yes                Yes\n",
              "3534      63013  In Service                    Yes                Yes\n",
              "3535      63014  In Service                    Yes                Yes\n",
              "3536      63015  In Service                    Yes                Yes\n",
              "3538      63017  In Service                    Yes                Yes\n",
              "3539      63018  In Service                    Yes                Yes\n",
              "3542      63022  In Service                    Yes                Yes\n",
              "3543      63025  In Service                    Yes                Yes\n",
              "3544      63026  In Service                    Yes                Yes\n",
              "3545      63027  In Service                    Yes                Yes\n",
              "3547      63029  In Service                    Yes                Yes\n",
              "3548      63030  In Service                    Yes                Yes\n",
              "3549      63031  In Service                    Yes                Yes\n",
              "3552      63035  In Service                    Yes                Yes\n",
              "3553      63036  In Service                    Yes                Yes\n",
              "3554      63037  In Service                    Yes                Yes\n",
              "3556      63039  In Service                    Yes                Yes\n",
              "3557      63040  In Service                    Yes                Yes\n",
              "3559      63044  In Service                    Yes                Yes\n",
              "3560      63045  In Service                    Yes                Yes\n",
              "3562      63083  In Service                    Yes                Yes\n",
              "3563      63856  In Service                    Yes                Yes\n",
              "3565      67476  In Service                    Yes                Yes\n",
              "3566      67540  In Service                    Yes                Yes\n",
              "3567      67575  In Service                    Yes                Yes"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBerUUmZhC3j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "56897a48-2ef3-49aa-c0ae-143183d77410"
      },
      "source": [
        "\"\"\"Check Bts Sites\"\"\"\n",
        "def check_bts(df_tw, bts_tw_columns, tw_index,  tw_status, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in bts_msa:\n",
        "                if i not in tw_bts_sites:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = [str(i) for i in df_msa[df_msa[bts_msa_column]=='Yes'][msa_index]]\n",
        "\n",
        "    tw_bts_sites = [str(i) for i in df_tw[df_tw[bts_tw_columns]=='Yes'][tw_index]]\n",
        "\n",
        "\n",
        "    #return of datas\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    filter = df_tw[[tw_index, tw_status, bts_tw_columns]]\n",
        "    filter[tw_index] = filter[tw_index].astype(str)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['MSA BTS Sites not BTS in month'])\n",
        "    df = pd.merge(df, filter, how='inner', left_on='MSA BTS Sites not BTS in month', right_on=tw_index)\n",
        "    return df\n",
        "#check_bts(df_tw, bts_tw_column, tw_index, df_msa, bts_msa_column, msa_index)\n",
        "df_bts_out_tw = check_bts(towerdb,tw_bts,tw_index, tw_status, df_msa,msa_bts,msa_index)\n",
        "df_bts_out_tw\n",
        "#retorna os sites novos, ou não estavam na msa anteriormente"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSA BTS Sites not BTS in month</th>\n",
              "      <th>site code</th>\n",
              "      <th>status</th>\n",
              "      <th>bts sites (yes/no)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50252</td>\n",
              "      <td>50252</td>\n",
              "      <td>WIP</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>52091</td>\n",
              "      <td>52091</td>\n",
              "      <td>In Service</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>57558</td>\n",
              "      <td>57558</td>\n",
              "      <td>WIP</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57716</td>\n",
              "      <td>57716</td>\n",
              "      <td>Pipeline</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>63858</td>\n",
              "      <td>63858</td>\n",
              "      <td>Pipeline</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>75881</td>\n",
              "      <td>75881</td>\n",
              "      <td>WIP</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  MSA BTS Sites not BTS in month site code      status bts sites (yes/no)\n",
              "0                          50252     50252         WIP                 No\n",
              "1                          52091     52091  In Service                 No\n",
              "2                          57558     57558         WIP                 No\n",
              "3                          57716     57716    Pipeline                 No\n",
              "4                          63858     63858    Pipeline                 No\n",
              "5                          75881     75881         WIP                 No"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvfcVm_dhBVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "outputId": "7c4cc5c4-1801-465f-badd-1ad83a6ec0cc"
      },
      "source": [
        "\"\"\"Decomissioned Sites Check\"\"\"\n",
        "#check_decommissioned(df,df_index, decom_col, doer_col)\n",
        "df_decom = check_decommissioned(towerdb,tw_index, tw_decom, tw_dismentled)\n",
        "df_decom\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site code</th>\n",
              "      <th>decommissioned sites(true/false)</th>\n",
              "      <th>infrastructure to be dismantled by</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [site code, decommissioned sites(true/false), infrastructure to be dismantled by]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYxDweZPg_0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "outputId": "bb4c2f83-d1c9-4822-8b30-169cdeb07aa8"
      },
      "source": [
        "\"\"\"DOER Check\"\"\"\n",
        "#check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col)\n",
        "df_doer = check_tw_bill_doer(towerdb,tw_index, tw_dismentled, tw_status, '', 'doer')\n",
        "df_doer\n",
        "# Tem Error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site code</th>\n",
              "      <th>status</th>\n",
              "      <th>infrastructure to be dismantled by</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [site code, status, infrastructure to be dismantled by]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXVFX7Hfg-rx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "outputId": "55a341a4-3cf6-43f9-b09c-104dc0ab55e2"
      },
      "source": [
        "\"\"\"M-o-M check\"\"\"\n",
        "df_mom = check_mom_bts(towerdb, tw_index, tw_bts, df_msa, msa_index, msa_bts)\n",
        "df_mom\n",
        "# No one error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site code</th>\n",
              "      <th>bts sites (yes/no)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [site code, bts sites (yes/no)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38vlL4U7g9n2"
      },
      "source": [
        "\"\"\" Check UIS and Towerdb matchs\"\"\"\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    filtered[tw_index] = filtered[tw_index].astype(str) \n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [str(i) for i in filtered[filtered[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "    print(uis_sites_not_in_towerdb)\n",
        "    if uis_sites_not_in_towerdb:\n",
        "        uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS In Month not active in TowerDB!'])\n",
        "        uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS In Month not active in TowerDB!',\\\n",
        "                                        right_on=tw_index)\n",
        "        uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS In Month not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS In Month!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS In Month!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS In Month!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    tw_decomiss = [str(i) for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "    tw_decomiss = [i for i in tw_decomiss if i in uip_sites]\n",
        "    decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "    decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                    right_on=tw_index)\n",
        "    decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "\n",
        "    #Check BTS sites\n",
        "    \"\"\"\"bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "    #uip_bts = [i for i in df_uip[df_uip['bts site applicable charge (annual)']!='']['site_id']]\n",
        "\n",
        "    if len(uip_bts)>1:\n",
        "        bts_sites_out_uip = []\n",
        "        #if not set(bts_sites).intersection(uip_sites):\n",
        "        bts_sites_out_uip = [i for i in uip_bts if i not in bts_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['UIS BTS not in TowerDB(BTS)'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='UIS BTS not in TowerDB(BTS)',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['UIS BTS not in TowerDB(BTS)', status_tw_col]]\n",
        "\n",
        "    #  Check for UIP critical sites \n",
        "    uip_critical = [i for i in df_uip[df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='']['site_id']]\n",
        "    bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "    print(len(uip_critical))\n",
        "    critical = []\n",
        "    if len(uip_critical) > 0:\n",
        "        #if set(uip_critical).intersection(bts_tw_critical):\n",
        "        critical = [i for i in uip_critical if i not in bts_tw_critical]\n",
        "        critical = pd.DataFrame(critical, columns=['UIS Sites(critical beyond 10%) different in TowerDB'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='UIS Sites(critical beyond 10%) different in TowerDB',\\\n",
        "                                    right_on=tw_index)\n",
        "        critical = critical[['UIS Sites(critical beyond 10%) different in TowerDB', status_tw_col]]\"\"\"\n",
        "    #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "    return uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip\n",
        "\n",
        "uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip = check_uip_tw(towerdb,tw_index,tw_status,tw_decom,tw_bts, tw_critical, uip, uip_sites)\n",
        "# NO one errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "2R8VsKvxM3IQ",
        "outputId": "72376314-3f81-4a0d-84c0-adeb7500a9c0"
      },
      "source": [
        "in_service_not_in_uis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TowerDB Sites out of UIS In Month!</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [TowerDB Sites out of UIS In Month!, status]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "fmC9nQkdhhpj",
        "outputId": "1a58e849-1721-4ded-bc86-80348cffcfd3"
      },
      "source": [
        "def check_diffs_v2(path_current, path_last, type_file='Excel', sheet='Commercial'):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def highlight_diff(data, color='yellow'):\n",
        "        attr = 'background-color: {}'.format(color)\n",
        "        other = data.xs('Current', axis='columns', level=-1)\n",
        "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                            index=data.index, columns=data.columns)\n",
        "    type_file = type_file.lower()\n",
        "    if type_file =='excel':\n",
        "        _actual = pd.read_excel(path_current,sheet_name=sheet).fillna('')\n",
        "\n",
        "        _before = pd.read_excel(path_last,sheet_name=sheet).fillna('')\n",
        "\n",
        "        df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "        df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "        #df_final.style.apply(highlight_diff, axis=None)\n",
        "        if not df_final.empty:\n",
        "            return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "        else:\n",
        "            print('\\nNo differences Founded!\\n')\n",
        "            \n",
        "#cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type','Input_Value_actual', 'Input_Value_before' ,'Description/Instruction', 'Frequency of Update']\n",
        "df_com = check_diffs_v2( '/content/UserInput_Portugal_20210831.xlsx', '/content/UserInput_Portugal_20210731.xlsx')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col9{\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" colspan=2>Sub_charge_Type</th>        <th class=\"col_heading level0 col2\" colspan=2>Param1</th>        <th class=\"col_heading level0 col4\" colspan=2>Param2</th>        <th class=\"col_heading level0 col6\" colspan=2>Data_Type</th>        <th class=\"col_heading level0 col8\" colspan=2>Input_Value</th>        <th class=\"col_heading level0 col10\" colspan=2>Description/Instruction</th>        <th class=\"col_heading level0 col12\" colspan=2>Frequency of Update</th>    </tr>    <tr>        <th class=\"blank level1\" ></th>        <th class=\"col_heading level1 col0\" >Current</th>        <th class=\"col_heading level1 col1\" >Last</th>        <th class=\"col_heading level1 col2\" >Current</th>        <th class=\"col_heading level1 col3\" >Last</th>        <th class=\"col_heading level1 col4\" >Current</th>        <th class=\"col_heading level1 col5\" >Last</th>        <th class=\"col_heading level1 col6\" >Current</th>        <th class=\"col_heading level1 col7\" >Last</th>        <th class=\"col_heading level1 col8\" >Current</th>        <th class=\"col_heading level1 col9\" >Last</th>        <th class=\"col_heading level1 col10\" >Current</th>        <th class=\"col_heading level1 col11\" >Last</th>        <th class=\"col_heading level1 col12\" >Current</th>        <th class=\"col_heading level1 col13\" >Last</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col0\" class=\"data row0 col0\" >LEGACY BASE SERVICE CHARGE</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col1\" class=\"data row0 col1\" >LEGACY BASE SERVICE CHARGE</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col2\" class=\"data row0 col2\" >PORTFOLIO FEE</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col3\" class=\"data row0 col3\" >PORTFOLIO FEE</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col4\" class=\"data row0 col4\" ></td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col5\" class=\"data row0 col5\" ></td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col6\" class=\"data row0 col6\" >NUMBER</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col7\" class=\"data row0 col7\" >NUMBER</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col8\" class=\"data row0 col8\" >48993585</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col9\" class=\"data row0 col9\" >49086000</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col10\" class=\"data row0 col10\" >- Annual value as per the MSA\n",
              "- To be edited in case of changes in the MSA/Side Letter \n",
              "- Please enter an Annual value</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col11\" class=\"data row0 col11\" >- Annual value as per the MSA\n",
              "- To be edited in case of changes in the MSA/Side Letter \n",
              "- Please enter an Annual value</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col12\" class=\"data row0 col12\" >Only in case of change in MSA/Side letter</td>\n",
              "                        <td id=\"T_0b75666e_fa86_11eb_902b_0242ac1c0002row0_col13\" class=\"data row0 col13\" >Only in case of change in MSA/Side letter</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f333c611450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OmH_Lp6NZaf"
      },
      "source": [
        "df_list = [ dftw_dates_errors, df_picklist,df_wip_and_bts_flagged, df_com]\n",
        "sheetnames = ['In service sites Dates Errors','Picklist Invalid Value', 'BTS and WIP columns Flagged', 'Commercial Values Differences']\n",
        "\n",
        "path = '/content/towerdb_pt_errors.xlsx'\n",
        "general_log_erros(df_list, sheetnames, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzc049-Bw9-5"
      },
      "source": [
        "UIS Comparison(Old vs New)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqvZapziw9nI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee6780b-ad3a-47cf-c918-e48bc8c8d89e"
      },
      "source": [
        "uis_new = '/content/UserInput_Portugal_20210831.xlsx'\n",
        "uis_old = '/content/UserInput_Portugal_20210731.xlsx'\n",
        "sheet = 'SiteLevel'\n",
        "uis_index = 'Site_ID (Numeric)'\n",
        "to_uis = '/content/PT_UIS_'\n",
        "old_uis = 'UIS_20210731.xlsx'\n",
        "new_uis = 'UIS_20210831.xlsx'\n",
        "bill = []\n",
        "find_diffs_between_files(uis_old, uis_new, uis_index, bill, to_uis, old_uis,new_uis,'excel',status_col='',\\\n",
        "                         kind='',kind_col='', sheetname=sheet, skipr=2, skipc=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['4350']\n",
            "Dropped Rows: []\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "476Zzq29-i3L"
      },
      "source": [
        "TA Input Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04oy2k1jJtuY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "8faad2ff-58ca-4136-a36f-0c56127037f7"
      },
      "source": [
        "# Lendo o Ficheiro de input\n",
        "path='/content/Vantage Towers_PT TowerDB Jun21_20210720.xlsx'\n",
        "#test = '/content/teste_dates.xlsx'\n",
        "sheet_ta = 'Tenant Agreements'\n",
        "skiprows_ta = 13\n",
        "skipcol = 1\n",
        "ta = read_files(path, sheet_ta, skiprows_ta, skipcol, \"VF Site code\")     \n",
        "ta_output_cols = [\"VF Site code\",\"VF Site name\",\"Other operator Site code\",\"Other operator Site name\",\\\n",
        "                    \"OWNER\",\"TENANT\",\"Tenant classification\",\"Type of sharing\",'Annual Hosting Fee (in €)',\\\n",
        "                    \"M1\",\"M2\",\"M3\",\"M4\",\"M5\",\"M6\",\"M7\",\"M8\",\"M9\",\"M10\",\"M11\",\"M12\",\"Annual Energy Fee (€)\",\\\n",
        "                    \"Annual Maintenance Fee (€)\",\"Annual Other Services Fee (€)\",\"Total Annual Fee (€)\",\"Authorisation Date\",\\\n",
        "                    \"Fee Starting Date\",\"Fee Expiring Date\",\"Renewal Option\",\"Expiring date after renewal\",\"Termination Date\",\\\n",
        "                    \"Terms of payments\",\"Payment type\",\"Index\",\"Indexation Driver\",\"Percentage\",\"VAT Subject\",\"Percentage (VAT)\",\\\n",
        "                    \"Inputs from filling responsible\",\"Tenancy duration until expiring date (in years)\",\"Sharing In Scope\",\"CONTINENT\"]\n",
        "\n",
        "print(len(ta_output_cols))\n",
        "ta['Tenancy duration until expiring date (in years)'] = ['' for i in range(ta.shape[0])]\n",
        "ta[\"CONTINENT\"] = ['' for i in range(ta.shape[0])]\n",
        "ta = ta[ta_output_cols]\n",
        "\n",
        "dates_tw = ['Authorisation Date', 'Fee Starting Date', 'Fee Expiring Date']\n",
        "for i in dates_tw:\n",
        "    ta[i] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in ta[i]]\n",
        "\n",
        "ta = ta.fillna('')\n",
        "\n",
        "ta.to_csv('/content/TA_Input_Portugal_20210831.csv', index=False)\n",
        "ta.head(2)\n",
        "  "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VF Site code</th>\n",
              "      <th>VF Site name</th>\n",
              "      <th>Other operator Site code</th>\n",
              "      <th>Other operator Site name</th>\n",
              "      <th>OWNER</th>\n",
              "      <th>TENANT</th>\n",
              "      <th>Tenant classification</th>\n",
              "      <th>Type of sharing</th>\n",
              "      <th>Annual Hosting Fee (in €)</th>\n",
              "      <th>M1</th>\n",
              "      <th>M2</th>\n",
              "      <th>M3</th>\n",
              "      <th>M4</th>\n",
              "      <th>M5</th>\n",
              "      <th>M6</th>\n",
              "      <th>M7</th>\n",
              "      <th>M8</th>\n",
              "      <th>M9</th>\n",
              "      <th>M10</th>\n",
              "      <th>M11</th>\n",
              "      <th>M12</th>\n",
              "      <th>Annual Energy Fee (€)</th>\n",
              "      <th>Annual Maintenance Fee (€)</th>\n",
              "      <th>Annual Other Services Fee (€)</th>\n",
              "      <th>Total Annual Fee (€)</th>\n",
              "      <th>Authorisation Date</th>\n",
              "      <th>Fee Starting Date</th>\n",
              "      <th>Fee Expiring Date</th>\n",
              "      <th>Renewal Option</th>\n",
              "      <th>Expiring date after renewal</th>\n",
              "      <th>Termination Date</th>\n",
              "      <th>Terms of payments</th>\n",
              "      <th>Payment type</th>\n",
              "      <th>Index</th>\n",
              "      <th>Indexation Driver</th>\n",
              "      <th>Percentage</th>\n",
              "      <th>VAT Subject</th>\n",
              "      <th>Percentage (VAT)</th>\n",
              "      <th>Inputs from filling responsible</th>\n",
              "      <th>Tenancy duration until expiring date (in years)</th>\n",
              "      <th>Sharing In Scope</th>\n",
              "      <th>CONTINENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>21</td>\n",
              "      <td>AVEIRO</td>\n",
              "      <td>99BL043</td>\n",
              "      <td>AVEIRO ESTE</td>\n",
              "      <td>VF</td>\n",
              "      <td>MEO</td>\n",
              "      <td>MNO</td>\n",
              "      <td>TOWER Sharing</td>\n",
              "      <td>2693.52</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2693.52</td>\n",
              "      <td>01/03/1999</td>\n",
              "      <td>01/03/1999</td>\n",
              "      <td></td>\n",
              "      <td>NO</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>EoP Deferred</td>\n",
              "      <td>Monthly</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>YES</td>\n",
              "      <td>23%</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Yes</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>26</td>\n",
              "      <td>SAO MIGUEL DE MACHEDE</td>\n",
              "      <td>01AL016</td>\n",
              "      <td>S MIGUEL MACHEDE</td>\n",
              "      <td>VF</td>\n",
              "      <td>MEO</td>\n",
              "      <td>MNO</td>\n",
              "      <td>TOWER Sharing</td>\n",
              "      <td>2693.52</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>224.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2693.52</td>\n",
              "      <td>01/06/2001</td>\n",
              "      <td>01/06/2001</td>\n",
              "      <td></td>\n",
              "      <td>NO</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>EoP Deferred</td>\n",
              "      <td>Monthly</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>YES</td>\n",
              "      <td>23%</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Yes</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   VF Site code           VF Site name  ... Sharing In Scope CONTINENT\n",
              "0            21                 AVEIRO  ...              Yes          \n",
              "1            26  SAO MIGUEL DE MACHEDE  ...              Yes          \n",
              "\n",
              "[2 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEtz1JOd5kp9",
        "outputId": "d9968056-ac56-4ac7-fde8-80c2f7d2af8c"
      },
      "source": [
        "ta[\"Percentage (VAT)\"].value_counts()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23%    748\n",
              "         3\n",
              "Name: Percentage (VAT), dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_VFqtArLsB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a1e14e-6440-4e3a-88cc-80b4c5aef75a"
      },
      "source": [
        "df_ta_dates = check_lc_ta_dates(ta,\"VF Site code\", 'Fee Starting Date',\"Fee Expiring Date\")\n",
        "df_ta_dates\n",
        "#No erros"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [VF Site code, Fee Starting Date, Fee Expiring Date]\n",
            "Index: []\n",
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_IU5_ZJlMxt"
      },
      "source": [
        "Comparison between oldest file and newest file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYB1TBjBqHc6",
        "outputId": "aa7bb6fa-bb93-42f8-90a4-1b5ed2a2b8fe"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col]\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old,  engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new).fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "\n",
        "# Lendo o Ficheiro de input\n",
        "path='/content/Vantage Towers_PT TowerDB Jun21_20210720.xlsx'\n",
        "#test = '/content/teste_dates.xlsx'\n",
        "skiprows_ta = 13\n",
        "skipcol = 1\n",
        "\n",
        "path_old_ta = '/content/TA_Input_Portugal_20210731.csv'\n",
        "sheet_ta = 'Tenant Agreements'\n",
        "\n",
        "ta_save = '/content/PT_TA'\n",
        "old_ta = 'TA_Input_Portugal_20210630.csv'\n",
        "new_ta = \"TA_Input_Portugal_20210731.csv\"\n",
        "ta_cols = ['VF Site code', 'TENANT', 'Tenant classification','Authorisation Date', 'Fee Starting Date',\"Fee Expiring Date\",\"Sharing In Scope\"]\n",
        "\n",
        "\"\"\"path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0)\"\"\"\n",
        "find_diffs_between_files(path_old_ta, path, 'VF Site code', ta_cols, ta_save, old_ta, new_ta,'mix', '', 'ta', 'TENANT', \n",
        "                         sheetname=sheet_ta, skipr=13, skipc=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     []\n",
            "Dropped Rows: []\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}