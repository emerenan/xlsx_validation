{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ie_validations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN11zAjD0/MWKzHf90kveWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emerenan/xlsx_validation/blob/main/ie_validations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8It8h3GcBwr_"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index):\n",
        "    \"\"\"\n",
        "    Params:\\n\n",
        "    path: parth of file in the computer.\\n\n",
        "    n_skiprows: Number of rows to delete in the original file,.\\n\n",
        "    columns_to_convert: Columns to convert the data general type. \\n\n",
        "    n_skipcolumn: Columns to skip in the original file. \\n\n",
        "    endrow = pass 0 to read everything, 1 to count entire\n",
        "    columns_order: List of columns names in specific order to pass in the engine.\\n\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    \"\"\"cont = 0\n",
        "    for i in df.iloc[:,site_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[:cont, :]\n",
        "    #df.columns = columns_order\"\"\"\n",
        "    \n",
        "    # convert intery columns to integer \n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].fillna(0)\n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].astype('int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def check_df(df, error_msg):\n",
        "    if df == None or df.empty:\n",
        "        return f'{error_msg}'\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def count_duplicates(lista):\n",
        "    count_dict = {}\n",
        "    for entry in lista:\n",
        "        if entry in count_dict.keys():\n",
        "            count_dict[entry] += 1\n",
        "        else:\n",
        "            count_dict[entry] = 1\n",
        "    \n",
        "    duplicates = {}\n",
        "    for k, v in count_dict.items():\n",
        "        if v > 1:\n",
        "            duplicates[k] = v\n",
        "    return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "def defining_df(df, column_range, number_col):\n",
        "    cont = 0\n",
        "    for i in df.iloc[:,number_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[0:cont, :]\n",
        "    return df\n",
        "\n",
        "#old version\n",
        "def check_columns(table, output_columns):\n",
        "    \"\"\"\n",
        "    Check the total of number of missing columns and the missing columns in passed table.\\n\n",
        "\n",
        "    Params:\\n\n",
        "    table: contain the columns to be check.\\n\n",
        "    output_columns: columns structure at the final file. \n",
        "\n",
        "    Returns:\\n\n",
        "    Number of missing columns and a list that contains the name os missing columns.\n",
        "    \"\"\"\n",
        "    \"\"\"\"\n",
        "    countries = ['DE':{'towerDB':[],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':[]}\\,\n",
        "                 'HU',\n",
        "                 'IE',\n",
        "                 'RO',\n",
        "                 'PT',\n",
        "                 'ES',\n",
        "                 'CZ': {'towerDB':[\"Code (Duplicate)\",\"Site Status\",\"VF - In scope / out of scope (Generalised scoping)\",\"Site in Skylon scope Actual (From Site List Sheet )\",\"Legacy Site Code(Duplicate)\",\"TIMS Site Code\",\"Legacy Site Code\",\"Site Name\",\"Macro Region\",\"Province\",\"Municipality\",\"Inhabitants\",\"Address\",\"Ground Register\",\"Altitude\",\"Latitude\",\"Longitude\",\"Categorization by Inhabitants\",\"Categorization by Transmission Sys\",\"Categorization by Site Type\",\"Categorization by Transmission Sys (subcluster)\",\"Other internal Categorization 1 (Identify ACQ Sites)\",\"Other internal Categorization 2 Energy provider (Eon/ LL)\",\"DAS+Macro\",\"DAS (Yes/ No)\",\"DAS Ownership (Complete/ Partial/ 3rd Party)\",\"Active/ Passive DAS\",\"# of remote units/ radiating points\",\"Type of Structure\",\"Distance highest antenna to ground level\",\"GBT Tower height\",\"POD ID\",\"Energy Consumption LTM (kwh)\",\"Annual Energy cost LTM (Euros)\",\"Infrastructure ready (existing)/ to be ready (new)\",\"Infrastructure to be dismantled by\",\"Radio equipments to be deactivated by\",\"Infrastructure to be shared by\",\"Technology VOD\",\"Fibre / Microwave\",\"Vertical passive structure owner\",\"Room configuration (detailed)\",\"Shelter passive structure ownership\",\"Type of Air Conditioning\",\"Number of cabinets (Full Capacity)\",\"Number of Antenna (Full Capacity)\",\"Number of MW (Full Capacity)\",\"Counterpart\",\"# of Lease Contracts\",\"Current annual lease fees \",\"Current other fees (Maintenance)\",\"Current other fees\",\"(Average) residual duration - Lease contract\",\"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\"# of Tenants Agreements\",\"Current Total Annual Hosting Fees\",\"Tenant (name/ID) MNO1 (Česká telekomunikační infrastruktura a.s.)\",\"Annual Fee per Tenant MNO1\",\"Annual Energy Fee MNO1\",\"Annual Maintenance Fee MNO1\",\"Other Services Fee MNO1\",\"Residual duration MNO1 (Years)\",\"Tenant (name/ID) MNO2 (T-Mobile Czech Republic a.s.)\",\"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\"Annual Maintenance Fee MNO2\",\"Other Services Fee MNO2\",\"Residual duration MNO2 (days)\",\"Tenant (name/ID) MNO3\",\"Annual Fee per Tenant MNO3\",\"Annual Energy Fee MNO3\",\"Annual Maintenance Fee MNO3\",\"Other Services Fee MNO3\",\"Residual duration MNO3\",\"# of OTMOs\",\"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\"Annual Maintenance Fee OTMOs\",\"Other Services Fee OTMOs\",\"Average residual duration (days)\",\"Check\",\"Strategic Macro Sites\",\"Critical Sites\",\"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\"Transmission Sites\",\"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\"VF-CZ Demerger phase\",\"EVO Location [FAR Site ID] \",\"Billing Trigger date \",\\\n",
        "                 \"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\"Wip_Site\",\"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', 'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', 'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index','Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', 'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', 'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month','Updated Item', 'Comment', 'Counterpart_extra_1', 'Counterpart_extra_2', 'x','SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count', 'not_def_1']}\\,\n",
        "                 }\n",
        "                 'GR']\n",
        "                 \"\"\"\n",
        "    total_received = len(table.columns)\n",
        "    number_missing_columns = 0\n",
        "    missing_columns = []\n",
        "    #Counting of missing columns       \n",
        "    #if country in contries:      \n",
        "    for columns in output_columns:\n",
        "        if columns.lower() not in [labels.lower() for labels in table]:\n",
        "            number_missing_columns +=1\n",
        "            missing_columns.append(columns)\n",
        "    \n",
        "    return total_received, number_missing_columns, missing_columns\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", 0, '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "      \n",
        "def date_parser(df, columns, format, type_dates):\n",
        "    t_col = type_dates.lower()\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Refactorar esse codigo para receber todas as colunas num dic\n",
        "# Sendo as keys=columns e values= picklist for each column\n",
        "def check_date_columns(df, df_index, columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                if date_format.match(df_dates.loc[de_site,de_column]) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "                    else:\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            return df_de\n",
        "        else: \n",
        "            print('No one columns with incorrect date format!')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                if date_format.match(df_dates.loc[de_site,de_column]) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "                    else:\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            return df_de\n",
        "        else: \n",
        "            print('No one columns with incorrect date format!')\n",
        "\n",
        "def check_amounts(df_check, df_index, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "        \n",
        "def check_picklist(df,df_index, df_status, df_cols, picklist_dict):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        df:\n",
        "        df_index: \n",
        "        picklist_dict:\n",
        "        nultiIndex: More than one column to check\n",
        "    \"\"\"\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        columns = [i for i in picklist_dict.keys()]\n",
        "        for column in set(columns): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            #print(value)\n",
        "            if not value in set(picklist_dict[column]) or pd.isnull(value):\n",
        "                #print(set(picklist_dict[column]))\n",
        "\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "                else:\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "\n",
        "    #df = df_errors.dropna()   \n",
        "    df_errors = df_errors.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    if len(df_errors)>0:\n",
        "        df = df[[df_index, df_status]]\n",
        "        df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "        df_errors = df_errors.reset_index()\n",
        "    else:\n",
        "        print('\\nNo one Picklist Error Founded!')\n",
        "    return df_errors\n",
        "\n",
        "def check_picklist_3(df,df_index, picklist_dict):\n",
        "    log = {}\n",
        "    for column in picklist_dict:\n",
        "        df_aux = df.copy()\n",
        "        new = df_aux[column].isin(picklist_dict[column])\n",
        "        #new = df_aux[column].apply(lambda x: x in picklist_dict[column])\n",
        "        # Aceita somento os valores que não estão na picklist\n",
        "        indexes = df.index[new == False].tolist()\n",
        "        #if len(indexes)>0:\n",
        "        if column not in log:log[column]=[]\n",
        "        log[column]=log[column]+indexes\n",
        "    #print(log.keys())\n",
        "\n",
        "    newDict ={}\n",
        "    df1 = pd.DataFrame()\n",
        "    for key,value in log.items():\n",
        "        for val in value:  \n",
        "            ID=df.iloc[val][df_index]\n",
        "            if ID in newDict:\n",
        "                newDict[ID].append(key)\n",
        "            else:\n",
        "                newDict[ID] = [key]\n",
        "        \n",
        "    logs = pd.DataFrame.from_dict(newDict, orient='index')\n",
        "    return logs\n",
        "\n",
        "#check on air foi trocado pelo check_picklist\n",
        "def check_on_air_sites(df, df_index, df_cols):\n",
        "\n",
        "    df_check = df[df_cols]\n",
        "    df_check['sites'] = df_check[df_index]\n",
        "    df_check = df_check.set_index('sites')\n",
        "    #df_check = replace_values(df_check, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in df_cols: \n",
        "            if df_check.loc[site,df_index] not in df_errors.index:\n",
        "                df_errors.loc[site,df_index] = df_check.loc[site,df_index]\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "            else:\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    df = df_errors[df_errors.iloc[:,1:]]\n",
        "    df = df.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    #df = df_errors.dropna(how='all', axis=1)   \n",
        "    return df\n",
        "\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [str(i) for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uip = [str(i) for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index].sort_values() if i not in uip_list]\n",
        "    bts_out_uip = pd.DataFrame(bts_out_uip, columns=['Bts_Sites_Out_UIP_File'])\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "\n",
        "    return new_sites, bts_out_uip, df_site_bts[[tw_index, bts_col, bill_col]]\n",
        "    \"\"\"Restruturar o script em CZ, DE, PT\"\"\"\n",
        "\n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    return df\n",
        "\n",
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[wip_tw]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, wip_tw, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "    \"\"\"Reestrurar os script em PT, DE, CZ\"\"\"\n",
        "    # Falta os outros países\n",
        "\n",
        "def check_decommissioned(df,df_index, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    return filtered[[df_index, decom_col, doer_col]]\n",
        "    \"\"\"Ajustar para CZ, DE, PT\"\"\"\n",
        "    \n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]') < current_date)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "        else:\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]').empty)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "\n",
        "\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col, status_col):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)]\n",
        "        return filtered[[tw_index, status_col, doer_col]]  \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "                                                              \n",
        "def check_mom_bts(df_tw, tw_index, tw_col, df_msa,msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    return filtered[[tw_index, tw_col]]         \n",
        "\n",
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "\n",
        "        filtered = df[df[start_date] > df[end_date]]\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "    t1 = ['pt', 'de', 'cz', 'ie', 'es', 'ro', 'hu']\n",
        "    if country.lower() in t1:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        if decom_col != \"\":\n",
        "            tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "            decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "\n",
        "        \n",
        "            return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "        else:\n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "            return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "        decomiss_sites_in_uip = []\n",
        "        if set(tw_decomiss).intersection(uip_sites):\n",
        "            decomiss_sites_in_uip = [i for i in tw_decomiss if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(decomiss_sites_in_uip, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip = [i for i in df_uip['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Yes'][tw_index]\n",
        "        critical = []\n",
        "        if set(uip).intersection(bts_tw_critical):\n",
        "            critical = [i for i in bts_tw_critical if i not in uip]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% out in UIP File'])\n",
        "\n",
        "        return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    return df_commercial_diffs\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() \n",
        "\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = fit_df(path_OLD,bill_cols)\n",
        "        df_NEW = fit_df(path_NEW,bill_cols)\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save}_old_file vs new_file.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve76wA2Kr7vW"
      },
      "source": [
        "tw_bills = ['Site ID ',\\\n",
        "            'Strategic',\\\n",
        "            'Critical',\\\n",
        "            'Site Type (See structure Type)',\\\n",
        "            'Core Site Type',\\\n",
        "            'Transmission System',\\\n",
        "            'Macro Site - Transmission Hub Site',\\\n",
        "            'Macro Site - Transmission Hub Site with/without Shelters',\\\n",
        "            'Transmission Sites',\\\n",
        "            'Room Configuration',\\\n",
        "            'Power Supply',\\\n",
        "            'Air Conditioning',\\\n",
        "            'Site Status',\\\n",
        "            'Strategic_Site_Bucket',\\\n",
        "            'CriticalSite_Beyond_10',\\\n",
        "            'Sites_As_Metered_Estimated',\\\n",
        "            'Meter_Sharing_Site',\\\n",
        "            'Bts_Sites',\\\n",
        "            'Phase1_Site_Or_Consent_Site',\\\n",
        "            'Transfer_Date_Of_Consent_Site',\\\n",
        "            'WIP_Sites',\\\n",
        "            'Active_Sharing_Arrangement',\\\n",
        "            'Subsequent_Sharing_Arrangement',\\\n",
        "            'First_Active_Sharing_Deployment_Type',\\\n",
        "            'First_Active_Sharing_Start_Date',\\\n",
        "            'First_Active_Sharing_End_Date']\n",
        "pathtw='/content/TowerDB_Ireland_20210531 1.xlsx'\n",
        "pathold = '/content/TowerDB_Ireland_20210630.csv'\n",
        "sheet='TowerDB_Input_Ireland_20210531'\n",
        "skiprows=0\n",
        "skipcols=0\n",
        "to_save = '/content/IE_TW'\n",
        "find_diffs_between_files(pathold, pathtw, 'Site ID ', tw_bills,'Site Status', to_save,'mix', sheetname=sheet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP-OGcatTK_H"
      },
      "source": [
        "Adding new columns for UIP\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQzS0jH-TLeZ"
      },
      "source": [
        "# Lendo o Ficheiro de input\n",
        "path_uip=''\n",
        "sheet = 'SiteLevel'\n",
        "site_level = pd.read_excel(path_uip,sheet, header=[0,1,2])\n",
        "head_1 = pd.MultiIndex.from_product([[''],['Numeric (in months)'],['Delay in Site Modification Projects', 'Delay in BTS Projects']])\n",
        "df_1 = pd.DataFrame(columns=head_1)\n",
        "\n",
        "head_2 = pd.MultiIndex.from_product([[''],['Numeric (in €)'],['Excess of Upgrade Capital Expenditure over Threshold']])\n",
        "df_2 = pd.DataFrame(columns=head_2)\n",
        "\n",
        "site_level = pd.concat([site_level, df_1, df_2], axis=1)\n",
        "site_level.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyBxhhVnhluV"
      },
      "source": [
        "Towerdb File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agg4jhh_OgNI"
      },
      "source": [
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index, col_dates, bill_col, format='mix', type_date=\"%d/%m/%Y\"):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\",0, '-', '_', np.nan,'nan', ' ']\n",
        "        #lista = []\n",
        "\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df\n",
        "\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                #df[column] = df[column].fillna('')\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                #df[column] = df[column].fillna('')\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, engine='openpyxl', skiprows = n_skiprows).fillna('')\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    #df = df.rename(columns={'Billing Trigger Date': 'Billing Trigger Date2'})\n",
        "    df = replace_values(df,col_dates)\n",
        "    date_parser(df, col_dates, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = df.replace(['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' '], '')\n",
        "\n",
        "    \"\"\"for col in cols_number:\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f'{x:,}')\"\"\"\n",
        "    \n",
        "    df = df.fillna('')\n",
        "\n",
        "    return df\n",
        "    \n",
        "\"\"\" Need Unlock spreadsheet before\"\"\"\n",
        "tw_cols = [\"Site ID \",\"Population\",\"County\",\"Site Name \",\"Address \",\"Site Provider\",\"Strategic\",\"Critical\",\\\n",
        "           \"Altitude (ASL)\",\"Lat (WGS 84)\",\"Long (WGS 84)\",\"Site Class \",\"Site Owner\",\"Hub List \",\\\n",
        "           \"Site Type (See structure Type)\",\"Active Dishes\",\"INTP \",\"Core Site Type\",\"Transmission System\",\\\n",
        "           \"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\\\n",
        "           \"Transmission Sites\",\"Public DAS Sites (Active/ Passive)\",\"Fibre/Microwave \",\"Technology VOD\",\\\n",
        "           \"Structure Type \",\"Structure Type (Standardised)\",\"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\\\n",
        "           \"Tower Height \",\"Site Area (sqm)\",\"Site Area (Available) \",\"Macro Region\",\"MPRN\",\"Site Decom\",\\\n",
        "           \"SAP Site Code\",\"Energy Cost FY1819\",\"Active / Not Active\",\"New Build planned \",\"Site Status\",\\\n",
        "           \"Planned Sublets \",\"Infrastructure Ready Date\",\"Strategic_Site_Bucket\",\"CriticalSite_Beyond_10\",\\\n",
        "           \"Sites_As_Metered_Estimated\",\"Meter_Sharing_Site\",\"Bts_Sites\",\"Phase1_Site_Or_Consent_Site\",\\\n",
        "           \"Transfer_Date_Of_Consent_Site\",\"Billing Trigger Date\",\"WIP_Sites\",\"Decommisioned_Site\",\\\n",
        "           \"Active_Sharing_Arrangement\",\"Subsequent_Sharing_Arrangement\",\"First_Active_Sharing_Deployment_Type\",\\\n",
        "           \"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"BTS Site Acceptance Date\",\\\n",
        "           \"Billing Trigger Date2\",\"Date of Equipment Removal\"]\n",
        "\n",
        "bill_cols = ['Site ID ','Population','County','Site Name ','Address ','Site Provider','Strategic','Critical','Altitude (ASL)','Lat (WGS 84)','Long (WGS 84)',\\\n",
        "             'Site Class ','Site Owner','Hub List ','Site Type (See structure Type)','Active Dishes','INTP ','Core Site Type','Transmission System',\\\n",
        "             'Macro Site - Transmission Hub Site','Macro Site - Transmission Hub Site with/without Shelters','Transmission Sites','Public DAS Sites (Active/ Passive)',\\\n",
        "             'Fibre/Microwave ','Technology VOD','Structure Type ','Structure Type (Standardised)','Room Configuration','Power Supply','Air Conditioning',\\\n",
        "             'Tower Height ','Site Area (sqm)','Site Area (Available) ','Macro Region','MPRN','Energy Consumption FY1819 (KWh)','Energy Consumption FYTD (KWh)',\\\n",
        "             'Energy Cost FY1819','Energy Cost FYTD','New Build planned ','Site Status','Planned Sublets ','Infrastructure Ready Date','Strategic_Site_Bucket',\\\n",
        "             'CriticalSite_Beyond_10','Sites_As_Metered_Estimated','Meter_Sharing_Site','Bts_Sites','Phase1_Site_Or_Consent_Site','Transfer_Date_Of_Consent_Site',\\\n",
        "             'Billing Trigger Date','WIP_Sites','Decommisioned_Site','Active_Sharing_Arrangement','Subsequent_Sharing_Arrangement','First_Active_Sharing_Deployment_Type',\\\n",
        "             'First_Active_Sharing_Start_Date','First_Active_Sharing_End_Date','BTS Site Acceptance Date','Billing Trigger Date','Date of Equipment Removal']\n",
        "\n",
        "path='/content/TowerDB_Ireland_20210630 v1.xlsx'\n",
        "sheet='TowerDB_Input_Ireland_20210630'\n",
        "skiprows=0\n",
        "skipcols=0\n",
        "\"\"\"A coluna Infrastructure Ready Date contem 0 e deve ser feito replace\"\"\"\n",
        "dates_tw = ['Infrastructure Ready Date']\n",
        "#(path, sheetname, n_skiprows, n_skip_columns, site_index, cols_number, col_dates,\n",
        "towerdb = read_files(path, sheet, skiprows, skipcols, 'Site ID ', dates_tw, bill_cols)\n",
        "#\n",
        "#towerdb['Billing Trigger Date'] = towerdb['Infrastructure Ready Date']\n",
        "\n",
        "# Read CZ towerdb files\n",
        "\"\"\"Merge to get DOER column\"\"\"\n",
        "msa = pd.read_csv('/content/TowerDB_Ireland_20210731.csv', engine='python')\n",
        "doer_col = msa[['Site ID ', 'Date of Equipment Removal']]\n",
        "towerdb = towerdb.merge(doer_col, on='Site ID ')\n",
        "\n",
        "towerdb.rename(columns={'Billing Trigger Date': 'Billing Trigger Date2'}, inplace=True)\n",
        "towerdb['Billing Trigger Date'] = towerdb['Infrastructure Ready Date'].values\n",
        "towerdb = towerdb[tw_cols]\n",
        "towerdb.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRy770oTk7BJ"
      },
      "source": [
        "towerdb.to_csv('/content/TowerDB_Ireland_20210831.csv', index=False)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vNkIMdNRO2i",
        "outputId": "2dfd02be-e6c4-4728-a61c-4e6468e17705"
      },
      "source": [
        "\"\"\"Check columns received\"\"\"\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = list(df.columns)\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "\n",
        "    if col_miss:\n",
        "        df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'])\n",
        "        return df_col_missing\n",
        "    else:\n",
        "        print('\\nNo missing columns.')\n",
        "\n",
        "df_col_missing = check_columns_received(towerdb, tw_cols)\n",
        "df_col_missing\n",
        "#Tem error 3 columns faltando"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No missing columns.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "cPoEM_j5QtEb",
        "outputId": "63654fe3-bf2d-4374-aeae-1d4f22b34e97"
      },
      "source": [
        "picklist = {\n",
        "    \"Strategic\": ['Yes', 'No'],\n",
        "    'Critical': ['Yes', 'No'],\n",
        "    'Site Type (See structure Type)': [\"DAS passive\",\"GBT\",\"RTT\"],\n",
        "    'INTP ': [\"Standard\",\"Core\",\"Transmission\"],\n",
        "    'Core Site Type': ['Non Core', 'Case A', 'Case B'],\n",
        "    'Transmission System': [\"Macro\",\"Public DAS\",\"Long-term Mobile\",\"Transmission\"],\n",
        "    'Macro Site - Transmission Hub Site': ['Yes', 'No'],\n",
        "    'Macro Site - Transmission Hub Site with/without Shelters': [\"With shelters\",\"Without shelters\",\"Non Transmission Hub Site\"],\n",
        "    'Room Configuration': ['Indoor','Outdoor', \"KN type Outdoor\",\"RBS type Outdoor\"],\n",
        "    'Power Supply': ['AC','DC', 'AC & DC','No Power'],\n",
        "    'Air Conditioning': ['No','Yes; Indoor Air Conditioning','Yes; Indoor Free Air cooling / Free cooling units'],\n",
        "    'Site Status': ['In Service','Ordered','Pipeline','Planned','To be dismantled'],\n",
        "    'Strategic_Site_Bucket': ['Yes - 0-5%','Yes - 5-10%','Non Strategic'],\n",
        "    'CriticalSite_Beyond_10': ['Beyond 10%','Within 10%','Non Critical'],\n",
        "    'Sites_As_Metered_Estimated': ['Estimated Model','Metered Model'],\n",
        "    'Meter_Sharing_Site': ['Yes', 'No'],\n",
        "    \"Bts_Sites\": ['Yes', 'No'],\n",
        "    'Phase1_Site_Or_Consent_Site': ['MSA (Phase 1)','PMA (Phase 2)'],\n",
        "    'WIP_Sites': ['Yes', 'No'],\n",
        "    'Active_Sharing_Arrangement': ['MORAN (On VF equipment)','MORAN (On non-VF equipment)','MOCN with Spectrum Pooling','Partial Active-Passive','No Active Sharing'],\n",
        "    'Subsequent_Sharing_Arrangement': ['Yes', 'No']\n",
        "\n",
        "}\n",
        "\"\"\"Picklist check\"\"\"\n",
        "actives = towerdb[towerdb['Site Status']=='In Service']\n",
        "col_pick =['Site ID ',\"Strategic\",'Critical','Site Type (See structure Type)','INTP ','Core Site Type','Transmission System',\\\n",
        "    'Macro Site - Transmission Hub Site','Macro Site - Transmission Hub Site with/without Shelters',\\\n",
        "    'Room Configuration','Power Supply','Air Conditioning','Site Status','Strategic_Site_Bucket','CriticalSite_Beyond_10',\\\n",
        "    'Sites_As_Metered_Estimated','Meter_Sharing_Site',\"Bts_Sites\",'Phase1_Site_Or_Consent_Site','WIP_Sites','Active_Sharing_Arrangement',\\\n",
        "    'Subsequent_Sharing_Arrangement']\n",
        "\"\"\"Picklist check\"\"\"\n",
        "actives = towerdb[towerdb['Site Status']=='In Service']\n",
        "df_picklist = check_picklist(actives, 'Site ID ','Site Status', col_pick, picklist)\n",
        "df_picklist"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:248: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Site ID</th>\n",
              "      <th>Site Status</th>\n",
              "      <th>Subsequent_Sharing_Arrangement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WH032</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DN304</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DN947</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DX252</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Site ID  Site Status Subsequent_Sharing_Arrangement\n",
              "0    WH032  In Service                    Blank Value\n",
              "1    DN304  In Service                    Blank Value\n",
              "2    DN947  In Service                    Blank Value\n",
              "3    DX252  In Service                    Blank Value"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "uWKG3hZRQ6SA",
        "outputId": "c34274b4-2cd5-49eb-f2ec-56ca7792f201"
      },
      "source": [
        "\"\"\"MoM check sites\"\"\"\n",
        "df_mom = check_mom_bts(towerdb, 'Site ID ', \"Bts_Sites\", msa ,'Site ID ', \"Bts_Sites\")\n",
        "df_mom"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Site ID</th>\n",
              "      <th>Bts_Sites</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Site ID , Bts_Sites]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puVgZAG4RT6F"
      },
      "source": [
        "\"\"\" Check On air Sites\"\"\"\n",
        "def check_on_air_sites(df, df_index, df_cols):\n",
        "\n",
        "    df_check = df[df_cols]\n",
        "    df_check['sites'] = df_check[df_index]\n",
        "    df_check = df_check.set_index('sites')\n",
        "    #df_check = replace_values(df_check, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in df_cols: \n",
        "            if df_check.loc[site,df_index] not in df_errors.index:\n",
        "                df_errors.loc[site,df_index] = df_check.loc[site,df_index]\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "            else:\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    df = df_errors[df_errors.iloc[:,1:]]\n",
        "    df = df.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    #df = df_errors.dropna(how='all', axis=1)   \n",
        "    return df\n",
        "\n",
        "on_cols = ['Site ID ','Transmission System', 'Site Type (See structure Type)', 'Room Configuration', 'Air Conditioning',\"Bts_Sites\", 'Sites_As_Metered_Estimated']\n",
        "df_on_air = check_on_air_sites(actives, 'Site ID ', on_cols)\n",
        "df_on_air\n",
        "#No errros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR9tLCZ5RoyT"
      },
      "source": [
        "path_uip = '/content/UserInput_Ireland_July 21_06.07.2021.xlsx'\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip = pd.read_excel(path_uip ,sheet_name='SiteLevel',usecols=[0,1,2],skiprows=2)\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [i for i in msa['Site ID ']]\n",
        "tw_sites = [i for i in towerdb ['Site ID ']]\n",
        "uip_sites = [i for i in uip['Site_ID']]\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN6Jg1eJSUD8",
        "outputId": "c234567b-d150-4f30-e169-2533077e84d9"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [i for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uis = [i for i in df_towerdb[(df_towerdb[bts_col]=='Yes')&(df_towerdb[status_col]=='In Service')][tw_index] if i not in uip_list]\n",
        "    bts_out_uis = pd.DataFrame(bts_out_uis, columns=['Bts_Sites_Out_UIS_File'])\n",
        "    bts_out_uis = pd.merge(bts_out_uis, filtered, how='left', left_on=['Bts_Sites_Out_UIS_File'], right_on=tw_index)\n",
        "    bts_out_uis = bts_out_uis[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    print(df[bill_col])\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "    #if not (new_sites.empty or bts_out_uis.empty or df_site_bts.empty):\n",
        "    return new_sites, bts_out_uis, df_site_bts[[tw_index,status_col, bts_col, bill_col]]\n",
        "\n",
        "msa_sites = [i for i in msa['Site ID ']]\n",
        "tw_sites = [i for i in towerdb ['Site ID ']]\n",
        "uip_sites = [i for i in uip['Site_ID']]\n",
        "#(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list)\n",
        "new_sites, bts_out_uis, df_site_bts = check_new_sites(towerdb, 'Site ID ', \"Bts_Sites\", \\\n",
        "                                                      'Billing Trigger Date','Site Status', msa_sites, tw_sites, uip_sites)\n",
        "#no errors"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Series([], Name: Billing Trigger Date, dtype: object)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlREMj57Tcgh"
      },
      "source": [
        "\"\"\"BTS Wip\"\"\"\n",
        "wip_out_tw_list, df_wip_and_bts_flagged = check_wip(actives,'Site ID ','WIP_Sites', \"Bts_Sites\", msa, 'Site ID ', \"Bts_Sites\")\n",
        "# No errors"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFq2dM-BUoBT"
      },
      "source": [
        "\"\"\"Decomissioned Sites\"\"\"\n",
        "def check_decommissioned(df,df_index, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    return filtered[[df_index, decom_col, doer_col]]\n",
        "\n",
        "df_decom = check_decommissioned(actives,'Site ID ','Decommisioned_Site', 'Date of Equipment Removal')\n",
        "df_decom\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "yidvieJtVXOq",
        "outputId": "eda00437-9708-42de-a5c8-84dfe2f54f94"
      },
      "source": [
        "\"\"\"Check doer in service sites\"\"\"\n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]') < current_date)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "        else:\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]').empty)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "#check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col)       \n",
        "df_doer = check_tw_bill_doer(actives, 'Site ID ', 'Date of Equipment Removal','Site Status','In Service', 'doer')\n",
        "df_doer\n",
        "# No errors"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Site ID</th>\n",
              "      <th>Site Status</th>\n",
              "      <th>Date of Equipment Removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Site ID , Site Status, Date of Equipment Removal]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IEdLtcFVb8u"
      },
      "source": [
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    uis_sites_not_in_towerdb = []\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "    if uis_sites_not_in_towerdb:\n",
        "        uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS In Month not active in TowerDB!'])\n",
        "        uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS In Month not active in TowerDB!',\\\n",
        "                                        right_on=tw_index)\n",
        "        uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS In Month not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS In Month!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS In Month!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS In Month!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    if decom_col != \"\":\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "        decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "    \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        \n",
        "        critical = []\n",
        "        if len(bts_tw_critical) > 0:\n",
        "            if set(uip_critical).intersection(bts_tw_critical):\n",
        "                critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "                #print(critical)\n",
        "        if critical:\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIPS File', status_tw_col]]\n",
        "        #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "        return uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        critical = []\n",
        "        #if len(uip_critical) > 0:\n",
        "        critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "        #if not (in_service_uip_sites.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "        return in_service_uip_sites, bts_sites_out_uip\n",
        "\n",
        "\"\"\"Check UIP sites matches with Towerdb sites\"\"\"\n",
        "uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical = check_uip_tw(towerdb,'Site ID ', 'Site Status', \\\n",
        "                                                              'Decommisioned_Site', \"Bts_Sites\",'CriticalSite_Beyond_10', \\\n",
        "                                                              uip, uip_sites)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "rYd1NJbRVeix",
        "outputId": "785445ab-d910-4313-e30e-79f9cfbd6e2b"
      },
      "source": [
        "uis_sites_not_in_towerdb"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UIS In Month not active in TowerDB!</th>\n",
              "      <th>Site Status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DN354</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RN038</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  UIS In Month not active in TowerDB! Site Status\n",
              "0                               DN354  Dismantled\n",
              "1                               RN038  Dismantled"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "1oT_k_kFWb6I",
        "outputId": "834394fd-f972-4f80-f75a-d2309cfab7bb"
      },
      "source": [
        "in_service_not_in_uis"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TowerDB Sites out of UIS In Month!</th>\n",
              "      <th>Site Status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DN433</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DN822</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DNTWR</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DN320</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DNCF1</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DNNGE</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>WH032</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>DN304</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  TowerDB Sites out of UIS In Month! Site Status\n",
              "0                              DN433  In Service\n",
              "1                              DN822  In Service\n",
              "2                              DNTWR  In Service\n",
              "3                              DN320  In Service\n",
              "4                              DNCF1  In Service\n",
              "5                              DNNGE  In Service\n",
              "6                              WH032  In Service\n",
              "7                              DN304  In Service"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LckjPL1rWfob"
      },
      "source": [
        "\"\"\"Check UIP sites matches with Towerdb sites\"\"\"\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    uis_sites_not_in_towerdb = []\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "    if uis_sites_not_in_towerdb:\n",
        "        uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS Sites True Up not active in TowerDB!'])\n",
        "        uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS Sites True Up not active in TowerDB!',\\\n",
        "                                        right_on=tw_index)\n",
        "        uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS Sites True Up not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS True Up Sites!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS True Up Sites!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS True Up Sites!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    if decom_col != \"\":\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "        decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "    \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        \n",
        "        critical = []\n",
        "        if len(bts_tw_critical) > 0:\n",
        "            if set(uip_critical).intersection(bts_tw_critical):\n",
        "                critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "                #print(critical)\n",
        "        if critical:\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIPS File', status_tw_col]]\n",
        "        #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "        return uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        critical = []\n",
        "        if len(uip_critical) > 0:\n",
        "            if set(uip_critical).intersection(bts_tw_critical):\n",
        "                critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "        #if not (in_service_uip_sites.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "        return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "\n",
        "\n",
        "path_uiptrue = '/content/UserInput_Ireland_20210630 June delta.xlsx'\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip_true = pd.read_excel(path_uip ,sheet_name='SiteLevel',usecols=[0,1,2],skiprows=2)\n",
        "uip_true.columns = uip_names\n",
        "uip_sites_true = [i for i in uip_true['Site_ID']]\n",
        "\n",
        "uis_true_not_in_towerdb,in_service_uis_true, decomiss_in_uis, bts_sites_out_uis_true, critical_true = check_uip_tw(towerdb,'Site ID ', 'Site Status', \\\n",
        "                                                                'Decommisioned_Site', \"Bts_Sites\",'CriticalSite_Beyond_10', uip, uip_sites)\n",
        "#No errors\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "cnYckc_DY81S",
        "outputId": "bd4a2345-2851-4a05-89b7-3bec3cefde42"
      },
      "source": [
        "uis_true_not_in_towerdb"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UIS Sites True Up not active in TowerDB!</th>\n",
              "      <th>Site Status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DN354</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RN038</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  UIS Sites True Up not active in TowerDB! Site Status\n",
              "0                                    DN354  Dismantled\n",
              "1                                    RN038  Dismantled"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "HiJf7VEsY_CU",
        "outputId": "e688466a-d8e4-44fd-f441-93daeeaecdc1"
      },
      "source": [
        "in_service_uis_true"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TowerDB Sites out of UIS True Up Sites!</th>\n",
              "      <th>Site Status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DN433</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DN822</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DNTWR</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DN320</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DNCF1</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DNNGE</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>WH032</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>DN304</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  TowerDB Sites out of UIS True Up Sites! Site Status\n",
              "0                                   DN433  In Service\n",
              "1                                   DN822  In Service\n",
              "2                                   DNTWR  In Service\n",
              "3                                   DN320  In Service\n",
              "4                                   DNCF1  In Service\n",
              "5                                   DNNGE  In Service\n",
              "6                                   WH032  In Service\n",
              "7                                   DN304  In Service"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "YnqjnzMoZKa4",
        "outputId": "758d3b22-9e14-406a-8d9f-1a817517ea46"
      },
      "source": [
        "def check_diffs_v2(path_current, path_last, sheet='Commercial'):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def highlight_diff(data, color='yellow'):\n",
        "        attr = 'background-color: {}'.format(color)\n",
        "        other = data.xs('Current', axis='columns', level=-1)\n",
        "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                            index=data.index, columns=data.columns)\n",
        "    #type_file = type_file.lower()\n",
        "    #if type_file =='excel':\n",
        "    _actual = pd.read_excel(path_current,sheet_name=sheet).fillna('')\n",
        "    _before = pd.read_excel(path_last,sheet_name=sheet).fillna('')\n",
        "\n",
        "    df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "    df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "    #df_final.style.apply(highlight_diff, axis=None)\n",
        "    if not df_final.empty:\n",
        "        return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "    else:\n",
        "        print('\\nNo differences Founded!\\n')\n",
        "path_before = '/content/UserInput_Ireland_20210731.xlsx'\n",
        "path_curr = '/content/UserInput_Ireland_July 21_06.07.2021.xlsx'\n",
        "df_comm = check_diffs_v2(path_curr, path_before)\n",
        "df_comm\n",
        "# No errors"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_fa8e71a8_f6d7_11eb_8d1e_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" colspan=2>Sub_charge_Type</th>        <th class=\"col_heading level0 col2\" colspan=2>Param1</th>        <th class=\"col_heading level0 col4\" colspan=2>Param2</th>        <th class=\"col_heading level0 col6\" colspan=2>Data_Type</th>        <th class=\"col_heading level0 col8\" colspan=2>Input_Value</th>        <th class=\"col_heading level0 col10\" colspan=2>Description/Instruction</th>        <th class=\"col_heading level0 col12\" colspan=2>Frequency of Update</th>    </tr>    <tr>        <th class=\"blank level1\" ></th>        <th class=\"col_heading level1 col0\" >Current</th>        <th class=\"col_heading level1 col1\" >Last</th>        <th class=\"col_heading level1 col2\" >Current</th>        <th class=\"col_heading level1 col3\" >Last</th>        <th class=\"col_heading level1 col4\" >Current</th>        <th class=\"col_heading level1 col5\" >Last</th>        <th class=\"col_heading level1 col6\" >Current</th>        <th class=\"col_heading level1 col7\" >Last</th>        <th class=\"col_heading level1 col8\" >Current</th>        <th class=\"col_heading level1 col9\" >Last</th>        <th class=\"col_heading level1 col10\" >Current</th>        <th class=\"col_heading level1 col11\" >Last</th>        <th class=\"col_heading level1 col12\" >Current</th>        <th class=\"col_heading level1 col13\" >Last</th>    </tr></thead><tbody>\n",
              "        </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fa4456aa150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaw6H__1aihQ",
        "outputId": "ee7b886e-6b9c-4f31-db10-fa47663c36e2"
      },
      "source": [
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "        #lista = []\n",
        "\n",
        "        df.fillna(\"\", inplace=True)\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df\n",
        "\n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    #df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    if not df_commercial_diffs.empty:\n",
        "        return df_commercial_diffs\n",
        "    else:\n",
        "        print('\\nNo Errors founded!')\n",
        "\n",
        "names = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', 'Input_Value',\\\n",
        "         'Description/Instruction', 'Frequency of Update']\n",
        "merge_cols = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', \\\n",
        "              'Description/Instruction', 'Frequency of Update']\n",
        "cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Data_Type','Input_Value_actual',\\\n",
        "                'Input_Value_before','Equal Values','Description/Instruction', 'Frequency of Update']\n",
        "# Check for commercial Values into current UIP File and compare with UIP File before\n",
        "\n",
        "df_com = check_commercial(path_curr, path_before,['Input_Value'], names, merge_cols, cols_ordered)\n",
        "df_com"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors founded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrflpqUkCs5M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUNmmb-3wsMu"
      },
      "source": [
        "Excel log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tCh5lYFv3Cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdd8071-1e11-4ef6-8eb7-ad3a9bac4809"
      },
      "source": [
        "df_tw = [df_picklist, in_service_not_in_uis, uis_true_not_in_towerdb]\n",
        "sheetnames_tw = ['Picklist Errors Actives Sites','TowerDB Sites out of UIS In Month!', \"TowerDB Sites out of UIS True Up Sites\", \"\"]\n",
        "\n",
        "path_tw = '/content/towerdb_IE_errors.xlsx'\n",
        "general_log_erros(df_tw, sheetnames_tw, path_tw)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/openpyxl/workbook/child.py:102: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
            "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZusOO39hZh9"
      },
      "source": [
        "TA Input File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUu8efkLrGYU",
        "outputId": "7b2c14fc-6fe7-4953-8e6d-e9555c07a4dd"
      },
      "source": [
        "#Comparando os ficheiros\n",
        "pathta='/content/TowerDB_Ireland_20210531 1.xlsx'\n",
        "pathold = '/content/TA_Input_Ireland_20210630.csv'\n",
        "sheet='TA_Input_Ireland_20210531'\n",
        "skiprows=0\n",
        "skipcols=0\n",
        "ta_save = '/content/TA_IE'\n",
        "ta_bills = ['Site ID ','Customer Type ','Lease Start Date ', 'Lease End date ']\n",
        "find_diffs_between_files(pathold, pathta, 'Site ID ', ta_bills,'',ta_save, 'mix','ta',sheet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['DL020 ', 'MNCVG']\n",
            "Dropped Rows: []\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0hTjigk4T0P"
      },
      "source": [
        "#Read TA Input File\n",
        "def read_files(path, sheetname,col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int,\\\n",
        "               cols_amount, bill_cols, format='mix', type_date=\"%d/%m/%Y\"):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "    #header=0, names = col_name,\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df[col_name] \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_amount, 0)\n",
        "    for col in cols_amount:\n",
        "        df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "    df = replace_values(df, cols_int, 0)\n",
        "    for col in cols_int:\n",
        "        df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int)\n",
        "    df = df.fillna('')\n",
        "    df = replace_values(df, bill_cols)\n",
        "    date_parser(df, cols_date, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    return df\n",
        "ta_col_ord = [\"Site ID \",\"Site Name \",\"Address \",\"Site Provider\",\"Site type\",\\\n",
        "              \"Tenant/Customer \",\"Customer Type \",\" Initial Rent  \",\" Current Rent  \",\" REC Uplift Charge \",\\\n",
        "              \"Current Power Charge\",\"Renegotiations \",\"Lease Start Date \",\"Lease End date \",\"Right to Renew \",\\\n",
        "              \"End of Renewal Period \",\"Payment Terms \",\"Payment Frequency \",\"Rent Review \",\"Indexation Driver\",\\\n",
        "              \"Fixed Increase \",\"CPI \",\"Percentage\",\"Increase frequency\",\"Increase Date\",\"VAT\",\"Rate \",\"Termination date \"]\n",
        "interger = ['Increase frequency']\n",
        "amount = [\" Initial Rent  \", \" Current Rent  \"]\n",
        "ta_bill_cols = [\"Site ID \",\"Tenant/Customer \",\"Customer Type \",\\\n",
        "                \"Lease Start Date \",\"Lease End date \"]\n",
        "ta_dates = [\"Lease Start Date \",\"Lease End date \", \"Increase Date\"]\n",
        "              \n",
        "path='/content/TowerDB_Ireland_20210630 v1.xlsx'\n",
        "sheet='TA_Input_Ireland_20210630'\n",
        "skiprows=0\n",
        "skipcols=0\n",
        "#(path, sheetname,col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int,\\\n",
        "               #cols_amount, bill_cols, format='mix', type_date=\"%d/%m/%Y\")\n",
        "ta = read_files(path, sheet,ta_col_ord, 0, 0, \"Site ID \", ta_dates, interger,amount, ta_bill_cols)\n",
        "ta.head(3)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcjgdlF976fT"
      },
      "source": [
        "ta.to_csv('/content/TA_Input_Ireland_20210831.csv', index=False)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QjbaAY_l2Gv"
      },
      "source": [
        "cols_date = ['Site ID ', 'Lease Start Date ', 'Lease End date ']\n",
        "df_dates_erros = check_date_columns(ta, 'Site ID ', cols_date, 2)\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "2p4MEVcBnfJv",
        "outputId": "e82f8bee-762b-4cac-ef9d-830ba0518d99"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "\n",
        "        filtered = df[pd.to_datetime(df[start_date]) > pd.to_datetime(df[end_date])]\n",
        "        return filtered[[tw_index, start_date, end_date]]\n",
        "\n",
        "df_ta_dates = check_lc_ta_dates(ta,\"Site ID \", \"Lease Start Date \",\"Lease End date \")\n",
        "df_ta_dates"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Site ID</th>\n",
              "      <th>Lease Start Date</th>\n",
              "      <th>Lease End date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Site ID , Lease Start Date , Lease End date ]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sSdql1FhKKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4557af8e-940f-40cd-c60d-9b3f35fa8b12"
      },
      "source": [
        "\"\"\"Picklist Values\"\"\"\n",
        "picklist_ta = {\n",
        "    'Customer Type ': ['MNO','OTMO']\n",
        "}\n",
        "ta['Customer Type '].value_counts()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MNO     494\n",
              "OTMO    229\n",
              "Name: Customer Type , dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUNai6YPvwV5"
      },
      "source": [
        "LC Input File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P36w6ygyr49X"
      },
      "source": [
        "#Comparando os ficheiros\n",
        "pathlc='/content/TowerDB_Ireland_20210531 1.xlsx'\n",
        "pathold_lc = '/content/LC_Input_Ireland_20210630.csv'\n",
        "sheet_lc='LC_Input_Ireland_20210531'\n",
        "skiprows=0\n",
        "skipcols=0\n",
        "lc_save = '/content/LC_IE'\n",
        "lc_bills = ['Site ID ', ' Total Rent ', 'Lease Start Date ', 'Lease End date ']\n",
        "find_diffs_between_files(pathold_lc, pathlc, 'Site ID ', lc_bills,'',lc_save, 'mix','ta',sheet_lc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjMBeXxb-Lqx"
      },
      "source": [
        "LC Validations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7i1rWzA-KxL"
      },
      "source": [
        "#Read TA Input File\n",
        "def read_files(path, sheetname,col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int,\\\n",
        "               cols_amount, bill_cols, format='mix', type_date=\"%d/%m/%Y\"):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "    #header=0, names = col_name,\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df[col_name] \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_amount, 0)\n",
        "    for col in cols_amount:\n",
        "        df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "    df = replace_values(df, cols_int, 0)\n",
        "    for col in cols_int:\n",
        "        df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].astype(int)\n",
        "    df = df.fillna('')\n",
        "    df = replace_values(df, bill_cols)\n",
        "    date_parser(df, cols_date, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    return df\n",
        "lc_ord = [\"Site ID \",\"Site Name \",\"Address \",\"Site Provider\",\"Contract Type\",\"Vendor\",\" Current Rent  \",\\\n",
        "          \" Sublet/Uplift \",\" Total Rent \",\"Misc Fees \",\"Sublet Clause \",\"Conditions \",\"Uplift for Sublet \",\\\n",
        "          \"Lease Start Date \",\"Lease End date \",\"Right to Renew \",\"End of Renewal Period \",\"Payment Terms \",\\\n",
        "          \"Payment Frequency\",\"Rent Review \",\"Fixed Increase \",\"Indexation Driver\",\"CPI/Increase Type\",\\\n",
        "          \"Percentage\",\"Increase Frequency\",\"Increase date\",\"VAT Subject\",\"VAT\",\"Termination date \",\"Observations\"]\n",
        "interger = [\" Sublet/Uplift \"]#'Increase frequency'\n",
        "amount = []#\" Current Rent  \",\" Total Rent \"\n",
        "ta_bill_cols = [\"Site ID \",\" Total Rent \",\\\n",
        "                \"Lease Start Date \",\"Lease End date \"]\n",
        "dates = [\"Lease Start Date \",\"Lease End date \", \"Increase date\"]             \n",
        "path='/content/TowerDB_Ireland_20210630 v1.xlsx'\n",
        "sheet_lc='LC_Input_Ireland_20210630'\n",
        "#(path, sheetname,col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int,\\\n",
        "               #cols_amount, bill_cols, format='mix', type_date=\"%d/%m/%Y\")\n",
        "lc = read_files(path, sheet_lc,lc_ord, 0, 0, \"Site ID \", dates, interger,amount, ta_bill_cols)\n",
        "lc[\"Uplift for Sublet \"] = [re.sub('^€', '', str(x)) for x in lc[\"Uplift for Sublet \"]]\n",
        "lc[\"Uplift for Sublet \"] = lc[\"Uplift for Sublet \"].replace('-', '')\n",
        "\n",
        "lc.head()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXXMSe5I-3Ns"
      },
      "source": [
        "lc.to_csv('/content/LC_Input_Ireland_20210831.csv', index=False)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "MSoO4rltm8Gm",
        "outputId": "ad9b7af4-e964-485a-e064-f325806377c1"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "\n",
        "        filtered = df[pd.to_datetime(df[start_date]) > pd.to_datetime(df[end_date])]\n",
        "        return filtered[[tw_index, start_date, end_date]]\n",
        "\n",
        "df_lc_dates = check_lc_ta_dates(lc,\"Site ID \", \"Lease Start Date \",\"Lease End date \")\n",
        "df_lc_dates"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Site ID</th>\n",
              "      <th>Lease Start Date</th>\n",
              "      <th>Lease End date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Site ID , Lease Start Date , Lease End date ]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bmmCXE1_tQU"
      },
      "source": [
        "def check_amounts(df_check, df_index, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df_check[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            print(df.loc[site,column])\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "\n",
        "\"\"\"Check Amount\"\"\"\n",
        "lc_col_amounts = [' Total Rent ']\n",
        "df_lc_amounts = check_amounts(lc, \"Site ID \", lc_col_amounts, 2)\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5giQV3Wwysa",
        "outputId": "5c84cb35-5819-4efd-e2ac-b2de5bd9f60e"
      },
      "source": [
        "df_lc = [df_lc_dates_erros]\n",
        "sheetnames_lc = [\"Sites In Service with incorrect date\"]\n",
        "\n",
        "path_lc = '/content/lc_ie_errors.xlsx'\n",
        "general_log_erros(df_lc, sheetnames_lc, path_lc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/openpyxl/workbook/child.py:102: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
            "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}