{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ro_validations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTvUidHgg53JI3uKfrqJfY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI87yjm0BP3x"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "\n",
        "def csv_files(path):\n",
        "    \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "    import csv\n",
        "    f = open(path, encoding='windows-1252', errors='ignore')\n",
        "    data = []\n",
        "    for row in csv.reader(f, delimiter=','):\n",
        "        data.append(row)\n",
        "    col = [*data[0]]\n",
        "    data.pop(0)\n",
        "    df = pd.DataFrame(data, columns=col)\n",
        "    return df, col\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index):\n",
        "    \"\"\"\n",
        "    Params:\\n\n",
        "    path: parth of file in the computer.\\n\n",
        "    n_skiprows: Number of rows to delete in the original file,.\\n\n",
        "    columns_to_convert: Columns to convert the data general type. \\n\n",
        "    n_skipcolumn: Columns to skip in the original file. \\n\n",
        "    endrow = pass 0 to read everything, 1 to count entire\n",
        "    columns_order: List of columns names in specific order to pass in the engine.\\n\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    \"\"\"cont = 0\n",
        "    for i in df.iloc[:,site_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[:cont, :]\n",
        "    #df.columns = columns_order\"\"\"\n",
        "    \n",
        "    # convert intery columns to integer \n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].fillna(0)\n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].astype('int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def check_df(df, error_msg):\n",
        "    if df == None or df.empty:\n",
        "        return f'{error_msg}'\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def count_duplicates(lista):\n",
        "    count_dict = {}\n",
        "    for entry in lista:\n",
        "        if entry in count_dict.keys():\n",
        "            count_dict[entry] += 1\n",
        "        else:\n",
        "            count_dict[entry] = 1\n",
        "    \n",
        "    duplicates = {}\n",
        "    for k, v in count_dict.items():\n",
        "        if v > 1:\n",
        "            duplicates[k] = v\n",
        "    return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "def defining_df(df, column_range, number_col):\n",
        "    cont = 0\n",
        "    for i in df.iloc[:,number_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[0:cont, :]\n",
        "    return df\n",
        "\n",
        "#old version\n",
        "def check_columns(table, output_columns):\n",
        "    \"\"\"\n",
        "    Check the total of number of missing columns and the missing columns in passed table.\\n\n",
        "\n",
        "    Params:\\n\n",
        "    table: contain the columns to be check.\\n\n",
        "    output_columns: columns structure at the final file. \n",
        "\n",
        "    Returns:\\n\n",
        "    Number of missing columns and a list that contains the name os missing columns.\n",
        "    \"\"\"\n",
        "    total_received = len(table.columns)\n",
        "    number_missing_columns = 0\n",
        "    missing_columns = []\n",
        "    #Counting of missing columns       \n",
        "    #if country in contries:      \n",
        "    for columns in output_columns:\n",
        "        if columns.lower() not in [labels.lower() for labels in table]:\n",
        "            number_missing_columns +=1\n",
        "            missing_columns.append(columns)\n",
        "    \n",
        "    return total_received, number_missing_columns, missing_columns\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    \"\"\"\n",
        "    for i in bill_cols:\n",
        "        if i not in twdb_col:\n",
        "            col_miss.append(i)\"\"\"\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", 0, '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "      \n",
        "def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "    t_col = type_dates.lower()\n",
        "    if format == 1:\n",
        "        type_date = \"%d/%m/%Y\"\n",
        "    else:\n",
        "        type_date = \"%d-%m-%Y\"\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Refactorar esse codigo para receber todas as colunas num dic\n",
        "# Sendo as keys=columns e values= picklist for each column\n",
        "def check_date_columns(df, df_index,status_col,columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "\n",
        "def check_amounts(df_check, df_index, status_col, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    filtered = df_check[[df_index, status_col]]\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        df_new = pd.merge(df_new, filtered, how='left', left_on='sites', right_on=tw_index)\n",
        "        df_new = df_new[['sites', status_col]]\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "        \n",
        "def check_picklist(df,df_index,df_status, df_cols, picklist_dict):\n",
        "\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        columns = [i.lower() for i in picklist_dict.keys()]\n",
        "        for column in set(columns): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            #print(value)\n",
        "            if not value in set(picklist_dict[column]) or pd.isnull(value):\n",
        "                #print(set(picklist_dict[column]))\n",
        "\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "                else:\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "\n",
        "    #df = df_errors.dropna()   \n",
        "    df_errors = df_errors.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    if len(df_errors)>0:\n",
        "        df = df[[df_index, df_status]]\n",
        "        df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "        df_errors = df_errors.reset_index()\n",
        "    else:\n",
        "        print('\\nNo one Picklist Error Founded!\\n')\n",
        "    return df_errors\n",
        "\n",
        "def check_picklist_3(df,df_index, picklist_dict):\n",
        "    log = {}\n",
        "    for column in picklist_dict:\n",
        "        df_aux = df.copy()\n",
        "        new = df_aux[column].isin(picklist_dict[column])\n",
        "        #new = df_aux[column].apply(lambda x: x in picklist_dict[column])\n",
        "        # Aceita somento os valores que não estão na picklist\n",
        "        indexes = df.index[new == False].tolist()\n",
        "        #if len(indexes)>0:\n",
        "        if column not in log:log[column]=[]\n",
        "        log[column]=log[column]+indexes\n",
        "    #print(log.keys())\n",
        "\n",
        "    newDict ={}\n",
        "    df1 = pd.DataFrame()\n",
        "    for key,value in log.items():\n",
        "        for val in value:  \n",
        "            ID=df.iloc[val][df_index]\n",
        "            if ID in newDict:\n",
        "                newDict[ID].append(key)\n",
        "                if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                    df.iloc[val,key] = 'Blank Value'\n",
        "                else:\n",
        "                    df.iloc[val,key] = f'Incorret picklist value: {value}'\n",
        "            else:\n",
        "                newDict[ID] = [key]\n",
        "        \n",
        "    logs = pd.DataFrame.from_dict(newDict, orient='index')\n",
        "    return logs\n",
        "\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [i for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uis = [i for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index] if str(i) not in uip_list]\n",
        "    bts_out_uis = pd.DataFrame(bts_out_uis, columns=['Bts_Sites_Out_UIS_File'])\n",
        "    bts_out_uis = pd.merge(bts_out_uis, filtered, how='left', left_on=['Bts_Sites_Out_UIS_File'], right_on=tw_index)\n",
        "    bts_out_uis = bts_out_uis[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "    if not (new_sites.empty or bts_out_uis.empty or df_site_bts.empty):\n",
        "        return new_sites, bts_out_uis, df_site_bts[[tw_index,status_col, bts_col, bill_col]]\n",
        "        \n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, status_col, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    filtered = df_tw[[tw_index, status_col]]\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    if not df.empty:\n",
        "        df = pd.merge(df, filtered, how='left', left_on=['New Sites'], right_on=tw_index)\n",
        "        df = df[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "        return df\n",
        "    else: \n",
        "        print('\\nNo errors founded!\\n')\n",
        "\n",
        "def check_wip(df_tw,tw_index,tw_status, tw_wip, tw_bts, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[tw_wip]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[tw_wip]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index,tw_status,tw_wip, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    if not(len(wip_out_tw_list)==0 or tw_wip_site_bts_flagged.empty):\n",
        "        return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "\n",
        "def check_decommissioned(df,df_index,status_col, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[df_index,status_col, decom_col, doer_col]]\n",
        "    else:\n",
        "        print('\\nNo errors Founded!\\n')\n",
        "    \n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    df_tw = df_tw[df_tw[status_col]==status][[tw_index, status_col, date_col]] \n",
        "    #print(df_tw)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[df_tw[date_col].astype('datetime64[ns]') < current_date]\n",
        "            return filtered\n",
        "        else:\n",
        "            #bill_dates = pd.to_datetime(df_tw[date_col], errors==coerrce)\n",
        "            filtered = df_tw[df_tw[date_col]=='']\n",
        "            return filtered\n",
        "    else:\n",
        "        print('\\nNo errors founded!\\n')\n",
        "\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col,bill_col, status_col, dt_format):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date, format=dt_format)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        df_tw[bill_col] = pd.to_datetime(df_tw[bill_col],errors='coerce', format=dt_format)\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)&\\\n",
        "                         (df_tw[bill_col].astype('datetime64[ns]') < current_date)]\n",
        "        return filtered[[tw_index, status_col, bill_col, doer_col]]  \n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "                                                              \n",
        "def check_mom_bts(df_tw, tw_index,status_col, tw_col, df_msa, msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index,status_col, tw_col]]    \n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "\n",
        "def check_lc_ta_dates(df,tw_index,status_col, start_date,end_date):\n",
        "    filtered = df[df[start_date] > df[end_date]]\n",
        "    return filtered[[tw_index,status_col, start_date,end_date]]\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "    t1 = ['pt', 'de', 'cz', 'ie', 'es', 'ro', 'hu']\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    if country.lower() in t1:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIS File!'])\n",
        "        in_service_uip_sites = pd.merge(in_service_uip_sites, filtered, how='left', left_on='Site In service out of UIS File!',\\\n",
        "                                        right_on=tw_index)\n",
        "        in_service_uip_sites = in_service_uip_sites[['Site In service out of UIS File!', status_tw_col]]\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        if decom_col != \"\":\n",
        "            tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "            decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "            decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "        \n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "            bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIPS File', status_tw_col]]\n",
        "            if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "                return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "        else:\n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "            bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "            if not (in_service_uip_sites.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "                return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIS File!'])\n",
        "        in_service_uip_sites = pd.merge(in_service_uip_sites, filtered, how='left', left_on='Site In service out of UIS File!',\\\n",
        "                                        right_on=tw_index)\n",
        "        in_service_uip_sites = in_service_uip_sites[['Site In service out of UIS File!', status_tw_col]]\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "        decomiss_sites_in_uip = []\n",
        "        if set(tw_decomiss).intersection(uip_sites):\n",
        "            decomiss_sites_in_uip = [i for i in tw_decomiss if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(decomiss_sites_in_uip, columns=['Decomissioned Site in UIS File'])\n",
        "        decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip = [i for i in df_uip['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Yes'][tw_index]\n",
        "        critical = []\n",
        "        if set(uip).intersection(bts_tw_critical):\n",
        "            critical = [i for i in bts_tw_critical if i not in uip]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% out in UIS File'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "        if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "            return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "        #lista = []\n",
        "\n",
        "        df.fillna(0, inplace=True)\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    \n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names).fillna('')\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names ).fillna('')\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols).fillna('')\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    if not df_commercial_diffs.empty:\n",
        "        return df_commercial_diffs\n",
        "    else:\n",
        "        print('\\nNo errors founded!')\n",
        "\n",
        "def check_diffs_v2(path_current, path_last, cols_order, type_file='Excel', sheet='Commercial'):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def highlight_diff(data, color='yellow'):\n",
        "        attr = 'background-color: {}'.format(color)\n",
        "        other = data.xs('Current', axis='columns', level=-1)\n",
        "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                            index=data.index, columns=data.columns)\n",
        "    type_file = type_file.lower()\n",
        "    if type_file =='excel':\n",
        "        _actual = pd.read_excel(path_current,sheet_name=sheet).fillna('')\n",
        "\n",
        "        _before = pd.read_excel(path_last,sheet_name=sheet).fillna('')\n",
        "\n",
        "        df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "        df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "        #df_final.style.apply(highlight_diff, axis=None)\n",
        "        if not df_final.empty:\n",
        "            return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "        else:\n",
        "            print('\\nNo differences Founded!\\n')\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() \n",
        "\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', dates_cols=[], status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col] \n",
        "            #df.columns = lower_str(list(df.columns))\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "        t_col = type_dates.lower()\n",
        "        if format == 1:\n",
        "            type_date = \"%d/%m/%Y\"\n",
        "        else:\n",
        "            type_date = \"%d-%m-%Y\"\n",
        "        for column in lower_str(columns):\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,engine='python').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, engine='python').fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD) header=0, names=cols_old,\n",
        "        df_OLD = pd.read_csv(path_OLD, engine='python').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols, 1, 'normal')\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEOQTYiBhuVz"
      },
      "source": [
        "Excel vs CSV(in month old)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYeRvgVAJfS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dab36a3-5e82-4376-ab87-bfbbd75530c2"
      },
      "source": [
        "# Compara two dfs, cause the header betweens files have a lot of names differents\n",
        "bill_cols = ['Code',\\\n",
        "             'Phase _1/_2',\\\n",
        "             'Categorization by Transmission Sys',\\\n",
        "\t\t\t 'Unused sites (TowerCo holds the property rights but does not host any Operator Equipment or Other Customer equipment )',\\\n",
        "\t\t\t 'Non-Vodafone equipment Sites (TowerCo holds the property rights and does not host any Operator Equipment but hosts Other Customer equipment  )',\\\n",
        "\t\t\t 'PowerOff Sites',\\\n",
        "\t\t\t 'Decommissioned sites',\\\n",
        "\t\t\t 'Categorization by Transmission Sys (sub-cluster)',\\\n",
        "\t\t\t 'Core Type',\\\n",
        "             'Macro Site - Transmission Hub Site with/Transmission Hub Site without Shelters',\\\n",
        "\t\t\t 'Transmission sites – with/Transmission Site without shelters.',\\\n",
        "\t\t\t 'Room Configuration',\\\n",
        "             'Power Supply',\\\n",
        "\t\t\t 'Air Conditioning',\\\n",
        "\t\t\t 'Active Sharing Arrangements involving the Operator',\\\n",
        "             'Categorization by Site Type',\\\n",
        "\t\t\t 'Billing Trigger Date',\\\n",
        "\t\t\t 'Current annual lease fees  ',\\\n",
        "\t\t\t 'Orange Passive Shared Sites ',\\\n",
        "\t\t\t 'Orange Active Shared Sites ',\\\n",
        "\t\t\t '10% Critical',\\\n",
        "\t\t\t 'Bundled sites',\\\n",
        "\t\t\t 'Strategic Sites',\\\n",
        "\t\t\t 'Unilateral Orange Transmission Site',\\\n",
        "\t\t\t 'First 10 Unilateral Orange Transmission Site',\\\n",
        "\t\t\t 'Transfer_Date_Of_Registration_Required_Sites',\\\n",
        "\t\t\t 'Wip_Site',\\\n",
        "\t\t\t 'Bts_Site',\\\n",
        "\t\t\t 'Sites_As_Metered_Estimated',\\\n",
        "\t\t\t 'Strategic_Site_Bucket',\\\n",
        "\t\t\t 'Subsequent_Sharing_Arrangement',\\\n",
        "             'First_Active_Sharing_Start_Date',\\\n",
        "             'First_Active_Sharing_End_Date',\\\n",
        "\t\t\t 'Sites_Within_500_Macro_Sites',\\\n",
        "\t\t\t 'Date_Of_Equipment_Removal',\\\n",
        "\t\t\t 'RFAI ( Ready For Active Installation )',\\\n",
        "\t\t\t 'X',\\\n",
        "\t\t\t 'Site Status',\\\n",
        "             'Critical site']\n",
        "\n",
        "path_tw = '/content/TowerDB_Romania_20210731.xlsx'\n",
        "sheet= 'Enhanced towerDB'\n",
        "pathtw_old = '/content/TowerDB_Romania_20210630.csv'\n",
        "tw_save = '/content/TW_RO'\n",
        "new_name = 'TowerDB_Romania_20210731.xlsx'\n",
        "old_name = 'TowerDB_Romania_20210630.csv'\n",
        "tw_dates = ['Billing Trigger Date', 'First_Active_Sharing_Start_Date','First_Active_Sharing_End_Date', 'RFAI ( Ready For Active Installation )']\n",
        "find_diffs_between_files(pathtw_old, path_tw, 'Code',bill_cols, tw_save, old_name, new_name, type_file='mix',\\\n",
        "                         dates_cols=tw_dates, status_col='Site Status',kind='tw',kind_col='',sheetname=sheet)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     []\n",
            "Dropped Rows: ['A6056', 'A4394', 'A4112', 'A856', 'N4590', 'N4861', 'N4523', 'N5033', 'E4013', 'E3012', 'E4036', 'E2806', 'N5353', 'N4962', 'N6509', 'A2320', 'N4598', '1633', 'N4652', 'N4551']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lro6s2yUh0dN"
      },
      "source": [
        "CSV(in month) vc CSV (in month old)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJDAYw2Lhy-U"
      },
      "source": [
        "bill_cols = ['Code',\\\n",
        "             'Phase _1/_2',\\\n",
        "             'Categorization by Transmission Sys',\\\n",
        "\t\t\t 'Unused sites (TowerCo holds the property rights but does not host any Operator Equipment or Other Customer equipment )',\\\n",
        "\t\t\t 'Non-Vodafone equipment Sites (TowerCo holds the property rights and does not host any Operator Equipment but hosts Other Customer equipment  )',\\\n",
        "\t\t\t 'PowerOff Sites',\\\n",
        "\t\t\t 'Decommissioned sites',\\\n",
        "\t\t\t 'Categorization by Transmission Sys (sub-cluster)',\\\n",
        "\t\t\t 'Core Type',\\\n",
        "             'Macro Site - Transmission Hub Site with/Transmission Hub Site without Shelters',\\\n",
        "\t\t\t 'Transmission sites – with/Transmission Site without shelters.',\\\n",
        "\t\t\t 'Room Configuration',\\\n",
        "             'Power Supply',\\\n",
        "\t\t\t 'Air Conditioning',\\\n",
        "\t\t\t 'Active Sharing Arrangements involving the Operator',\\\n",
        "             'Categorization by Site Type',\\\n",
        "\t\t\t 'Billing Trigger Date',\\\n",
        "\t\t\t 'Current annual lease fees  ',\\\n",
        "\t\t\t 'Orange Passive Shared Sites ',\\\n",
        "\t\t\t 'Orange Active Shared Sites ',\\\n",
        "\t\t\t '10% Critical',\\\n",
        "\t\t\t 'Bundled sites',\\\n",
        "\t\t\t 'Strategic Sites',\\\n",
        "\t\t\t 'Unilateral Orange Transmission Site',\\\n",
        "\t\t\t 'First 10 Unilateral Orange Transmission Site',\\\n",
        "\t\t\t 'Transfer_Date_Of_Registration_Required_Sites',\\\n",
        "\t\t\t 'Wip_Site',\\\n",
        "\t\t\t 'Bts_Site',\\\n",
        "\t\t\t 'Sites_As_Metered_Estimated',\\\n",
        "\t\t\t 'Strategic_Site_Bucket',\\\n",
        "\t\t\t 'Subsequent_Sharing_Arrangement',\\\n",
        "             'First_Active_Sharing_Start_Date',\\\n",
        "             'First_Active_Sharing_End_Date',\\\n",
        "\t\t\t 'Sites_Within_500_Macro_Sites',\\\n",
        "\t\t\t 'Date_Of_Equipment_Removal',\\\n",
        "\t\t\t 'RFAI ( Ready For Active Installation )',\\\n",
        "\t\t\t 'X',\\\n",
        "\t\t\t 'Site Status',\\\n",
        "             'Critical site']\n",
        "\n",
        "path_tw = '/content/TowerDB_Romania_20210731.csv'\n",
        "sheet= 'Enhanced towerDB'\n",
        "pathtw_old = '/content/TowerDB_Romania_20210630.csv'\n",
        "tw_save = '/content/TW_RO'\n",
        "new_name = 'TowerDB_Romania_20210731.csv'\n",
        "old_name = 'TowerDB_Romania_20210630.csv'\n",
        "tw_dates = ['Billing Trigger Date', 'First_Active_Sharing_Start_Date','First_Active_Sharing_End_Date', 'RFAI ( Ready For Active Installation )']\n",
        "find_diffs_between_files(pathtw_old, path_tw, 'Code',bill_cols, tw_save, old_name, new_name, type_file='csv',\\\n",
        "                         dates_cols=tw_dates, status_col='Site Status',kind='tw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMj1v7iDvkWN"
      },
      "source": [
        "CSV(In month) vs CSV(True up)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTq_aZhrutKg"
      },
      "source": [
        "bill_cols = ['Code',\\\n",
        "             'Phase _1/_2',\\\n",
        "             'Categorization by Transmission Sys',\\\n",
        "\t\t\t 'Unused sites (TowerCo holds the property rights but does not host any Operator Equipment or Other Customer equipment )',\\\n",
        "\t\t\t 'Non-Vodafone equipment Sites (TowerCo holds the property rights and does not host any Operator Equipment but hosts Other Customer equipment  )',\\\n",
        "\t\t\t 'PowerOff Sites',\\\n",
        "\t\t\t 'Decommissioned sites',\\\n",
        "\t\t\t 'Categorization by Transmission Sys (sub-cluster)',\\\n",
        "\t\t\t 'Core Type',\\\n",
        "             'Macro Site - Transmission Hub Site with/Transmission Hub Site without Shelters',\\\n",
        "\t\t\t 'Transmission sites – with/Transmission Site without shelters.',\\\n",
        "\t\t\t 'Room Configuration',\\\n",
        "             'Power Supply',\\\n",
        "\t\t\t 'Air Conditioning',\\\n",
        "\t\t\t 'Active Sharing Arrangements involving the Operator',\\\n",
        "             'Categorization by Site Type',\\\n",
        "\t\t\t 'Billing Trigger Date',\\\n",
        "\t\t\t 'Current annual lease fees  ',\\\n",
        "\t\t\t 'Orange Passive Shared Sites ',\\\n",
        "\t\t\t 'Orange Active Shared Sites ',\\\n",
        "\t\t\t '10% Critical',\\\n",
        "\t\t\t 'Bundled sites',\\\n",
        "\t\t\t 'Strategic Sites',\\\n",
        "\t\t\t 'Unilateral Orange Transmission Site',\\\n",
        "\t\t\t 'First 10 Unilateral Orange Transmission Site',\\\n",
        "\t\t\t 'Transfer_Date_Of_Registration_Required_Sites',\\\n",
        "\t\t\t 'Wip_Site',\\\n",
        "\t\t\t 'Bts_Site',\\\n",
        "\t\t\t 'Sites_As_Metered_Estimated',\\\n",
        "\t\t\t 'Strategic_Site_Bucket',\\\n",
        "\t\t\t 'Subsequent_Sharing_Arrangement',\\\n",
        "             'First_Active_Sharing_Start_Date',\\\n",
        "             'First_Active_Sharing_End_Date',\\\n",
        "\t\t\t 'Sites_Within_500_Macro_Sites',\\\n",
        "\t\t\t 'Date_Of_Equipment_Removal',\\\n",
        "\t\t\t 'RFAI ( Ready For Active Installation )',\\\n",
        "\t\t\t 'X',\\\n",
        "\t\t\t 'Site Status',\\\n",
        "             'Critical site']\n",
        "\n",
        "path_tw = '/content/TowerDB_Romania_20210731.csv'\n",
        "sheet= 'Enhanced towerDB'\n",
        "pathtw_trueup = '/content/TowerDB_Romania_20210531.csv'\n",
        "tw_save = '/content/TW_RO'\n",
        "new_name = 'TowerDB_Romania_20210731.csv'\n",
        "old_name = 'TowerDB_Romania_20210531.csv'\n",
        "tw_dates = ['Billing Trigger Date', 'First_Active_Sharing_Start_Date','First_Active_Sharing_End_Date', 'RFAI ( Ready For Active Installation )']\n",
        "find_diffs_between_files(pathtw_trueup, path_tw, 'Code',bill_cols, tw_save, old_name, new_name, type_file='csv',\\\n",
        "                         dates_cols=tw_dates, status_col='Site Status',kind='tw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iupK8WYif50b"
      },
      "source": [
        "col_order = ['Scope','phase msa (phase 1)/pma (phase 2)','Code','Site Name','Macro Region','Region','Province','Municipality','Inhabitants','Address',\\\n",
        "             'Altitude','Latitude','Longitude','Categorization by Transmission Sys','Active or passive DAS',\\\n",
        "             'Unused sites (TowerCo holds the property rights but does not host any Operator Equipment or Other Customer equipment )',\\\n",
        "             'Non-Vodafone equipment Sites (TowerCo holds the property rights and does not host any Operator Equipment but hosts Other Customer equipment  )',\\\n",
        "             'PowerOff Sites','Decommissioned sites','Categorization by Transmission Sys (sub-cluster)','Core Type',\\\n",
        "             'macro site - transmission hub site with/without shelters','transmission sites – with/without shelters.','Room Configuration',\\\n",
        "             'Power Supply','Air Conditioning','Active Sharing Arrangements involving the Operator','Transmission Hub sites’','Antenna Positions','Weight of RRUs',\\\n",
        "             'Total power transmitted per site ','categorization by transmission sys.1','Categorization by Site Type','Categorization by inhabitants',\\\n",
        "             'Rural/ Suburban/ Urban','unnamed: 35','Technology VOD','Fiber / Microwave','Type of Structure','Tower Height','Floor space','POD ID','Energy Provider',\\\n",
        "             'Energy cost FYTD_DEC_20','Energy consumption FYTD_DEC_20','Infrastructure ready (existing)/ to be ready (new)','Billing Trigger Date','unnamed: 47',\\\n",
        "             'Radio equipments to be deactivated by','Infrastructure to be shared by','Counterpart','# of Lease Contracts','Current annual lease fees ',\\\n",
        "             'Current energy lease fees ','Current annual other fees ','Total Annual Lease','Sub-lease','(Average) residual duration','Maturity Cluster',\\\n",
        "             'ExCo rep. Avg Annual Lease costs','Total Energy Cost (Energy provider + LL)','VOD (y/n)','TLK','Annual Hosting TLK','Annual Hosting TLK (with Discount)',\\\n",
        "             'Annual Energy TLK','Annual Maintenance Fee TLK','Other Services Fee TLK','Total Revenues TLK','Total Revenues TLK (with Discount)','Residual duration TLK',\\\n",
        "             'Maturity Clusters TLK','Orange Total (without active sharing)','Orange(Transmission)','Orange(Indoor)','Orange Passive Shared Sites ',\\\n",
        "             'Annual Hosting ORANGE (Excl. active sharing)','Annual Hosting ORANGE (Excl. active sharing) (with Discount)',\\\n",
        "             'Annual Energy ORANGE(Excl. active sharing)','Annual Maintenance Fee ORANGE (Excl. active sharing)','Other Services Fee ORANGE (Excl. active sharing)',\\\n",
        "             'Total Revenues ORANGE(Excl. active sharing)','Total Revenues ORANGE(Excl. active sharing) (with Discount)','Residual duration ORANGE(Excl. active sharing)',\\\n",
        "             'Maturity Clusters ORANGE(Excl. active sharing)','RCS&RDS','Annual Hosting RCS&RDS','Annual Hosting RCS&RDS (with Discount)','Annual Energy RCS&RDS',\\\n",
        "             'Annual Maintenance Fee RCS&RDS','Other Services Fee RCS&RDS','Total Revenues RCS&RDS','Total Revenues RCS&RDS (with Discount)','Residual duration RCS&RDS',\\\n",
        "             'Maturity Clusters RCS&RDS','OTMO','Annual Hosting OTMOs','Annual Hosting OTMOs (with Discount)','Annual Energy OTMOs','Annual Maintenance Fee OTMOs',\\\n",
        "             'Other Services Fee OTMOs','Total Revenues OTMOs','Total Revenues OTMOs (with Discount)','Residual duration OTMOs','Maturity Clusters OTMOs',\\\n",
        "             'Total # of 3rd Party Tenants (excluding active sharing)','Annual Fee from 3rd Party Tenants','Annual Fee from 3rd Party Tenants (with Discount)',\\\n",
        "             'Annual Energy Fee from 3rd Party Tenants','Annual Maintenance Fee from 3rd Party Tenants','Other Services Fee from 3rd Party Tenants',\\\n",
        "             'Total Hosting Fee & Services from 3rd Party Tenants','Total Hosting Fee & Services from 3rd Party Tenants (with Discount)',\\\n",
        "             'Waighted Average residual duration','Macro Cluster Tenancy','Final Cluster Type of Contract','Macro Cluster 1','Sites w/ at list a DDS (Lease Contract Type)',\\\n",
        "             '# of Tenants','Categorization by Tenant combination','Categorization by Type of Passive contracts','Type of contract (principle contract)',\\\n",
        "             'Cluster Type of contract (principle contract)','Easement (Servitù di passaggio)','Final Cluster','Turistic sites','TLK.1','Orange','RCS&RDS.1','OTMOs',\\\n",
        "             'Orange Active Shared Sites ','Annual Hosting ORANGE','unnamed: 132','Annual Energy ORANGE','unnamed: 134','Total Revenues ORANGE','Only - Active Sharing Orange',\\\n",
        "             'Total # of 3rd Party Tenants (including active sharing)','Total 3rd Party Tenants hosting revenues (including active sharing)','FSO Criticality',\\\n",
        "             'TX LH POC','10% Critical','EVO SAP ID ','Enhanced BBU','Diameter\\nVodafone Antenna','Diameter\\n(Orange rural & Unilateranl ',\\\n",
        "             'Total diameter of microwave and mmWave antennas','Is the Site Under Construction','indicative completion date',\\\n",
        "             'Capital expenditure incurred to the MSA Effective Date','indicative capital expenditure required to complete the site build',\\\n",
        "             'proposed configuration for the completed site',\\\n",
        "             'Details of the Operator Equipment installed at the Site (including details of the Standard Configuration Attributes set out in Schedule 7 (Standard Configuration)',\\\n",
        "             'Details of the access arrangements applicable at each Site (including any access restrictions and applicable public access requirements)',\\\n",
        "             'Is the Existing Configuration is within the applicable Standard Configuration?','Bundled sites','Strategic Sites','Unilateral Orange Transmission Site',\\\n",
        "             'First 10 Unilateral Orange Transmission Site','Details of any Orange Equipment at Orange Shared Sites ','Transfer_Date_Of_Registration_Required_Sites',\\\n",
        "             'Wip_Site','bts sites','Sites_As_Metered_Estimated','Strategic_Site_Bucket','Subsequent_Sharing_Arrangement','First_Active_Sharing_Start_Date',\\\n",
        "             'First_Active_Sharing_End_Date','Sites_Within_500_Macro_Sites','Date_Of_Equipment_Removal','RFAI ( Ready For Active Installation ) ','X','Site Status',\\\n",
        "             'Critical site']\n",
        "\n",
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = 'code'\n",
        "tw_doer = 'date_of_equipment_removal'\n",
        "tw_status = 'site status'\n",
        "tw_bts = 'bts sites'\n",
        "tw_bill = 'rfai ( ready for active installation ) '\n",
        "tw_wip = 'wip_site'\n",
        "tw_decom = 'decommissioned sites'\n",
        "tw_critical = 'critical site'\n",
        "\n",
        "msa_index = 'code'\n",
        "msa_doer = 'date_of_equipment_removal'\n",
        "msa_status = 'site status'\n",
        "msa_bts = 'bts sites'\n",
        "msa_bill = 'rfai ( ready for active installation ) '\n",
        "msa_wip = 'wip_site'\n",
        "msa_decom = 'decommissioned sites'\n",
        "msa_critical = 'critical site'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-9L3MsJTt2V"
      },
      "source": [
        "path_tw = '/content/TowerDB_Romania_20210731.xlsx'\n",
        "sheet = 'Enhanced towerDB'\n",
        "\n",
        "towerdb = read_files(path_tw, sheet, 0, 0, 'Code')\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "dic_cols = {'phase _1/_2': 'phase msa (phase 1)/pma (phase 2)',\\\n",
        "            'macro site - transmission hub site with/transmission hub site without shelters': 'macro site - transmission hub site with/without shelters',\\\n",
        "            'transmission sites – with/transmission site without shelters.' : 'transmission sites – with/without shelters.',\\\n",
        "            'categorization by transmission sys_1' : 'categorization by transmission sys.1',\\\n",
        "            'no_1': 'unnamed: 35',\\\n",
        "            'no_2': 'unnamed: 47',\\\n",
        "            'current annual lease fees  ' : 'current annual lease fees ',\\\n",
        "            'tlk_1' : 'tlk.1',\\\n",
        "            'rcs&rds_1': 'rcs&rds.1',\\\n",
        "            'no_3':'unnamed: 132',\\\n",
        "            'no_4':'unnamed: 134',\\\n",
        "            'bts_site':'bts sites',\\\n",
        "            'rfai ( ready for active installation )' : 'rfai ( ready for active installation ) ',\\\n",
        "            'vdf energy cost eur': 'energy cost fytd_dec_20', 'vdf energy consum kwh': 'energy consumption fytd_dec_20'}\n",
        "towerdb.rename(columns=dic_cols, inplace=True)\n",
        "#towerdb = towerdb[lower_str(col_order)]\n",
        "towerdb = towerdb.reindex(columns=lower_str(col_order))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dodwtpu6yAGd"
      },
      "source": [
        "In month Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca9PKUaDx_tm"
      },
      "source": [
        "towerdb = pd.read_csv('/content/TowerDB_Romania_20210531.csv', engine='python').fillna('')\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "towerdb.columns = lower_str(col_order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGRcY-NY0vS-"
      },
      "source": [
        "new = []\n",
        "for i in towerdb['phase msa (phase 1)/pma (phase 2)']:\n",
        "    if i =='_1':\n",
        "        new.append('MSA (Phase 1)')\n",
        "    else:\n",
        "        new.append('MSA (Phase 2)')\n",
        "\n",
        "towerdb['phase _1/_2'] = new\n",
        "towerdb['phase _1/_2'].value_counts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsU54KiZ3qaE"
      },
      "source": [
        "path_msa = '/content/TowerDB_Romania_20210630.csv'\n",
        "msa = pd.read_csv(path_msa, engine='python', encoding='windows-1252').fillna('')\n",
        "msa.columns = lower_str(list(msa.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRyJlck4cn50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "ea57a9da-c587-409c-cd4a-e4705ac5ceaa"
      },
      "source": [
        "\"\"\"Check Columns Received\"\"\"             \n",
        "df_cols = check_columns_received(towerdb, lower_str(list(msa.columns)))\n",
        "#No columns missing\n",
        "df_cols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column(s) Missing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Column(s) Missing]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM4xIb3-8GlD"
      },
      "source": [
        "First Check - Dates Formats (dd/mm/YYYY)\n",
        "\n",
        "Columns: Date of equipment removal (from MAR´21)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8anMHTJbc8vS"
      },
      "source": [
        "\"\"\"You need to convert all values in cols for string format to check adn fill na with '' \"\"\"\n",
        "# Columns to functions\n",
        "dates_doer = [tw_index, tw_doer]\n",
        "dates_bill = [tw_index, tw_bill ]\n",
        "#Columns to parser\n",
        "bill=[tw_bill]\n",
        "doer=[tw_doer]\n",
        "\n",
        "#date_parser(towerdb, bill, \"%d/%m/%Y\", 'no')\n",
        "#date_parser(towerdb, doer, \"%d/%m/%Y\", 'mixed')\n",
        "\n",
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "no_actives = towerdb[towerdb[tw_status]=='Dismantled']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLSPcNFqUmwo"
      },
      "source": [
        "dates_bill = [tw_index, tw_bill]\n",
        "\"\"\"Checking columns for errors\"\"\"\n",
        "actives_dates_errors = check_date_columns(actives, tw_index, tw_status, dates_bill, 2) \n",
        "# Actives sites with blank billing trigger date\n",
        "\n",
        "no_actives_dates_errors = check_date_columns(no_actives, tw_index,tw_status, dates_doer, 2) \n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Rds_RiHFzmIq",
        "outputId": "f8ef676f-5bc4-4870-95d1-7afd01cdff01"
      },
      "source": [
        "actives_dates_errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>site status</th>\n",
              "      <th>rfai ( ready for active installation )</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2395</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   code site status rfai ( ready for active installation ) \n",
              "0  2395  In Service                             Blank Value"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go_Rn-GV9KIO"
      },
      "source": [
        "Second Check - TW Amount value General (xxx.xx)\n",
        "\n",
        "Column(s): ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuBSPpAV9J1W"
      },
      "source": [
        "amount_cols = [tw_index, \"\"\"???\"\"\"]\n",
        "df_amount_errors = check_amounts(actives, tw_index,tw_status, amount_cols, 1)\n",
        "#No one error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSv0C2ur9NVi"
      },
      "source": [
        "Thirth - Check Picklist values All sites\n",
        "\n",
        "Do this check in all sites\n",
        "\n",
        "Check the picklist for each case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umJ5BWMYbtmC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "dc38c0ed-0935-4a87-8d26-daa931b4abf0"
      },
      "source": [
        "picklist_tw_general = {\n",
        "    'categorization by transmission sys': ['0','Long-term Mobile', 'Macro', 'Outdoor Small Cells', 'Public DAS',\\\n",
        "                                           'Repeater', 'Transmission', 'w/o equipment']\n",
        "}\n",
        "pick_col_general = ['code', 'categorization by transmission sys']\n",
        "\n",
        "df_general_pick = check_picklist(towerdb, tw_index, tw_status, pick_col_general, picklist_tw_general)\n",
        "df_general_pick\n",
        "#no errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:272: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>site status</th>\n",
              "      <th>categorization by transmission sys</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A5321</td>\n",
              "      <td></td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>N4622</td>\n",
              "      <td></td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>N4862</td>\n",
              "      <td></td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>N6548</td>\n",
              "      <td></td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    code site status categorization by transmission sys\n",
              "0  A5321                                    Blank Value\n",
              "1  N4622                                    Blank Value\n",
              "2  N4862                                    Blank Value\n",
              "3  N6548                                    Blank Value"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTrLq5shA9AO"
      },
      "source": [
        "Fourth Check - Remove \"N/A\", \"0\" or \"-\" values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDmgn7M4A8dO"
      },
      "source": [
        "towerdb = replace_values(towerdb, col_order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e3gpaf6KufS"
      },
      "source": [
        "Fifth Check MoM Sites (BTS, decomissoned...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jgeXm_3PADC"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "path_uip = '/content/UserInput_Romania.xlsx'\n",
        "uip_names = ['Site_ID','Site Categorization', 'BTS site applicable charge (Annual)',\\\n",
        "             'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip = pd.read_excel(path_uip ,sheet_name='SiteLevel',usecols=[0,1,2,3],skiprows=2)\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [str(i) for i in msa[msa_index]]\n",
        "tw_sites = [str(i) for i in towerdb[tw_index]]\n",
        "uip_sites = [str(i) for i in uip['Site_ID']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKLJPDH0S3Ld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae5fb05-ec90-49dd-95ee-1b808d2cee5d"
      },
      "source": [
        "df_mom_bts = check_mom_bts(actives, tw_index,tw_status, tw_bts, msa, msa_index, msa_bts)\n",
        "# No one error df_mom_bts\n",
        "\n",
        "decomiss = towerdb[towerdb[tw_decom]=='Yes']\n",
        "df_mom_decom = check_mom_bts(decomiss, tw_index, tw_status, tw_decom, msa, msa_index, msa_decom)\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n",
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqO-e_WvWHr0"
      },
      "source": [
        "Check Picklist and dates formats for In service sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti-SjgXhU5Sr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "aba34237-e94e-4988-8c73-1b33faf0ac7c"
      },
      "source": [
        "picklis_dict = {\n",
        "    'categorization by transmission sys': ['Long-term Mobile', 'Macro', 'Outdoor Small Cells', 'Public DAS',\\\n",
        "                                           'Repeater', 'Transmission', 'w/o equipment'],\n",
        "    'categorization by site type': ['DAS passive','GBT','RTT', 'Outdoor Small Cells'],\n",
        "    'sites_as_metered_estimated': ['Estimated Model','Metered Model'],\n",
        "    'infrastructure ready (existing)/ to be ready (new)': ['Yes', 'No'],\n",
        "    'air conditioning': ['No','Yes'],\n",
        "    'bts sites': ['Yes', 'No'],\n",
        "    'strategic sites': ['Yes', 'No'],\n",
        "    'strategic_site_bucket': ['Non Strategic'],\n",
        "    'critical site': ['Yes', 'No'],\n",
        "    '10% critical': ['Beyond 10%', 'Within 10%','Non Critical'],\n",
        "    'wip_site': ['Yes', 'No'],\n",
        "    'bundled sites': ['Yes', 'No'],\n",
        "    'decommissioned sites': ['Yes', 'No']\n",
        "}\n",
        "\n",
        "picklist_cols = ['Code','Categorization by Transmission Sys','Categorization by Site Type','Sites_As_Metered_Estimated',\\\n",
        "                'Infrastructure ready (existing)/ to be ready (new)','Air Conditioning','bts sites','Strategic Sites',\\\n",
        "                'Strategic_Site_Bucket','Critical site','10% Critical','Wip_Site','Bundled sites','Decommissioned sites']\n",
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "df_in_service_picklist = check_picklist(actives, tw_index,tw_status, lower_str(picklist_cols), picklis_dict)\n",
        "df_in_service_picklist\n",
        "#Tem error em 4 colunas "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:272: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>site status</th>\n",
              "      <th>categorization by site type</th>\n",
              "      <th>infrastructure ready (existing)/ to be ready (new)</th>\n",
              "      <th>air conditioning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2287</th>\n",
              "      <td>N6258</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Incorret picklist value: YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2288</th>\n",
              "      <td>N6289</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: DAS Passive</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Incorret picklist value: YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2289</th>\n",
              "      <td>N6529</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Ok!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2290</th>\n",
              "      <td>N852</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Incorret picklist value: YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2291</th>\n",
              "      <td>N9700</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Ok!</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Incorret picklist value: YES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2292 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       code  ...              air conditioning\n",
              "0         1  ...                           Ok!\n",
              "1         5  ...                           Ok!\n",
              "2         6  ...                           Ok!\n",
              "3        11  ...                           Ok!\n",
              "4        12  ...                           Ok!\n",
              "...     ...  ...                           ...\n",
              "2287  N6258  ...  Incorret picklist value: YES\n",
              "2288  N6289  ...  Incorret picklist value: YES\n",
              "2289  N6529  ...                           Ok!\n",
              "2290   N852  ...  Incorret picklist value: YES\n",
              "2291  N9700  ...  Incorret picklist value: YES\n",
              "\n",
              "[2292 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P92ob_0t8ahp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "e599bf1e-476b-49e6-da54-7a0564ac744f"
      },
      "source": [
        "#check dates in columns\n",
        "start_dates = ['first_active_sharing_start_date', 'first_active_sharing_end_date']\n",
        "\n",
        "date_parser(actives, start_dates, \"%d/%m/%Y\", 'no')\n",
        "#actives['First_Active_Sharing_End_Date'] = actives['First_Active_Sharing_End_Date'].fillna('')\n",
        "in_service_cols = [tw_index, 'first_active_sharing_start_date', 'first_active_sharing_end_date']\n",
        "df_in_service_dates = check_date_columns(actives, tw_index,tw_status, in_service_cols, 2)\n",
        "df_in_service_dates\n",
        "# Varias linhas em branco"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:167: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>site status</th>\n",
              "      <th>first_active_sharing_start_date</th>\n",
              "      <th>first_active_sharing_end_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1634</th>\n",
              "      <td>N6258</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1635</th>\n",
              "      <td>N6289</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1636</th>\n",
              "      <td>N6529</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1637</th>\n",
              "      <td>N852</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1638</th>\n",
              "      <td>N9700</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1639 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       code  ... first_active_sharing_end_date\n",
              "0         1  ...                   Blank Value\n",
              "1         5  ...                   Blank Value\n",
              "2         6  ...                   Blank Value\n",
              "3        11  ...                   Blank Value\n",
              "4        12  ...                   Blank Value\n",
              "...     ...  ...                           ...\n",
              "1634  N6258  ...                   Blank Value\n",
              "1635  N6289  ...                   Blank Value\n",
              "1636  N6529  ...                   Blank Value\n",
              "1637   N852  ...                   Blank Value\n",
              "1638  N9700  ...                   Blank Value\n",
              "\n",
              "[1639 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnjM9tBg_T3u"
      },
      "source": [
        "Fifth Check BTS Flagged(Billing Trigger and Commercial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMuKKxOQ_GDe"
      },
      "source": [
        "actives_1 = towerdb[towerdb[tw_status]=='In Service']\n",
        "status = 'Yes'\n",
        "df_bts_flagged = check_tw_bill_doer(actives_1, tw_index, tw_bill, tw_bts, status, 'bill')\n",
        "df_bts_flagged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIk8H4EHCFgK"
      },
      "source": [
        "Usar as datas no formato Datetime para rodar esse check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMY_QvNfBj_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3084dd50-2965-47a7-978e-67843e1851ee"
      },
      "source": [
        "#actives_1 = towerdb[towerdb[tw_status]=='In Service']\n",
        "try:\n",
        "    new_sites, bts_out_uip, df_bts_errors = check_new_sites(towerdb, tw_index, tw_bts, tw_bill,tw_status,\\\n",
        "                                                        msa_sites, tw_sites, uip_sites)\n",
        "except:\n",
        "    print('No errors Founded!')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No errors Founded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3-ETlMrDIc9"
      },
      "source": [
        "Check WIP Flagged sites\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK_EajFZDe9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a41ccc-35c4-4630-d02b-eef91fadd39e"
      },
      "source": [
        "# Pode ter errors nos Codes por ficarem em int no momento que roda\n",
        "try:    \n",
        "    wip_out_tw_list, df_wip_and_bts_flagged = check_wip(towerdb,tw_index, tw_status, tw_wip, tw_bts, msa, msa_index, msa_wip)\n",
        "except:\n",
        "    print('No errors Founded!')\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No errors Founded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMXe7u9QGApN"
      },
      "source": [
        "Check Decomissioned sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaBUKjONFrwo"
      },
      "source": [
        "df_decom_sites = check_decommissioned(towerdb, tw_index,tw_status, tw_decom, tw_doer)\n",
        "\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV7zVfu4GIvZ"
      },
      "source": [
        "Check Doer columns for in service sites\n",
        "\n",
        "Should not to be in past or different of blank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RPXbD-FHSNu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "dba870e5-fd13-48bc-8eeb-c888a85000e3"
      },
      "source": [
        "#Coluna Doer tem valores fora do formato\n",
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "df_doer = check_tw_bill_doer(actives, tw_index, tw_doer, tw_status, 'In Service', 'doer')\n",
        "df_doer\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>site status</th>\n",
              "      <th>date_of_equipment_removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>702</th>\n",
              "      <td>2395</td>\n",
              "      <td>In Service</td>\n",
              "      <td>31/03/2021</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     code site status date_of_equipment_removal\n",
              "702  2395  In Service                31/03/2021"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ2sZAchIsyE"
      },
      "source": [
        "BTS sites are in subsequent Month"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAnIM7eEH8Ub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19ea899-6e0d-4274-8b8f-ea91af808684"
      },
      "source": [
        "#Retorna os sites novos\n",
        "df_bts_out = check_bts(towerdb, tw_bts, tw_index, tw_status, msa, msa_bts, msa_index)\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No errors founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcBDakUFKwun"
      },
      "source": [
        "*Tenth* - Check UIP Towerdb matches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch405uueK0UG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f064ef-7f6e-4914-a52b-14c3adea2b3b"
      },
      "source": [
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    uis_sites_not_in_towerdb = []\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "    if uis_sites_not_in_towerdb:\n",
        "        uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS In Month not active in TowerDB!'])\n",
        "        uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS In Month not active in TowerDB!',\\\n",
        "                                        right_on=tw_index)\n",
        "        uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS In Month not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS In Month!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS In Month!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS In Month!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    if decom_col != \"\":\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "        decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "    \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        \n",
        "        critical = []\n",
        "        if len(bts_tw_critical) > 0:\n",
        "            if set(uip_critical).intersection(bts_tw_critical):\n",
        "                critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "                #print(critical)\n",
        "        if critical:\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIPS File', status_tw_col]]\n",
        "        #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "        return uis_sites_not_in_towerdb, in_service_not_in_uis, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        critical = []\n",
        "        if len(uip_critical) > 0:\n",
        "            critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                        right_on=tw_index)\n",
        "        critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "        #if not (in_service_uip_sites.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "        return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "        \n",
        "try: \n",
        "    in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical = check_uip_tw(towerdb,tw_index, tw_status, \\\n",
        "                                                              tw_decom, tw_bts,tw_critical, \\\n",
        "                                                              uip, uip_sites, 'ro')\n",
        "except:\n",
        "    print('\\nNo errors founded!\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No errors founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA0Tlm-gM8-9"
      },
      "source": [
        "Commercial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-IxBhePM8fl"
      },
      "source": [
        "path_uis = '/content/UserInput_Romania_20210731.xlsx'\n",
        "path_before = '/content/UserInput_Romania_20210630.xlsx'\n",
        "commercial_diffs = check_diffs_v2(path_uis, path_before, '')\n",
        "commercial_diffs\n",
        "#Erro na Coluna Param2 Vazio no anterior preenchido no atual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx0hHrtCT-Pg"
      },
      "source": [
        "Verificar errors nas colunas "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5q850ISz-_t"
      },
      "source": [
        "    elif type_file == 'csv':\n",
        "        _actual = pd.read_csv(path_current, engine='python').fillna('')\n",
        "        print(_actual.columns.to_list())\n",
        "        _before = pd.read_csv(path_last, engine='python', encoding='windows-1252').fillna('')\n",
        "        _before.columns = lower_str(list(_before.columns))\n",
        "        print(_before.columns.to_list())\n",
        "        df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "        df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "        return df_final[(_actual != _before).any(1)]\n",
        "        \"\"\"if not df_final.empty:\n",
        "            #return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "        else:\n",
        "            print('\\nNo differences Founded!\\n')\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUsKTgjPTSef"
      },
      "source": [
        "Creating Excel Log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK5lkFQHS14-",
        "outputId": "6c99e8a8-87c2-4334-ae2b-689d22b2ba88"
      },
      "source": [
        "df_list = [df_cols, actives_dates_errors, df_general_pick, df_in_service_picklist, df_in_service_dates, commercial_diffs]\n",
        "sheetnames = ['Missing Columns','Actives Sites Date Errors','General Picklist Errors', 'In Service Sites Picklist Errors', 'In Service Sites Blank Dates', 'Commercial differences']\n",
        "\n",
        "path = '/content/towerdb_ro_true_up_errors.xlsx'\n",
        "general_log_erros(df_list, sheetnames, path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/openpyxl/workbook/child.py:102: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
            "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwurYhXodJDj"
      },
      "source": [
        "Creating CSV File\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5EcZ8kYdMLm"
      },
      "source": [
        "#Columns to parser\n",
        "dates_parser_cols = [tw_bill,'first_active_sharing_start_date', 'first_active_sharing_end_date', tw_doer]\n",
        "date_parser(towerdb, dates_parser_cols, \"%d/%m/%Y\")\n",
        "\n",
        "#Sempre fazer o replace depois de todas as modificações\n",
        "towerdb = replace_values(towerdb, lower_str(col_order))\n",
        "\n",
        "towerdb.round(2).to_csv('/content/TowerDB_Romenia_20210731_renan.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goCqcbeIUO2E"
      },
      "source": [
        "TA Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccRAXkvYUTa0"
      },
      "source": [
        "pathtw = '/content/TowerDB_Romania_20210731.xlsx'\n",
        "sheet= 'Tenant Template'\n",
        "skipr = 7\n",
        "skipc = 2\n",
        "ta = read_files(pathtw, sheet, skipr, skipc, tw_index)\n",
        "\n",
        "ta_cols = ['Code', 'Current Annual Fee per Tenant (Annual Hosting (with Discount))']  \n",
        "\n",
        "#ta['Importe anual'] = ta['Importe anual'].astype(str)\n",
        "\n",
        "df_ta_amount = check_amounts(ta, 'Code', ta_cols)\n",
        "# No errors\n",
        "\n",
        "df_ta_dates = check_lc_ta_dates(ta,'Code', 'Starting date', 'Expiring date')\n",
        "#NO erros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eriIx08fqyu1"
      },
      "source": [
        "ta_bills = ['Code', 'Tenant Name', 'Classification','Starting date', 'Expiring date', 'expiring date after renewal']\n",
        "pathta = '/content/TowerDB_Romania_20210731.xlsx'\n",
        "sheet= 'Tenant Template'\n",
        "skipr = 7\n",
        "skipc = 2\n",
        "pathold = '/content/TA_Input_Romania_20210630.csv'\n",
        "ta_save = '/content/TA_RO'\n",
        "new_name = 'TA_Romania_20210731.xlsx'\n",
        "old_name = 'TA_Input_Romania_20210630.csv'\n",
        "dates = ['starting date', 'expiring date', 'expiring date after renewal']\n",
        "find_diffs_between_files(pathold, pathta, 'Code',ta_bills,ta_save, old_name, new_name, type_file='mix',dates_cols=dates, status_col='',kind='ta',kind_col='Tenant Name',sheetname=sheet, skipr=7, skipc=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NHt-fWdFtej"
      },
      "source": [
        "(In Months Comparisons)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbCRVCeYFooT"
      },
      "source": [
        "ta_bills = ['Code', 'Tenant Name', 'Classification','Starting date', 'Expiring date', 'expiring date after renewal']\n",
        "inmonth_ta = '/content/TA_Input_Romania_20210731.csv'\n",
        "pathold = '/content/TA_Input_Romania_20210630.csv'\n",
        "ta_save = '/content/TA_RO'\n",
        "new_name = 'TA_Input_Romania_20210731.csv'\n",
        "old_name = 'TA_Input_Romania_20210630.csv'\n",
        "dates = ['starting date', 'expiring date', 'expiring date after renewal']\n",
        "find_diffs_between_files(pathold, inmonth_ta, 'Code',ta_bills,ta_save, old_name, new_name, type_file='csv',dates_cols=dates, status_col='',kind='ta',kind_col='Tenant Name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT9hGSeU45JV",
        "outputId": "7afcce32-f557-46a8-a222-063bccb724a1"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', dates_cols=[], status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col] \n",
        "            #df.columns = lower_str(list(df.columns))\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "        t_col = type_dates.lower()\n",
        "        if format == 1:\n",
        "            type_date = \"%d/%m/%Y\"\n",
        "        else:\n",
        "            type_date = \"%d-%m-%Y\"\n",
        "        for column in lower_str(columns):\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,engine='python').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, engine='python').fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD) header=0, names=cols_old,\n",
        "        df_OLD = pd.read_csv(path_OLD, engine='python').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols, 1, 'mixed')\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "uis_inmonth = '/content/UserInput_Romania.xlsx'\n",
        "uis_old = '/content/UserInput_Romania_20210630.xlsx'\n",
        "sheet = 'SiteLevel'\n",
        "uis_index = 'Site_ID (Alphanumeric, Alphabetical or Numeric)'\n",
        "to_uis = '/content/RO_UIS_SiteLevel'\n",
        "old_uis = 'UserInput_Romania_20210630.xlsx'\n",
        "new_uis = 'UserInput_Romania.xlsx'\n",
        "bill = []\n",
        "find_diffs_between_files(uis_old, uis_inmonth, uis_index, bill, to_uis, old_uis,new_uis,'excel',dates_cols=[],status_col='',\\\n",
        "                         kind='',kind_col='', sheetname=sheet, skipr=2, skipc=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['681']\n",
            "Dropped Rows: ['A6056', 'A4394', 'A856', 'N4590', 'N4861', 'N5033', 'E4013', 'E3012', 'E4036', 'E2806', 'N4962', 'N6509', 'N4598', '1633', 'N4652', 'N4551', 'A4112', 'N4523', 'N5353', 'A2320']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}