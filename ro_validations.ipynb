{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ro_validations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHib8yqpVrBS8hcRDbcULj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emerenan/xlsx_validation/blob/main/ro_validations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI87yjm0BP3x"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook\n",
        "\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index):\n",
        "    \"\"\"\n",
        "    Params:\\n\n",
        "    path: parth of file in the computer.\\n\n",
        "    n_skiprows: Number of rows to delete in the original file,.\\n\n",
        "    columns_to_convert: Columns to convert the data general type. \\n\n",
        "    n_skipcolumn: Columns to skip in the original file. \\n\n",
        "    endrow = pass 0 to read everything, 1 to count entire\n",
        "    columns_order: List of columns names in specific order to pass in the engine.\\n\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    \"\"\"cont = 0\n",
        "    for i in df.iloc[:,site_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[:cont, :]\n",
        "    #df.columns = columns_order\"\"\"\n",
        "    \n",
        "    # convert intery columns to integer \n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].fillna(0)\n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].astype('int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "def check_df(df, error_msg):\n",
        "    if df == None or df.empty:\n",
        "        return f'{error_msg}'\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def count_duplicates(lista):\n",
        "    count_dict = {}\n",
        "    for entry in lista:\n",
        "        if entry in count_dict.keys():\n",
        "            count_dict[entry] += 1\n",
        "        else:\n",
        "            count_dict[entry] = 1\n",
        "    \n",
        "    duplicates = {}\n",
        "    for k, v in count_dict.items():\n",
        "        if v > 1:\n",
        "            duplicates[k] = v\n",
        "    return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "def defining_df(df, column_range, number_col):\n",
        "    cont = 0\n",
        "    for i in df.iloc[:,number_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[0:cont, :]\n",
        "    return df\n",
        "\n",
        "#old version\n",
        "def check_columns(table, output_columns):\n",
        "    \"\"\"\n",
        "    Check the total of number of missing columns and the missing columns in passed table.\\n\n",
        "\n",
        "    Params:\\n\n",
        "    table: contain the columns to be check.\\n\n",
        "    output_columns: columns structure at the final file. \n",
        "\n",
        "    Returns:\\n\n",
        "    Number of missing columns and a list that contains the name os missing columns.\n",
        "    \"\"\"\n",
        "    \"\"\"\"\n",
        "    countries = ['DE':{'towerDB':[],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':[]}\\,\n",
        "                 'HU',\n",
        "                 'IE',\n",
        "                 'RO',\n",
        "                 'PT',\n",
        "                 'ES',\n",
        "                 'CZ': {'towerDB':[\"Code (Duplicate)\",\"Site Status\",\"VF - In scope / out of scope (Generalised scoping)\",\"Site in Skylon scope Actual (From Site List Sheet )\",\"Legacy Site Code(Duplicate)\",\"TIMS Site Code\",\"Legacy Site Code\",\"Site Name\",\"Macro Region\",\"Province\",\"Municipality\",\"Inhabitants\",\"Address\",\"Ground Register\",\"Altitude\",\"Latitude\",\"Longitude\",\"Categorization by Inhabitants\",\"Categorization by Transmission Sys\",\"Categorization by Site Type\",\"Categorization by Transmission Sys (subcluster)\",\"Other internal Categorization 1 (Identify ACQ Sites)\",\"Other internal Categorization 2 Energy provider (Eon/ LL)\",\"DAS+Macro\",\"DAS (Yes/ No)\",\"DAS Ownership (Complete/ Partial/ 3rd Party)\",\"Active/ Passive DAS\",\"# of remote units/ radiating points\",\"Type of Structure\",\"Distance highest antenna to ground level\",\"GBT Tower height\",\"POD ID\",\"Energy Consumption LTM (kwh)\",\"Annual Energy cost LTM (Euros)\",\"Infrastructure ready (existing)/ to be ready (new)\",\"Infrastructure to be dismantled by\",\"Radio equipments to be deactivated by\",\"Infrastructure to be shared by\",\"Technology VOD\",\"Fibre / Microwave\",\"Vertical passive structure owner\",\"Room configuration (detailed)\",\"Shelter passive structure ownership\",\"Type of Air Conditioning\",\"Number of cabinets (Full Capacity)\",\"Number of Antenna (Full Capacity)\",\"Number of MW (Full Capacity)\",\"Counterpart\",\"# of Lease Contracts\",\"Current annual lease fees \",\"Current other fees (Maintenance)\",\"Current other fees\",\"(Average) residual duration - Lease contract\",\"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\"# of Tenants Agreements\",\"Current Total Annual Hosting Fees\",\"Tenant (name/ID) MNO1 (Česká telekomunikační infrastruktura a.s.)\",\"Annual Fee per Tenant MNO1\",\"Annual Energy Fee MNO1\",\"Annual Maintenance Fee MNO1\",\"Other Services Fee MNO1\",\"Residual duration MNO1 (Years)\",\"Tenant (name/ID) MNO2 (T-Mobile Czech Republic a.s.)\",\"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\"Annual Maintenance Fee MNO2\",\"Other Services Fee MNO2\",\"Residual duration MNO2 (days)\",\"Tenant (name/ID) MNO3\",\"Annual Fee per Tenant MNO3\",\"Annual Energy Fee MNO3\",\"Annual Maintenance Fee MNO3\",\"Other Services Fee MNO3\",\"Residual duration MNO3\",\"# of OTMOs\",\"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\"Annual Maintenance Fee OTMOs\",\"Other Services Fee OTMOs\",\"Average residual duration (days)\",\"Check\",\"Strategic Macro Sites\",\"Critical Sites\",\"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\"Transmission Sites\",\"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\"VF-CZ Demerger phase\",\"EVO Location [FAR Site ID] \",\"Billing Trigger date \",\\\n",
        "                 \"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\"Wip_Site\",\"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', 'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', 'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index','Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', 'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', 'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month','Updated Item', 'Comment', 'Counterpart_extra_1', 'Counterpart_extra_2', 'x','SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count', 'not_def_1']}\\,\n",
        "                 }\n",
        "                 'GR']\n",
        "                 \"\"\"\n",
        "    total_received = len(table.columns)\n",
        "    number_missing_columns = 0\n",
        "    missing_columns = []\n",
        "    #Counting of missing columns       \n",
        "    #if country in contries:      \n",
        "    for columns in output_columns:\n",
        "        if columns.lower() not in [labels.lower() for labels in table]:\n",
        "            number_missing_columns +=1\n",
        "            missing_columns.append(columns)\n",
        "    \n",
        "    return total_received, number_missing_columns, missing_columns\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = [i for i in df.columns]\n",
        "    col_miss = [ i for i in bill_cols if i not in twdb_col]\n",
        "    for i in bill_cols:\n",
        "        if i not in twdb_col:\n",
        "            col_miss.append(i)\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a', 0, '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "      \n",
        "def date_parser(df, columns, format, type_dates):\n",
        "    t_col = type_dates.lower()\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Refactorar esse codigo para receber todas as colunas num dic\n",
        "# Sendo as keys=columns e values= picklist for each column\n",
        "def check_date_columns(df, df_index, columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                if date_format.match(df_dates.loc[de_site,de_column]) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "                    else:\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            return df_de\n",
        "        else: \n",
        "            print('No one columns with incorrect date format!')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                if date_format.match(df_dates.loc[de_site,de_column]) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "                    else:\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            return df_de\n",
        "        else: \n",
        "            print('No one columns with incorrect date format!')\n",
        "\n",
        "def check_amounts(df_check, df_index, columns, pattern):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            #print(f'{df_dates.loc[de_site,df_index]}: {df_dates.loc[de_site,de_column]}')\n",
        "            if pattern == 1:\n",
        "                if not str(df.loc[site,column]).__contains__('.'):\n",
        "                    #print(str(df.loc[site,column]))\n",
        "                    if df.loc[site,df_index] not in df_new.index:\n",
        "                        df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                        df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                    else:\n",
        "                        df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "            else:\n",
        "                if not str(df.loc[site,column]).__contains__(','):\n",
        "                    #print(str(df.loc[site,column]))\n",
        "                    if df.loc[site,df_index] not in df_new.index:\n",
        "                        df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                        df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                    else:\n",
        "                        df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "        \n",
        "def check_picklist(df,df_index,df_cols, picklist_dict):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        df:\n",
        "        df_index: \n",
        "        picklist_dict:\n",
        "        nultiIndex: More than one column to check\n",
        "    \"\"\"\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in picklist_dict.keys(): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            col_values = [i.lower() for i in picklist_dict[column]]\n",
        "            if not value.lower() in col_values or pd.isnull(value):\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "                else:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    #df = df_errors.dropna()   \n",
        "    df = df_errors.dropna(how='all', axis=1)   \n",
        "    return df\n",
        "\n",
        "def check_picklist_3(df,df_index, picklist_dict):\n",
        "    log = {}\n",
        "    for column in picklist_dict:\n",
        "        df_aux = df.copy()\n",
        "        new = df_aux[column].isin(picklist_dict[column])\n",
        "        #new = df_aux[column].apply(lambda x: x in picklist_dict[column])\n",
        "        # Aceita somento os valores que não estão na picklist\n",
        "        indexes = df.index[new == False].tolist()\n",
        "        #if len(indexes)>0:\n",
        "        if column not in log:log[column]=[]\n",
        "        log[column]=log[column]+indexes\n",
        "    #print(log.keys())\n",
        "\n",
        "    newDict ={}\n",
        "    df1 = pd.DataFrame()\n",
        "    for key,value in log.items():\n",
        "        for val in value:  \n",
        "            ID=df.iloc[val][df_index]\n",
        "            if ID in newDict:\n",
        "                newDict[ID].append(key)\n",
        "            else:\n",
        "                newDict[ID] = [key]\n",
        "        \n",
        "    logs = pd.DataFrame.from_dict(newDict, orient='index')\n",
        "    return logs\n",
        "\n",
        "def check_on_air_sites(df, df_index, df_cols):\n",
        "\n",
        "    df_check = df[df_cols]\n",
        "    df_check['sites'] = df_check[df_index]\n",
        "    df_check = df_check.set_index('sites')\n",
        "    #df_check = replace_values(df_check, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in df_cols: \n",
        "            if df_check.loc[site,df_index] not in df_errors.index:\n",
        "                df_errors.loc[site,df_index] = df_check.loc[site,df_index]\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "            else:\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    df = df_errors[df_errors.iloc[:,1:]]\n",
        "    df = df.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    #df = df_errors.dropna(how='all', axis=1)   \n",
        "    return df\n",
        "\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [str(i) for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uip = [str(i) for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index].sort_values() if i not in uip_list]\n",
        "    bts_out_uip = pd.DataFrame(bts_out_uip, columns=['Bts_Sites_Out_UIP_File'])\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "\n",
        "    return new_sites, bts_out_uip, df_site_bts[[tw_index, bts_col, bill_col]]\n",
        "    \"\"\"Restruturar o script em CZ, DE, PT\"\"\"\n",
        "\n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    return df\n",
        "\n",
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[wip_tw]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, wip_tw, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "    \"\"\"Reestrurar os script em PT, DE, CZ\"\"\"\n",
        "    # Falta os outros países\n",
        "\n",
        "def check_decommissioned(df,df_index, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    return filtered[[df_index, decom_col, doer_col]]\n",
        "    \"\"\"Ajustar para CZ, DE, PT\"\"\"\n",
        "    \n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]') < current_date)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "        else:\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]').empty)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "    \"\"\"Ajustar para CZ, DE, PT, IE\"\"\"\n",
        "\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col, status_col):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)]\n",
        "        return filtered[[tw_index, status_col, doer_col]]  \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "                                                              \n",
        "def check_mom_bts(df_tw, tw_index, tw_col, df_msa,msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    return filtered[[tw_index, tw_col]]         \n",
        "\n",
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "\n",
        "        filtered = df[df[start_date] > df[end_date]]\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "    t1 = ['pt', 'de', 'cz', 'ie', 'es', 'ro']\n",
        "    if country.lower() in t1:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                    df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "        critical = []\n",
        "        if len(uip_critical) > 0:\n",
        "            if set(uip_critical).intersection(bts_tw_critical):\n",
        "                critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "\n",
        "        return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "        decomiss_sites_in_uip = []\n",
        "        if set(tw_decomiss).intersection(uip_sites):\n",
        "            decomiss_sites_in_uip = [i for i in tw_decomiss if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(decomiss_sites_in_uip, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip = [i for i in df_uip['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Yes'][tw_index]\n",
        "        critical = []\n",
        "        if set(uip).intersection(bts_tw_critical):\n",
        "            critical = [i for i in bts_tw_critical if i not in uip]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% out in UIP File'])\n",
        "\n",
        "        return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    #uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    #uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=cols_ordered)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    return df_commercial_diffs\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() "
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYeRvgVAJfS_"
      },
      "source": [
        "towerdb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-9L3MsJTt2V"
      },
      "source": [
        "path_tw = '/content/TowerDB_Romania_20210731.xlsx'\n",
        "sheet = 'Enhanced towerDB'\n",
        "\n",
        "towerdb = read_files(path_tw, sheet, 0, 0, 'Code')\n",
        "\n",
        "col_order = ['Scope','Phase _1/_2','Code','Site Name','Macro Region','Region','Province','Municipality','Inhabitants','Address',\\\n",
        "             'Altitude','Latitude','Longitude','Categorization by Transmission Sys','Active or passive DAS',\\\n",
        "             'Unused sites (TowerCo holds the property rights but does not host any Operator Equipment or Other Customer equipment )',\\\n",
        "             'Non-Vodafone equipment Sites (TowerCo holds the property rights and does not host any Operator Equipment but hosts Other Customer equipment  )',\\\n",
        "             'PowerOff Sites','Decommissioned sites','Categorization by Transmission Sys (sub-cluster)','Core Type',\\\n",
        "             'Macro Site - Transmission Hub Site with/Transmission Hub Site without Shelters','Transmission sites – with/Transmission Site without shelters.','Room Configuration',\\\n",
        "             'Power Supply','Air Conditioning','Active Sharing Arrangements involving the Operator','Transmission Hub sites’','Antenna Positions','Weight of RRUs',\\\n",
        "             'Total power transmitted per site ','Categorization by Transmission Sys_1','Categorization by Site Type','Categorization by inhabitants',\\\n",
        "             'Rural/ Suburban/ Urban','No_1','Technology VOD','Fiber / Microwave','Type of Structure','Tower Height','Floor space','POD ID','Energy Provider',\\\n",
        "             'Energy cost FYTD_DEC_20','Energy consumption FYTD_DEC_20','Infrastructure ready (existing)/ to be ready (new)','Billing Trigger Date','No_2',\\\n",
        "             'Radio equipments to be deactivated by','Infrastructure to be shared by','Counterpart','# of Lease Contracts','Current annual lease fees  ',\\\n",
        "             'Current energy lease fees ','Current annual other fees ','Total Annual Lease','Sub-lease','(Average) residual duration','Maturity Cluster',\\\n",
        "             'ExCo rep. Avg Annual Lease costs','Total Energy Cost (Energy provider + LL)','VOD (y/n)','TLK','Annual Hosting TLK','Annual Hosting TLK (with Discount)',\\\n",
        "             'Annual Energy TLK','Annual Maintenance Fee TLK','Other Services Fee TLK','Total Revenues TLK','Total Revenues TLK (with Discount)','Residual duration TLK',\\\n",
        "             'Maturity Clusters TLK','Orange Total (without active sharing)','Orange(Transmission)','Orange(Indoor)','Orange Passive Shared Sites ',\\\n",
        "             'Annual Hosting ORANGE (Excl. active sharing)','Annual Hosting ORANGE (Excl. active sharing) (with Discount)',\\\n",
        "             'Annual Energy ORANGE(Excl. active sharing)','Annual Maintenance Fee ORANGE (Excl. active sharing)','Other Services Fee ORANGE (Excl. active sharing)',\\\n",
        "             'Total Revenues ORANGE(Excl. active sharing)','Total Revenues ORANGE(Excl. active sharing) (with Discount)','Residual duration ORANGE(Excl. active sharing)',\\\n",
        "             'Maturity Clusters ORANGE(Excl. active sharing)','RCS&RDS','Annual Hosting RCS&RDS','Annual Hosting RCS&RDS (with Discount)','Annual Energy RCS&RDS',\\\n",
        "             'Annual Maintenance Fee RCS&RDS','Other Services Fee RCS&RDS','Total Revenues RCS&RDS','Total Revenues RCS&RDS (with Discount)','Residual duration RCS&RDS',\\\n",
        "             'Maturity Clusters RCS&RDS','OTMO','Annual Hosting OTMOs','Annual Hosting OTMOs (with Discount)','Annual Energy OTMOs','Annual Maintenance Fee OTMOs',\\\n",
        "             'Other Services Fee OTMOs','Total Revenues OTMOs','Total Revenues OTMOs (with Discount)','Residual duration OTMOs','Maturity Clusters OTMOs',\\\n",
        "             'Total # of 3rd Party Tenants (excluding active sharing)','Annual Fee from 3rd Party Tenants','Annual Fee from 3rd Party Tenants (with Discount)',\\\n",
        "             'Annual Energy Fee from 3rd Party Tenants','Annual Maintenance Fee from 3rd Party Tenants','Other Services Fee from 3rd Party Tenants',\\\n",
        "             'Total Hosting Fee & Services from 3rd Party Tenants','Total Hosting Fee & Services from 3rd Party Tenants (with Discount)',\\\n",
        "             'Waighted Average residual duration','Macro Cluster Tenancy','Final Cluster Type of Contract','Macro Cluster 1','Sites w/ at list a DDS (Lease Contract Type)',\\\n",
        "             '# of Tenants','Categorization by Tenant combination','Categorization by Type of Passive contracts','Type of contract (principle contract)',\\\n",
        "             'Cluster Type of contract (principle contract)','Easement (Servitù di passaggio)','Final Cluster','Turistic sites','TLK_1','Orange','RCS&RDS_1','OTMOs',\\\n",
        "             'Orange Active Shared Sites ','Annual Hosting ORANGE','No_3','Annual Energy ORANGE','No_4','Total Revenues ORANGE','Only - Active Sharing Orange',\\\n",
        "             'Total # of 3rd Party Tenants (including active sharing)','Total 3rd Party Tenants hosting revenues (including active sharing)','FSO Criticality',\\\n",
        "             'TX LH POC','10% Critical','EVO SAP ID ','Enhanced BBU','Diameter\\nVodafone Antenna','Diameter\\n(Orange rural & Unilateranl ',\\\n",
        "             'Total diameter of microwave and mmWave antennas','Is the Site Under Construction','indicative completion date',\\\n",
        "             'Capital expenditure incurred to the MSA Effective Date','indicative capital expenditure required to complete the site build',\\\n",
        "             'proposed configuration for the completed site',\\\n",
        "             'Details of the Operator Equipment installed at the Site (including details of the Standard Configuration Attributes set out in Schedule 7 (Standard Configuration)',\\\n",
        "             'Details of the access arrangements applicable at each Site (including any access restrictions and applicable public access requirements)',\\\n",
        "             'Is the Existing Configuration is within the applicable Standard Configuration?','Bundled sites','Strategic Sites','Unilateral Orange Transmission Site',\\\n",
        "             'First 10 Unilateral Orange Transmission Site','Details of any Orange Equipment at Orange Shared Sites ','Transfer_Date_Of_Registration_Required_Sites',\\\n",
        "             'Wip_Site','Bts_Site','Sites_As_Metered_Estimated','Strategic_Site_Bucket','Subsequent_Sharing_Arrangement','First_Active_Sharing_Start_Date',\\\n",
        "             'First_Active_Sharing_End_Date','Sites_Within_500_Macro_Sites','Date_Of_Equipment_Removal','RFAI ( Ready For Active Installation )','X','Site Status',\\\n",
        "             'Critical site']\n",
        "\n",
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = 'Code'\n",
        "tw_doer = 'Date_Of_Equipment_Removal'\n",
        "tw_status = 'Site Status'\n",
        "tw_bts = 'Bts_Site'\n",
        "tw_bill = 'Billing Trigger Date'\n",
        "tw_wip = 'Wip_Site'\n",
        "tw_decom = 'Decommissioned sites'\n",
        "tw_critical = 'Critical site'\n",
        "\n",
        "path_msa = '/content/TowerDB_Romania_20210630.csv'\n",
        "msa = pd.read_csv(path_msa, encoding='latin')\n",
        "msa.columns = col_order\n",
        "msa_index = 'Code'\n",
        "msa_doer = 'Date_Of_Equipment_Removal'\n",
        "msa_status = 'Site Status'\n",
        "msa_bts = 'Bts_Site'\n",
        "msa_bill = 'Billing Trigger Date'\n",
        "msa_wip = 'Wip_Site'\n",
        "msa_decom = 'Decommissioned sites'\n",
        "msa_critical = 'Critical site'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsU54KiZ3qaE"
      },
      "source": [
        "towerdb.rename({'VDF ENERGY COST EUR': 'Energy cost FYTD_DEC_20', 'VDF ENERGY Consum kWh': 'Energy consumption FYTD_DEC_20'}, axis=1, inplace=True)\n",
        "towerdb = towerdb.reindex(columns=col_order)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRyJlck4cn50"
      },
      "source": [
        "\"\"\"Check Columns Received\"\"\"             \n",
        "df_cols = check_columns_received(towerdb, col_order)\n",
        "#No columns missing\n",
        "df_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM4xIb3-8GlD"
      },
      "source": [
        "First Check - Dates Formats (dd/mm/YYYY)\n",
        "\n",
        "Columns: Date of equipment removal (from MAR´21)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8anMHTJbc8vS"
      },
      "source": [
        "\"\"\"You need to convert all values in cols for string format to check\"\"\"\n",
        "# Columns to functions\n",
        "dates_doer = [tw_index, tw_doer]\n",
        "#Columns to parser\n",
        "bill=[tw_bill]\n",
        "doer=[tw_doer]\n",
        "\n",
        "#actives_1 = towerdb[towerdb[tw_status]=='In Service']\n",
        "no_actives_1 = towerdb[towerdb[tw_status]=='Dismantled']\n",
        "\n",
        "date_parser(towerdb, bill, \"%d/%m/%Y\", 'no')\n",
        "date_parser(no_actives_1, doer, \"%d/%m/%Y\", 'mixed')\n",
        "\n",
        "\n",
        "#Checking columns for errors\n",
        "# actives_dates_errors = check_date_columns(actives_1, tw_index, dates_bill, 2) \n",
        "# Actives sites with blank billing trigger date\n",
        "no_actives_dates_errors = check_date_columns(no_actives_1, tw_index, dates_doer, 2) \n",
        "no_actives_dates_errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go_Rn-GV9KIO"
      },
      "source": [
        "Second Check - TW Amount value General (xxx.xx)\n",
        "\n",
        "Column(s): ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuBSPpAV9J1W"
      },
      "source": [
        "amount_cols = [tw_index, \"\"\"???\"\"\"]\n",
        "df_amount_errors = check_amounts(actives, tw_index, amount_cols, 1)\n",
        "#No one error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSv0C2ur9NVi"
      },
      "source": [
        "Thirth - Check Picklist values All sites\n",
        "\n",
        "Do this check in all sites\n",
        "\n",
        "Check the picklist for each case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umJ5BWMYbtmC"
      },
      "source": [
        "picklist_tw_general = {\n",
        "    'Categorization by Transmission Sys': ['0','Long-term Mobile', 'Macro', 'Outdoor Small Cells', 'Public DAS',\\\n",
        "                                           'Repeater', 'Transmission', 'w/o equipment']\n",
        "}\n",
        "pick_col_general = ['Code', 'Categorization by Transmission Sys']\n",
        "\n",
        "df_general_pick = check_picklist(towerdb, tw_index, pick_col_general, picklist_tw_general)\n",
        "df_general_pick\n",
        "#no errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTrLq5shA9AO"
      },
      "source": [
        "Fourth Check - Remove \"N/A\", \"0\" or \"-\" values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDmgn7M4A8dO"
      },
      "source": [
        "towerdb = replace_values(towerdb, col_order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e3gpaf6KufS"
      },
      "source": [
        "Fifth Check MoM Sites (BTS, decomissoned...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jgeXm_3PADC"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "path_uip = '/content/UserInput_Romania.xlsx'\n",
        "uip_names = ['Site_ID','Site Categorization', 'BTS site applicable charge (Annual)',\\\n",
        "             'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip = pd.read_excel(path_uip ,sheet_name='SiteLevel',usecols=[0,1,2,3],skiprows=2)\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [str(i) for i in msa[msa_index]]\n",
        "tw_sites = [str(i) for i in towerdb[tw_index]]\n",
        "uip_sites = [str(i) for i in uip['Site_ID']]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKLJPDH0S3Ld"
      },
      "source": [
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "df_mom_bts = check_mom_bts(actives, tw_index, tw_bts, msa, msa_index, msa_bts)\n",
        "# No one error df_mom_bts\n",
        "\n",
        "decomiss = towerdb[towerdb[tw_decom]=='Yes']\n",
        "df_mom_decom = check_mom_bts(decomiss, tw_index, tw_decom, msa, msa_index, msa_decom)\n",
        "#No errors"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqO-e_WvWHr0"
      },
      "source": [
        "Check Picklist and dates formats for In service sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti-SjgXhU5Sr"
      },
      "source": [
        "picklis_dict = {\n",
        "    'Categorization by Transmission Sys': ['Long-term Mobile', 'Macro', 'Outdoor Small Cells', 'Public DAS',\\\n",
        "                                           'Repeater', 'Transmission', 'w/o equipment'],\n",
        "    'Categorization by Site Type': ['DAS passive','GBT','RTT', 'Outdoor Small Cells'],\n",
        "    'Sites_As_Metered_Estimated': ['Estimated Model','Metered Model'],\n",
        "    'Infrastructure ready (existing)/ to be ready (new)': ['Yes', 'No'],\n",
        "    'Air Conditioning': ['No','Yes'],\n",
        "    'Bts_Site': ['Yes', 'No'],\n",
        "    'Strategic Sites': ['Yes', 'No'],\n",
        "    'Strategic_Site_Bucket': ['Non Strategic'],\n",
        "    'Critical site': ['Yes', 'No'],\n",
        "    '10% Critical': ['Within 10%','Non Critical'],\n",
        "    'Wip_Site': ['Yes', 'No'],\n",
        "    'Bundled sites': ['Yes', 'No'],\n",
        "    'Decommissioned sites': ['Yes', 'No']\n",
        "}\n",
        "\n",
        "picklist_cols = ['Code','Categorization by Transmission Sys','Categorization by Site Type','Sites_As_Metered_Estimated',\\\n",
        "                'Infrastructure ready (existing)/ to be ready (new)','Air Conditioning','Bts_Site','Strategic Sites',\\\n",
        "                'Strategic_Site_Bucket','Critical site','10% Critical','Wip_Site','Bundled sites','Decommissioned sites']\n",
        "\n",
        "df_in_service_picklist = check_picklist(actives, tw_index, picklist_cols, picklis_dict)\n",
        "#Tem error em 3 colunas "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P92ob_0t8ahp"
      },
      "source": [
        "#check dates in columns\n",
        "start_dates = ['First_Active_Sharing_Start_Date', 'First_Active_Sharing_End_Date']\n",
        "\n",
        "date_parser(actives, start_dates, \"%d/%m/%Y\", 'no')\n",
        "#actives['First_Active_Sharing_End_Date'] = actives['First_Active_Sharing_End_Date'].fillna('')\n",
        "in_service_cols = ['Code', 'First_Active_Sharing_Start_Date', 'First_Active_Sharing_End_Date']\n",
        "df_in_service_dates = check_date_columns(actives, tw_index, in_service_cols, 2)\n",
        "df_in_service_dates\n",
        "# Varias linhas em branco"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnjM9tBg_T3u"
      },
      "source": [
        "Fifth Check BTS Flagged(Billing Trigger and Commercial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMuKKxOQ_GDe"
      },
      "source": [
        "actives_1 = towerdb[towerdb[tw_status]=='In Service']\n",
        "status = 'Yes'\n",
        "df_bts_flagged = check_tw_bill_doer(actives_1, tw_index,tw_bill, tw_bts, status, 'bill')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIk8H4EHCFgK"
      },
      "source": [
        "Usar as datas no formato Datetime para rodar esse check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMY_QvNfBj_d"
      },
      "source": [
        "#actives_1 = towerdb[towerdb[tw_status]=='In Service']\n",
        "new_sites, bts_out_uip, df_bts_errors = check_new_sites(towerdb, tw_index, tw_bts, tw_bill, msa_sites, tw_sites, uip_sites)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3-ETlMrDIc9"
      },
      "source": [
        "Check WIP Flagged sites\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK_EajFZDe9H"
      },
      "source": [
        "# Pode ter errors nos Codes por ficarem em int no momento que roda\n",
        "wip_out_tw_list, df_wip_and_bts_flagged = check_wip(towerdb,tw_index, tw_wip, tw_bts, msa, msa_index, msa_wip)\n",
        "# No errors"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMXe7u9QGApN"
      },
      "source": [
        "Check Decomissioned sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaBUKjONFrwo"
      },
      "source": [
        "df_decom_sites = check_decommissioned(towerdb, tw_index, tw_decom, tw_doer)\n",
        "df_decom_sites\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV7zVfu4GIvZ"
      },
      "source": [
        "Check Doer columns for in service sites\n",
        "\n",
        "Should not to be in past or different of blank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RPXbD-FHSNu"
      },
      "source": [
        "#Coluna Doer tem valores fora do formato\n",
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "df_doer = check_tw_bill_doer(actives, tw_index, tw_doer, tw_bts, 'Yes', 'doer')\n",
        "df_doer\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ2sZAchIsyE"
      },
      "source": [
        "BTS sites are in subsequent Month"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAnIM7eEH8Ub"
      },
      "source": [
        "#Retorna os sites novos\n",
        "df_bts_out = check_bts(towerdb, tw_bts, tw_index, msa, msa_bts, msa_index)\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcBDakUFKwun"
      },
      "source": [
        "*Tenth* - Check UIP Towerdb matches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch405uueK0UG"
      },
      "source": [
        "in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical = check_uip_tw(towerdb,tw_index, tw_status, \\\n",
        "                                                              tw_decom, tw_bts,tw_critical, \\\n",
        "                                                              uip, uip_sites, 'ro')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA0Tlm-gM8-9"
      },
      "source": [
        "Commercial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-IxBhePM8fl"
      },
      "source": [
        "\n",
        "path_before = '/content/UserInput_Romania_20210630.xlsx'\n",
        "names = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Data_Type','Input_Value_1','Input_Value_2',\\\n",
        "        'Description/Instruction', 'Frequency of Update']\n",
        "merge_cols = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', \\\n",
        "              'Description/Instruction', 'Frequency of Update']\n",
        "cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Data_Type','Input_Value_1_actual',\\\n",
        "                'Input_Value_1_before','Input_Value_2_actual','Input_Value_2_before','Equal Values',\\\n",
        "                'Description/Instruction', 'Frequency of Update']\n",
        "replace_values=['Input_Value_1','Input_Value_2']\n",
        "df_commercial_diffs = check_commercial(path_uip, path_before, replace_values, names, merge_cols, cols_ordered)\n",
        "df_commercial_diffs\n",
        "#Erro na Coluna Param2 Vazio no anterior preenchido no atual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUsKTgjPTSef"
      },
      "source": [
        "Creating Excel Log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK5lkFQHS14-",
        "outputId": "2eb36afc-10bf-413c-ef78-29f7a09bc842"
      },
      "source": [
        "df_list = [df_in_service_picklist, df_in_service_dates]\n",
        "sheetnames = ['Sites with Picklist Error', 'Incorrect date format(In Service)']\n",
        "\n",
        "path = '/content/towerdb_ro_errors.xlsx'\n",
        "general_log_erros(df_list, sheetnames, path)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/openpyxl/workbook/child.py:102: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
            "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goCqcbeIUO2E"
      },
      "source": [
        "TA Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccRAXkvYUTa0"
      },
      "source": [
        "pathtw = '/content/TowerDB_Romania_20210731.xlsx'\n",
        "sheet= 'Tenant Template'\n",
        "skipr = 7\n",
        "skipc = 2\n",
        "ta = read_files(pathtw, sheet, skipr, skipc, tw_index)\n",
        "\n",
        "ta_cols = ['Code', 'Current Annual Fee per Tenant (Annual Hosting (with Discount))']  \n",
        "\n",
        "#ta['Importe anual'] = ta['Importe anual'].astype(str)\n",
        "\n",
        "df_ta_amount = check_amounts(ta, 'Code', ta_cols, 1)\n",
        "# No errors\n",
        "\n",
        "df_ta_dates = check_lc_ta_dates(ta,'Code', 'Starting date', 'Expiring date')\n",
        "#NO erros"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}