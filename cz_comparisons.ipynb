{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cz_comparisons.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwIdwAc3iGEuA/MfAVedJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emerenan/xlsx_validation/blob/main/cz_comparisons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXmIHnYMdaUO"
      },
      "source": [
        "!pip install unidecode\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "from unidecode import unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-PK7UUdeRmK"
      },
      "source": [
        "TowerDB Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMJc-FPsFGF6"
      },
      "source": [
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save,old_file,new_file, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    import re\n",
        "    import xlsxwriter\n",
        "    from openpyxl import Workbook, styles\n",
        "    from openpyxl.styles import PatternFill, Font\n",
        "    from openpyxl.styles.differential import DifferentialStyle\n",
        "    from openpyxl.formatting.rule import Rule\n",
        "\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        #df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = path_OLD\n",
        "        df_OLD = fit_df(df_OLD)\n",
        "\n",
        "        df_NEW = path_NEW\n",
        "        df_NEW = fit_df(df_NEW)\n",
        "        \"\"\"df_NEW[index_col] = df_NEW[index_col].astype(str)\n",
        "        df_NEW = df_NEW.drop_duplicates(subset=index_col)\n",
        "        df_NEW['sites'] = df_NEW[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_NEW = df_NEW.set_index('sites').fillna('')\"\"\"\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "        status_1 = f'{status}_current'\n",
        "        status_2 = f'{status}_before'\n",
        "        new_copy = new_copy.set_index('sites')\n",
        "        new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save}_{old_file} vs {new_file}.xlsx'\n",
        "    #file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    file = pd.ExcelWriter(fname, engine='xlsxwriter')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "    \n",
        "    file.save()\n",
        "\n",
        "    # get openpyxl objects\n",
        "    from openpyxl import load_workbook\n",
        "\n",
        "    wb  = load_workbook(fname)\n",
        "    ws = wb['diffs_founded']\n",
        "    # get openpyxl objects\n",
        "    \"\"\"wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\"\"\"\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ20000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "        \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ20000', header_style)\n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index, cols_number, col_dates, format='normal', type_date=\"%d/%m/%Y\"):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ']\n",
        "        #lista = []\n",
        "\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(0)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df\n",
        "\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    date_parser(df, col_dates, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_number)\n",
        "\n",
        "    for col in cols_number:\n",
        "        df[col] = df[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "    \n",
        "    df = df.fillna('')\n",
        "\n",
        "    return df\n",
        "\n",
        "path_towerdb = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "towerdb_col = [\"Code (Duplicate)\",\"Site Status\",\"VF - In scope / out of scope (Generalised scoping)\",\"Site in Skylon scope Actual (From Site List Sheet )\",\\\n",
        "               \"Legacy Site Code(Duplicate)\",\"TIMS Site Code\",\"Legacy Site Code\",\"Site Name\",\"Macro Region\",\"Province\",\"Municipality\",\"Inhabitants\",\"Address\",\\\n",
        "               \"Ground Register\",\"Altitude\",\"Latitude\",\"Longitude\",\"Categorization by Inhabitants\",\"Categorization by Transmission Sys\",\\\n",
        "               \"Categorization by Site Type\",\"Categorization by Transmission Sys (subcluster)\",\"Other internal Categorization 1 (Identify ACQ Sites)\",\\\n",
        "               \"Other internal Categorization 2 Energy provider (Eon/ LL)\",\"DAS+Macro\",\"DAS (Yes/ No)\",\"DAS Ownership (Complete/ Partial/ 3rd Party)\",\\\n",
        "               \"Active/ Passive DAS\",\"# of remote units/ radiating points\",\"Type of Structure\",\"Distance highest antenna to ground level\",\"GBT Tower height\",\\\n",
        "               \"POD ID\",\"Energy Consumption LTM (kwh)\",\"Annual Energy cost LTM (Euros)\",\"Infrastructure ready (existing)/ to be ready (new)\",\\\n",
        "               \"Infrastructure to be dismantled by\",\"Radio equipments to be deactivated by\",\"Infrastructure to be shared by\",\"Technology VOD\",\"Fibre / Microwave\",\\\n",
        "               \"Vertical passive structure owner\",\"Room configuration (detailed)\",\"Shelter passive structure ownership\",\"Type of Air Conditioning\",\\\n",
        "               \"Number of cabinets (Full Capacity)\",\"Number of Antenna (Full Capacity)\",\"Number of MW (Full Capacity)\",\"Counterpart\",\"# of Lease Contracts\",\\\n",
        "               \"Current annual lease fees \",\"Current other fees (Maintenance)\",\"Current other fees\",\"(Average) residual duration - Lease contract\",\\\n",
        "               \"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\"# of Tenants Agreements\",\\\n",
        "               \"Current Total Annual Hosting Fees\",\"Tenant (name/ID) MNO1 (Česká telekomunikační infrastruktura a.s.)\",\"Annual Fee per Tenant MNO1\",\\\n",
        "               \"Annual Energy Fee MNO1\",\"Annual Maintenance Fee MNO1\",\"Other Services Fee MNO1\",\"Residual duration MNO1 (Years)\",\\\n",
        "               \"Tenant (name/ID) MNO2 (T-Mobile Czech Republic a.s.)\",\"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\"Annual Maintenance Fee MNO2\",\\\n",
        "               \"Other Services Fee MNO2\",\"Residual duration MNO2 (days)\",\"Tenant (name/ID) MNO3\",\"Annual Fee per Tenant MNO3\",\"Annual Energy Fee MNO3\",\\\n",
        "               \"Annual Maintenance Fee MNO3\",\"Other Services Fee MNO3\",\"Residual duration MNO3\",\"# of OTMOs\",\"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\\\n",
        "               \"Annual Maintenance Fee OTMOs\",\"Other Services Fee OTMOs\",\"Average residual duration (days)\",\"Check\",\"Strategic Macro Sites\",\"Critical Sites\",\\\n",
        "               \"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\"Transmission Sites\",\\\n",
        "               \"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\"VF-CZ Demerger phase\",\\\n",
        "               \"EVO Location [FAR Site ID] \",\"Billing Trigger date \",\"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\\\n",
        "               \"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\"Wip_Site\",\\\n",
        "               \"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"]\n",
        "towerdb_col = lower_str(towerdb_col)\n",
        "\n",
        "dates = ['Billing Trigger date ', \"Infrastructure ready (existing)/ to be ready (new)\", \"Infrastructure to be dismantled by\",\\\n",
        "         'First_Active_Sharing_Start_Date','First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "         'Date_Of_Equipment_Removal']\n",
        "num_parse = [\"Inhabitants\", \"Current annual lease fees \",\"(Average) residual duration - Lease contract\",\\\n",
        "             \"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\\\n",
        "             \"# of Tenants Agreements\",\"Current Total Annual Hosting Fees\", \"Annual Fee per Tenant MNO1\",\\\n",
        "             \"Annual Energy Fee MNO1\", \"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\\\n",
        "             \"Residual duration MNO2 (days)\", \"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\\\n",
        "             \"Other Services Fee OTMOs\",\"Average residual duration (days)\"]\n",
        "\n",
        "tab = 'Tower DB'\n",
        "row = 3\n",
        "towerdb = read_files(path_towerdb,tab, 3, 0,\"Code (Duplicate)\",num_parse,dates)\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "towerdb = towerdb[towerdb_col]\n",
        "\n",
        "path_msa = '/content/TowerDB_CzechRepublic_20210731 (3).csv'\n",
        "msa = pd.read_csv(path_msa,encoding='ISO-8859-1').fillna('')\n",
        "msa_cols = lower_str(list(msa.columns))\n",
        "msa.columns = msa_cols\n",
        "msa = msa.rename(columns={'ï»¿code (duplicate)': 'code (duplicate)'})\n",
        "\n",
        "bill_col= ['Code (Duplicate)',\\\n",
        "            \"Site Status\",\\\n",
        "            \"Categorization by Transmission Sys\",\\\n",
        "            \"Categorization by Site Type\",\\\n",
        "            'Strategic Macro Sites',\\\n",
        "            'Critical Sites',\\\n",
        "            'Case A Core Site',\\\n",
        "            'Macro Site - Transmission Hub Site',\\\n",
        "            'Macro Site - Transmission Hub Site with/without Shelters',\\\n",
        "            'Transmission Sites',\\\n",
        "            'Room Configuration',\\\n",
        "            'Power Supply',\\\n",
        "            'Air Conditioning',\\\n",
        "            'Active Sharing Arrangements involving the Operator',\\\n",
        "            'VF-CZ Demerger phase',\\\n",
        "            \"Billing Trigger date \",\\\n",
        "            'Strategic_Site_Bucket',\\\n",
        "            'Critical Site Bucket',\\\n",
        "            'Wip_Site',\\\n",
        "            'Bts_Site',\\\n",
        "            'Sites_As_Metered_Estimated',\\\n",
        "            'Subsequent_Sharing_Arrangement',\\\n",
        "            \"First_Active_Sharing_Deployment_Type\",\\\n",
        "            \"First_Active_Sharing_Start_Date\",\\\n",
        "            \"First_Active_Sharing_End_Date\",\\\n",
        "            \"Decommissioned_Site\",\\\n",
        "            \"Transfer_Date_Of_Consent_Required_Sites\",\\\n",
        "            \"Date_Of_Equipment_Removal\"]\n",
        "\n",
        "tw_save = '/content/CZ_TW'\n",
        "old_n = 'twdb_20210831 (2).csv'\n",
        "new_n = 'twdb_celfinet_20210831.csv'\n",
        "\n",
        "\n",
        "find_diffs_between_files(msa, towerdb, 'Code (Duplicate)', bill_col, \\\n",
        "                             \"Site Status\", tw_save,old_n,new_n, type_file='else',kind='tw')\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvNojQ9vePh0"
      },
      "source": [
        "def find_diffs_between_files_1(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix',status_col='', \\\n",
        "                             kind='tw', dates_cols=[], kind_col='',type_date=\"%d/%m/%Y\", sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col] \n",
        "            #df.columns = lower_str(list(df.columns))\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            \"\"\"for col in cols_conv:\n",
        "                df[col] = df[col].astype(str).apply(unidecode)\"\"\"\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df = df.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "            #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']]\n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    #cols_conv = lower_str(cols_conv)\n",
        "    dates_cols = lower_str(dates_cols)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        date_parser(df_OLD, dates_cols,format , type_date)\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols,format , type_date)\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD) header=0, names=cols_old,\n",
        "        df_OLD = pd.read_csv(path_OLD,encoding='ISO-8859-1').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col,cols_conv, kind, kind_col)\n",
        "        for col in ['province','municipality', 'address']:\n",
        "            df_OLD[col] = df_OLD[col].apply(unidecode)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,encoding='ISO-8859-1').fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,cols_conv, kind, kind_col)\n",
        "        for col in ['province','municipality', 'address']:\n",
        "            df_NEW[col] = df_NEW[col].apply(unidecode)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD) header=0, names=cols_old,\n",
        "        df_OLD = pd.read_csv(path_OLD,encoding='ISO-8859-1').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "        for col in ['province','municipality', 'address']:\n",
        "            df_OLD[col] = df_OLD[col].apply(unidecode)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        #print(df_NEW.columns)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols,format , type_date)\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "        for col in ['province','municipality', 'address']:\n",
        "            df_NEW[col] = df_NEW[col].apply(unidecode)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                            end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "bill_col= ['Code (Duplicate)',\\\n",
        "            \"Site Status\",\\\n",
        "            \"Categorization by Transmission Sys\",\\\n",
        "            \"Categorization by Site Type\",\\\n",
        "            'Strategic Macro Sites',\\\n",
        "            'Critical Sites',\\\n",
        "            'Case A Core Site',\\\n",
        "            'Macro Site - Transmission Hub Site',\\\n",
        "            'Macro Site - Transmission Hub Site with/without Shelters',\\\n",
        "            'Transmission Sites',\\\n",
        "            'Room Configuration',\\\n",
        "            'Power Supply',\\\n",
        "            'Air Conditioning',\\\n",
        "            'Active Sharing Arrangements involving the Operator',\\\n",
        "            'VF-CZ Demerger phase',\\\n",
        "            \"Billing Trigger date \",\\\n",
        "            'Strategic_Site_Bucket',\\\n",
        "            'Critical Site Bucket',\\\n",
        "            'Wip_Site',\\\n",
        "            'Bts_Site',\\\n",
        "            'Sites_As_Metered_Estimated',\\\n",
        "            'Subsequent_Sharing_Arrangement',\\\n",
        "            \"First_Active_Sharing_Deployment_Type\",\\\n",
        "            \"First_Active_Sharing_Start_Date\",\\\n",
        "            \"First_Active_Sharing_End_Date\",\\\n",
        "            \"Decommissioned_Site\",\\\n",
        "            \"Transfer_Date_Of_Consent_Required_Sites\",\\\n",
        "            \"Date_Of_Equipment_Removal\"]\n",
        "\n",
        "pathtw = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "pathold = '/content/TowerDB_CzechRepublic_20210731 (3).csv'\n",
        "tw_save = '/content/CZ_TW'\n",
        "tab = 'Tower DB'\n",
        "row = 3\n",
        "old_n = 'twdb_20210831 (2).csv'\n",
        "new_n = 'twdb_celfinet_20210831.csv'\n",
        "cols_con = ['Province','Municipality','Address']\n",
        "dates = ['Billing Trigger date ','infrastructure ready (existing)/ to be ready (new)',\\\n",
        "         'First_Active_Sharing_Start_Date',\\\n",
        "         'First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "         'Date_Of_Equipment_Removal']  \n",
        "\n",
        "find_diffs_between_files_1(pathold, pathtw, 'Code (Duplicate)', bill_col,tw_save, old_n,new_n,'mix',\\\n",
        "                           status_col=\"Site Status\" , kind='tw', dates_cols=dates , kind_col='', sheetname=tab, skipr = row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4154LwReeL_Y"
      },
      "source": [
        "# TA Input Comparisons\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ7xNrfQNpZ7"
      },
      "source": [
        "CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBOQombqmQSB"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW,new_file,old_file, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', kind_col='', k='', am_cols = [],\\\n",
        "                             cols_int=[], cols_date=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "     \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "    \n",
        "    def date_parser(df, column,format, type_date):\n",
        "        if format == 'mix':\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "            return df[column]\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "        ta_cols = lower_str(ta_cols)\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=ta_cols)\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD = df_OLD.iloc[natsort.index_humansorted(df_OLD['tenant agreement id - new'])]\n",
        "        \n",
        "        #Ordenando as duplicatas\n",
        "        old_lista=[]\n",
        "        dico = {}\n",
        "        for id in df_OLD['tenant agreement id - new']:\n",
        "            nome = str(id)\n",
        "            if nome not in dico.keys():\n",
        "                dico[nome] = 1\n",
        "                n = nome+f'_{dico[nome]}'\n",
        "                old_lista.append(n)\n",
        "            else:\n",
        "                dico[nome] += 1\n",
        "                n = nome+f'_{dico[nome]}'\n",
        "                old_lista.append(n)\n",
        "        print(old_lista)\n",
        "        df_OLD['sites'] = old_lista\n",
        "        print(df_OLD['sites'])\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD['site name']+df_OLD['count of contracts'].astype(str)+df_OLD['service type'].astype(str)\n",
        "        df_OLD = df_OLD.fillna('')\n",
        "        for in_col in ['counterpart','annual amount in czk', 'amount in eur', 'index', 'counterpart_1']:\n",
        "            df_OLD[in_col] = df_OLD[in_col].apply(unidecode)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "\n",
        "        \"\"\"New File\"\"\"\n",
        "        df_NEW = pd.read_csv(path_NEW, header=0, names=ta_cols)\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_NEW = df_NEW.iloc[natsort.index_humansorted(df_NEW['tenant agreement id - new'])]\n",
        "        \n",
        "        #Ordenando as duplicatas\n",
        "        new_lista=[]\n",
        "        dic = {}\n",
        "        for id in df_NEW['tenant agreement id - new']:\n",
        "            nome = str(id)\n",
        "            if nome not in dic.keys():\n",
        "                dic[nome] = 1\n",
        "                n = nome+f'_{dic[nome]}'\n",
        "                new_lista.append(n)\n",
        "            else:\n",
        "                dic[nome] += 1\n",
        "                n = nome+f'_{dic[nome]}'\n",
        "                new_lista.append(n)\n",
        "        print(new_lista)\n",
        "        df_NEW['sites'] = new_lista\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD['site name']+df_OLD['count of contracts'].astype(str)+df_OLD['service type'].astype(str)\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "        for in_col in ['counterpart','annual amount in czk', 'amount in eur', 'index', 'counterpart_1']:\n",
        "            df_NEW[in_col] = df_NEW[in_col].apply(unidecode)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "\n",
        "    else:\n",
        "        ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "        ta_cols = lower_str(ta_cols)\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=ta_cols)\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD = df_OLD.iloc[natsort.index_humansorted(df_OLD['tenant agreement id - new'])]\n",
        "        \n",
        "        #Ordenando as duplicatas\n",
        "        old_lista=[]\n",
        "        dico = {}\n",
        "        for id in df_OLD['tenant agreement id - new']:\n",
        "            nome = str(id)\n",
        "            if nome not in dico.keys():\n",
        "                dico[nome] = 1\n",
        "                n = nome+f'_{dico[nome]}'\n",
        "                old_lista.append(n)\n",
        "            else:\n",
        "                dico[nome] += 1\n",
        "                n = nome+f'_{dico[nome]}'\n",
        "                old_lista.append(n)\n",
        "        print(old_lista)\n",
        "        df_OLD['sites'] = old_lista\n",
        "        print(df_OLD['sites'])\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD['site name']+df_OLD['count of contracts'].astype(str)+df_OLD['service type'].astype(str)\n",
        "        df_OLD = df_OLD.fillna('')\n",
        "        for in_col in ['counterpart','annual amount in czk', 'amount in eur', 'index', 'counterpart_1']:\n",
        "            df_OLD[in_col] = df_OLD[in_col].apply(unidecode)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "\n",
        "        col = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type',\\\n",
        "                'Contract start date', 'Contract end date', 'Status of contract',\\\n",
        "                'Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart',\\\n",
        "                'Classification of Tenant', 'Annual amount - billing currency',\\\n",
        "                'Billing Currency', 'Annual Amount in CZK', 'Amount in EUR',\\\n",
        "                'Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "                'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key',\\\n",
        "                'Classification', 'Residual Period in Years', 'Remarks',\\\n",
        "                'Count of unique key', 'Count of contracts', 'Scoping classification',\\\n",
        "                'DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "                'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x',\\\n",
        "                'SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count','Quota of Unique Key','unamed']\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,header=0, names=col, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.reindex(columns=ta_cols)\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW = df_NEW.iloc[natsort.index_humansorted(df_NEW['tenant agreement id - new'])]\n",
        "        \n",
        "        #Ordenando as duplicatas\n",
        "        new_lista=[]\n",
        "        dicn = {}\n",
        "        for id in df_NEW['tenant agreement id - new']:\n",
        "            nome = str(id)\n",
        "            if nome not in dicn.keys():\n",
        "                dicn[nome] = 1\n",
        "                n = nome+f'_{dicn[nome]}'\n",
        "                new_lista.append(n)\n",
        "            else:\n",
        "                dicn[nome] += 1\n",
        "                n = nome+f'_{dicn[nome]}'\n",
        "                new_lista.append(n)\n",
        "        print(new_lista)\n",
        "        df_NEW['sites'] = new_lista\n",
        "        #df_NEW['sites'] = df_NEW[index_col].astype(str)+df_NEW[kind_col].astype(str)+df_NEW['site name']+df_NEW['count of contracts'].astype(str)+df_NEW['service type'].astype(str)\n",
        "        #print([i for i in list(df_NEW['sites']) if list(df_NEW['sites']).count(i)>1])\n",
        "        df_NEW = replace_values(df_NEW, am_cols, 0)\n",
        "        for col in am_cols:\n",
        "            #df[col] = df[col].fillna(0)\n",
        "            df_NEW[col] = df_NEW[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "\n",
        "        for col in cols_int:\n",
        "            df_NEW[col] = df_NEW[col].fillna(0)\n",
        "            df_NEW[col] = df_NEW[col].astype(int)\n",
        "\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "        for in_col in ['counterpart','annual amount in czk', 'amount in eur', 'index', 'counterpart_1']:\n",
        "            df_NEW[in_col] = df_NEW[in_col].apply(unidecode)\n",
        "        df_NEW = replace_values(df_NEW,bill_cols)\n",
        "        for i in cols_date:\n",
        "            df_NEW[i] = date_parser(df_NEW, i, 'mix', \"%d/%m/%Y\")\n",
        "        df_NEW = df_NEW.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "        #print(df_NEW.info())\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "\n",
        "        try:\n",
        "            df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        except:\n",
        "            print('falha')\n",
        "        #print(list(df_NEW.columns)==(df_OLD.columns))\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    \n",
        "    n = 10\n",
        "    lista = []\n",
        "    for site in new_copy['sites']:\n",
        "        chunks = [site[i:i+n] for i in range(0, len(site), n)]\n",
        "        lista.append(chunks[0])\n",
        "    new_copy['sites_code'] = lista\n",
        "    new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} {old_file} vs {new_file}.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                             end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "\n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    ws.delete_cols(2)\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "ta_cols = ['Tenant Agreement ID - NEW', 'Code','Contract start date', 'Contract end date','Counterpart',\\\n",
        "           'Classification of Tenant',]\n",
        "path_ta_input = '/content/TA_Input_CzechRepublic_20210831.csv'\n",
        "sheet = 'Tenant '\n",
        "ta_old = '/content/TA_Input_CzechRepublic_20210831 (2).csv'\n",
        "ta_save = '/content/CZ_TA'\n",
        "#find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, \"\", ta_save,'csv', 'ta')\n",
        "\n",
        "#cols_con_ta = ['Province','Municipality','Address']\n",
        "dates = lower_str(['Contract start date', 'Contract end date', 'Update in month'])\n",
        "amount_ta = lower_str(['Annual Amount in CZK', 'Amount in EUR'])\n",
        "interger = lower_str(['annual amount - billing currency','Residual Period in Years','Count of unique key', 'Count of contracts', 'Unique Tenant Count'])\n",
        "ta_bill_cols = lower_str(['Tenant Agreement ID - NEW','Contract start date','Contract end date',\\\n",
        "                'Counterpart','Classification of Tenant'])\n",
        "ta_nn='Celfinet_20210831.csv'\n",
        "ta_on='TA_20210831 (2).csv'\n",
        "\n",
        "find_diffs_between_files(ta_old, path_ta_input, ta_nn, ta_on, 'Code', ta_cols, status_col='', path_save=ta_save, type_file='csv',kind='ta',\\\n",
        "                         kind_col='tenant agreement id - new',k='service type',am_cols = amount_ta, cols_int=interger, cols_date=dates, sheetname=sheet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu2MukDCNrEd"
      },
      "source": [
        "Excel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwSGZdNDNPYc",
        "outputId": "808d5917-3f9b-4376-b26a-41fb32b85bd2"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', kind_col='', k='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        #Ajustado só para CZ_Julho, tirei o encoding\n",
        "        df_NEW = pd.read_csv(path_NEW).fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    else:\n",
        "        ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "        ta_cols = lower_str(ta_cols)\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=ta_cols, encoding='latin')\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #df_OLD['sites'] = [site if type(site) != str else len(df_OLD[index_col])+i for i, site in enumerate(df_OLD[index_col])]\n",
        "        #df_OLD = df_OLD.sort_values(by='sites')\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD['site name']+df_OLD['count of contracts'].astype(str)+df_OLD['service type'].astype(str)\n",
        "        \n",
        "\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "\n",
        "        col = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type',\\\n",
        "                'Contract start date', 'Contract end date', 'Status of contract',\\\n",
        "                'Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart',\\\n",
        "                'Classification of Tenant', 'Annual amount - billing currency',\\\n",
        "                'Billing Currency', 'Annual Amount in CZK', 'Amount in EUR',\\\n",
        "                'Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "                'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key',\\\n",
        "                'Classification', 'Residual Period in Years', 'Remarks',\\\n",
        "                'Count of unique key', 'Count of contracts', 'Scoping classification',\\\n",
        "                'DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "                'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x',\\\n",
        "                'SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count','Quota of Unique Key','unamed']\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,header=0, names=col, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.reindex(columns=ta_cols)\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW['sites'] = df_NEW[index_col].astype(str)+df_NEW[kind_col].astype(str)+df_NEW['site name']+df_NEW['count of contracts'].astype(str)+df_NEW['service type'].astype(str)\n",
        "        #print([i for i in list(df_NEW['sites']) if list(df_NEW['sites']).count(i)>1])\n",
        "\n",
        "        #print(df_NEW.info())\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "\n",
        "        try:\n",
        "            df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        except:\n",
        "            print('falha')\n",
        "        #print(list(df_NEW.columns)==(df_OLD.columns))\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "        status_1 = f'{status_col}_current'\n",
        "        status_2 = f'{status_col}_before'\n",
        "        new_copy = new_copy.set_index('sites')\n",
        "        new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} old_file vs new_file.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                             end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "\n",
        "    #print(len(newRows))\n",
        "    for site in new_copy[index_col]:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "ta_cols = ['Tenant Agreement ID - NEW', 'Code','Contract start date', 'Contract end date','Counterpart',\\\n",
        "           'Classification of Tenant',]\n",
        "path_ta_input = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "sheet = 'Tenant '\n",
        "ta_old = '/content/TA_Input_CzechRepublic_20210731.csv'\n",
        "ta_save = '/content/CZ_TA'\n",
        "old_ta = 'TA_old.csv'\n",
        "new_ta = 'TA_new.xlsx'\n",
        "\n",
        "find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, status_col='', path_save=ta_save, type_file='mix',kind='ta',\\\n",
        "                         kind_col='tenant agreement id - new',k='service type', sheetname=sheet)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['CBUD1Gnot available yetJHBUD0.0Lease', 'BOJVR5not available yetBOJVR0.0Lease', '111790not available yetPRSLA0.0Lease', '170140not available yetCKDOM0.0Lease', '750430not available yetA3CHE0.0Lease', '750120not available yetA3CHU0.0Lease']\n",
            "Dropped Rows: ['1610902400000063ULPOD1.0Energy', '1610902400000063ULPOD0.0Lease']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWDk2R5x-nin"
      },
      "source": [
        "# LC Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE4_AV_SQY6O"
      },
      "source": [
        "# Excel\n",
        "Fit some columns before run the comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW_ETZtpi5b2"
      },
      "source": [
        "def read_files(path, sheetname,col_name, n_skiprows, n_skip_columns, site_index, cols_date, cols_int, cols_amount, bill_cols, format='mix', type_date=\"%d/%m/%Y\"):\n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "    def date_parser(df, columns,format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    df = pd.read_excel(path, sheet_name = sheetname,header=0, names = col_name, engine='openpyxl', skiprows = n_skiprows)\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    df = replace_values(df, cols_amount, 0)\n",
        "    for col in cols_amount:\n",
        "        #df[col] = df[col].fillna(0)\n",
        "        df[col] = df[col].apply(lambda x: '{:0,.2f}'.format(x))\n",
        "    for i in bill_cols:\n",
        "        df[i] = df[i].replace(['N/A', 'n/a',\"0\", '-', np.nan,'nan', ' '], '')\n",
        "\n",
        "    df = df.fillna('')\n",
        "    #replace_values(df, bill_cols)\n",
        "    date_parser(df, cols_date, format, type_date)\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    return df\n",
        "\n",
        "def change_incorret(df, df_index, col_chg, incorrect_value, new_value):\n",
        "    list_sites = list(df[df[col_chg]==incorrect_value][df_index])\n",
        "    df.loc[df[df_index].isin(list_sites), col_chg] = new_value\n",
        "    #print(df.loc[df[df_index].isin(list_sites)][[df_index, col_chg]])\n",
        "\n",
        "col_names = ['Contract ID - NEW', 'FIN ID', 'Contract Crncy', 'Contract Type', 'Freq.', 'Freq. Unit', 'Indexation',\\\n",
        "         'Index upon request', 'Counterpart ID', 'Counterpart', 'LC Amount CZK\\nyearly', 'Amount in EUR yearly (Actual)', \\\n",
        "         'Payment terms Code', 'VAT Subject', '% of VAT', 'Contr. Start date', 'Contr. 1st End', 'Contr. Term End', \\\n",
        "         'Sublease Consession', 'Renewal Option', 'Unique Key', 'Count of Key', 'Count of ContrBct ', \\\n",
        "         'Remarks - Consistency check', 'Residual Period in years', 'Scoping classification', 'DAS sites', \\\n",
        "         'DAS ownership', '31_12_9999', 'Unique Key1', 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021', \\\n",
        "         'Check LC amount', 'Updated Item\\nAmount yearly', 'Contr. 1st End as of 31.05.2021', \\\n",
        "         'Contr. 1st End as of 30.06.2021', 'Check Contr. 1st End', 'Updated Item\\n1st End date', 'Comment', \\\n",
        "         'Comment date', 'Date']\n",
        "\n",
        "#ta = pd.read_csv(path_ta_input, header=0, names=col)\n",
        "dates_lc = ['Contr. Start date', 'Contr. 1st End','contr. term end', 'Contr. 1st End as of 31.05.2021', 'Contr. 1st End as of 30.06.2021']\n",
        "interger_lc = []\n",
        "amount_lc = [\"Amount in EUR yearly (Actual)\", 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021']\n",
        "lc_bill_cols = ['Contract ID - NEW','Counterpart']\n",
        "\n",
        "lc = read_files('/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx', 'Final data_Lease',col_names, 1, 0, 'FIN ID', \\\n",
        "                dates_lc, interger_lc,amount_lc, lc_bill_cols)\n",
        "\n",
        "col_order_lc = [\"Contract ID - NEW\",\"FIN ID\",\"Contract Crncy\",\"Contract Type\",\"Freq.\",\"Freq. Unit\",\"Indexation\",\\\n",
        "                \"Index upon request\",\"Counterpart ID\",\"Counterpart\",\"LC Amount CZK\\nyearly\",\"Amount in EUR yearly (Actual)\",\\\n",
        "                \"Payment terms Code\",\"VAT Subject\",\"% of VAT\",\"Contr. Start date\",\"Contr. 1st End\",\"Contr. Term End\",\\\n",
        "                \"Sublease Consession\",\"Renewal Option\",\"Unique Key\",\"Count of Key\",\"Count of ContrBct \",\\\n",
        "                \"Remarks - Consistency check\",\"Residual Period in years\",\"Scoping classification\",\"DAS sites\",\\\n",
        "                \"DAS ownership\",\"31_12_9999\",\"Unique Key1\",'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021',\\\n",
        "                \"Check LC amount\",\"Updated Item\\nAmount yearly\",'Contr. 1st End as of 31.05.2021','Contr. 1st End as of 30.06.2021',\\\n",
        "                \"Check Contr. 1st End\",\"Updated Item\\n1st End date\",\"Comment\",\"Comment date\",\"Date\"]\n",
        "lc = lc[col_order_lc]\n",
        "\n",
        "change_incorret(lc, \"Contract ID - NEW\", '% of VAT', 0.21,'21%')\n",
        "\n",
        "lc.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSvntPvkjROH",
        "outputId": "29411027-bd19-4ba8-ceeb-125666bfd210"
      },
      "source": [
        "lc_old = pd.read_csv('/content/LC_Input_CzechRepublic_20210731.csv').fillna('')\n",
        "lc_old.head(3)\n",
        "\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save,old_file,new_file, add_col='', type_file='mix', sheetname='', skipr=0, skipc=0):\n",
        "    import re\n",
        "    import xlsxwriter\n",
        "    from openpyxl import Workbook, styles\n",
        "    from openpyxl.styles import PatternFill, Font\n",
        "    from openpyxl.styles.differential import DifferentialStyle\n",
        "    from openpyxl.formatting.rule import Rule\n",
        "\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        #df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def number_add(df, index_col, add_col):\n",
        "        df = df.sort_values(by=[index_col, add_col], ascending=[True, False])\n",
        "        \"\"\"lista = []\n",
        "        df['sites'] = df[index_col].copy()\n",
        "        for i in range(len(df[index_col])):\n",
        "            if str(df.iloc[i][index_col]) in lista:            \n",
        "                df.iloc[i]['sites'] = str(df.iloc[i][index_col])+\"_\"+str(lista.count(str(df.iloc[i][index_col])))\n",
        "            else:\n",
        "                df.iloc[i]['sites'] = str(df.iloc[i][index_col])+\"_0\"    \n",
        "        lista.append(str(df_OLD.iloc[i][index_col]))\"\"\"\n",
        "        lista=[]\n",
        "        dico = {}\n",
        "        for id in df[index_col]:\n",
        "            nome = str(id)\n",
        "            if nome not in dico.keys():\n",
        "                dico[nome] = 1\n",
        "                n = nome+f'_{dico[nome]}'\n",
        "                lista.append(n)\n",
        "            else:\n",
        "                dico[nome] += 1\n",
        "                n = nome+f'_{dico[nome]}'\n",
        "                lista.append(n)\n",
        "\n",
        "        return lista\n",
        "        \n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = path_OLD\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD[index_col] = df_OLD[index_col].astype(str)\n",
        "        #df_OLD['sites'] = df_OLD[index_col]+df_OLD['counterpart']    \n",
        "        df_OLD['sites'] = number_add(df_OLD, index_col, add_col)\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "        #df_OLD = fit_df(df_OLD)\n",
        "\n",
        "        df_NEW = path_NEW\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW[index_col] = df_NEW[index_col].astype(str)\n",
        "        #df_NEW['sites'] = df_NEW[index_col]+df_NEW['counterpart']\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_NEW['sites'] = number_add(df_NEW, index_col, add_col)\n",
        "        #df_NEW = fit_df(df_NEW)\n",
        "        df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    new_copy = new_copy.reset_index()\n",
        "    print(new_copy)\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save}_{old_file} vs {new_file}.xlsx'\n",
        "    #file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "    \n",
        "    \"\"\"file.save()\n",
        "\n",
        "    # get openpyxl objects\n",
        "    from openpyxl import load_workbook\n",
        "\n",
        "    wb  = load_workbook(fname)\n",
        "    ws = wb['diffs_founded']\"\"\"\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ20000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "        \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ20000', header_style)\n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "lc_bills = ['Contract ID - NEW','Counterpart', 'LC Amount CZK\\nyearly', 'Contr. Start date', 'Contr. 1st End']\n",
        "lc_save = '/content/CZ_LC'\n",
        "old_name = '20210731.csv'\n",
        "new_name = 'VF CZ_v1.xlsx'\n",
        "find_diffs_between_files(lc_old, lc, 'Contract ID - NEW', lc_bills, \\\n",
        "                         lc_save,old_name,new_name,'counterpart',type_file='else')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             sites  ... lc amount as of 30.04.2021\n",
            "0     1100000000_1  ...                           \n",
            "1     1100000001_1  ...                           \n",
            "2     1100000002_1  ...                           \n",
            "3     1100000006_1  ...                           \n",
            "4     1100000006_2  ...                           \n",
            "...            ...  ...                        ...\n",
            "4674  1100007693_1  ...                           \n",
            "4675  1100007696_1  ...                           \n",
            "4676  1100007698_1  ...                           \n",
            "4677  1100007699_1  ...                           \n",
            "4678  1100007702_1  ...                           \n",
            "\n",
            "[4679 rows x 45 columns]\n",
            "\n",
            "New Rows:     ['1100000238_2', '1100000440_2', '1100001041_2', '1100001587_2', '1100002195_2', '1100002342_2', '1100002615_2', '1100002641_2', '1100004334_2', '1100007625_1', '1100007696_1', '1100007698_1', '1100007699_1', '1100007702_1']\n",
            "Dropped Rows: ['1100002084_1', '1100004297_1', '1100004297_2']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukJlrT0UqDsO"
      },
      "source": [
        "# Old version\n",
        "More difficult to run cause I need to fit a lot of columns insede de function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGUGtw1MP1c_"
      },
      "source": [
        "def find_diffs_between_files_lc(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', kind_col='', k='', am_cols = [],\\\n",
        "                             cols_int=[], cols_date=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "     \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "    \n",
        "    def date_parser(df, column,format, type_date):\n",
        "        if format == 'mix':\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "            return df[column]\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    am_cols = lower_str(am_cols)\n",
        "    cols_int = lower_str(cols_int)\n",
        "    cols_date = lower_str(cols_date)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        #Ajustado só para CZ_Julho, tirei o encoding\n",
        "        df_NEW = pd.read_csv(path_NEW).fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    else:\n",
        "        col_order_lc = lower_str([\"Contract ID - NEW\",\"FIN ID\",\"Contract Crncy\",\"Contract Type\",\"Freq.\",\"Freq. Unit\",\"Indexation\",\\\n",
        "                        \"Index upon request\",\"Counterpart ID\",\"Counterpart\",\"LC Amount CZK\\nyearly\",\"Amount in EUR yearly (Actual)\",\\\n",
        "                        \"Payment terms Code\",\"VAT Subject\",\"% of VAT\",\"Contr. Start date\",\"Contr. 1st End\",\"Contr. Term End\",\\\n",
        "                        \"Sublease Consession\",\"Renewal Option\",\"Unique Key\",\"Count of Key\",\"Count of ContrBct \",\\\n",
        "                        \"Remarks - Consistency check\",\"Residual Period in years\",\"Scoping classification\",\"DAS sites\",\\\n",
        "                        \"DAS ownership\",\"31_12_9999\",\"Unique Key1\",'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021',\\\n",
        "                        \"Check LC amount\",\"Updated Item\\nAmount yearly\",'Contr. 1st End as of 31.05.2021','Contr. 1st End as of 30.06.2021',\\\n",
        "                        \"Check Contr. 1st End\",\"Updated Item\\n1st End date\",\"Comment\",\"Comment date\",\"Date\"])\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=col_order_lc)\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD = df_OLD.sort_values(by='contract id - new')\n",
        "        \n",
        "        lista = []\n",
        "        df_OLD['sites'] = df_OLD['contract id - new'].copy()\n",
        "        for i in range(len(df_OLD['contract id - new'])):\n",
        "            if str(df_OLD.iloc[i]['contract id - new']) in lista:            \n",
        "                df_OLD.iloc[i]['sites'] = str(df_OLD.iloc[i]['contract id - new'])+\"_\"+str(lista.count(str(df_OLD.iloc[i]['contract id - new'])))\n",
        "            else:\n",
        "                df_OLD.iloc[i]['sites'] = str(df_OLD.iloc[i]['contract id - new'])+\"_0\"    \n",
        "            lista.append(str(df_OLD.iloc[i]['contract id - new']))   \n",
        "\n",
        "       \n",
        "\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD[k].astype(str)+df_OLD['count of contrbct '].astype(str)+df_OLD[\"count of key\"].astype(str)+df_OLD[\"counterpart\"].astype(str)\n",
        "        df_OLD = df_OLD.fillna('')\n",
        "\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        #df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "\n",
        "        col_names = ['Contract ID - NEW', 'FIN ID', 'Contract Crncy', 'Contract Type', 'Freq.', 'Freq. Unit', 'Indexation',\\\n",
        "                'Index upon request', 'Counterpart ID', 'Counterpart', 'LC Amount CZK\\nyearly', 'Amount in EUR yearly (Actual)', \\\n",
        "                'Payment terms Code', 'VAT Subject', '% of VAT', 'Contr. Start date', 'Contr. 1st End', 'Contr. Term End', \\\n",
        "                'Sublease Consession', 'Renewal Option', 'Unique Key', 'Count of Key', 'Count of ContrBct ', \\\n",
        "                'Remarks - Consistency check', 'Residual Period in years', 'Scoping classification', 'DAS sites', \\\n",
        "                'DAS ownership', '31_12_9999', 'Unique Key1', 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021', \\\n",
        "                'Check LC amount', 'Updated Item\\nAmount yearly', 'Contr. 1st End as of 31.05.2021', \\\n",
        "                'Contr. 1st End as of 30.06.2021', 'Check Contr. 1st End', 'Updated Item\\n1st End date', 'Comment', \\\n",
        "                'Comment date', 'Date']\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,header=0, names=col_names, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.reindex(columns=col_order_lc)\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW = df_NEW.sort_values(by='contract id - new')\n",
        "\n",
        "        df_NEW['sites'] = df_NEW[index_col].copy()\n",
        "        df_NEW['sites'] = df_NEW['sites'].astype(str)\n",
        "        df_NEW['sites'] = np.where(df_NEW['sites'].duplicated(keep=False), \n",
        "                      df_NEW['sites'] + df_NEW.groupby('sites').cumcount().add(1).astype(str),\n",
        "                      df_NEW['sites'])\n",
        "        print(df_NEW['sites'])\n",
        "        #df_NEW['sites'] = [str(site)+\"_\"+str(i) for i, site in enumerate(df_NEW['contract id - new'])]\n",
        "\n",
        "        #df_NEW['sites'] = df_NEW[index_col].astype(str)+df_NEW[kind_col].astype(str)+df_NEW[k].astype(str)+df_NEW['count of contrbct '].astype(str)+df_NEW[\"counterpart\"].astype(str)+df_NEW[\"counterpart\"].astype(str)\n",
        "        \n",
        "        df_NEW = replace_values(df_NEW, am_cols, 0)\n",
        "        for col in am_cols:\n",
        "            #df[col] = df[col].fillna(0)\n",
        "            df_NEW[col] = df_NEW[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "\n",
        "        df_NEW = replace_values(df_NEW,bill_cols)\n",
        "        for i in cols_date:\n",
        "            df_NEW[i] = date_parser(df_NEW, i, 'mix', \"%d/%m/%Y\")\n",
        "        df_NEW = df_NEW.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "        #print(df_NEW.info())\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "\n",
        "        try:\n",
        "            df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        except:\n",
        "            print('falha')\n",
        "        #print(list(df_NEW.columns)==(df_OLD.columns))\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} old_file vs new_file.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                             end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "\n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "lc_col = ['FIN ID', 'Counterpart', 'LC Amount CZK\\nyearly','Contr. Start date', 'Contr. 1st End']\n",
        "path_lc_input = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "lc_old = '/content/LC_Input_CzechRepublic_20210731.csv'\n",
        "lc_save = '/content/CZ_LC'\n",
        "ta_tab = 'Final data_Lease'\n",
        "dates_lc = ['Contr. Start date', 'Contr. 1st End', 'Contr. 1st End as of 31.05.2021', 'Contr. 1st End as of 30.06.2021']\n",
        "interger_lc = []\n",
        "amount_lc = ['LC Amount CZK\\nyearly']\n",
        "lc_bill_cols = ['Contract ID - NEW','Counterpart']\n",
        "\"\"\"find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, \\\n",
        "                         status_col='', path_save=ta_save, type_file='mix',kind='ta',\\\n",
        "                         kind_col='tenant agreement id - new',k='service type',am_cols = amount_ta,\\\n",
        "                         cols_int=interger, cols_date=dates, sheetname=sheet)\"\"\"\n",
        "find_diffs_between_files_lc(lc_old, path_lc_input, 'FIN ID', lc_col,status_col='', path_save=lc_save,type_file='mix', kind='ta',\\\n",
        "                            kind_col = 'contract id - new', k='contract type',am_cols=amount_lc,\\\n",
        "                           cols_int=[], cols_date=dates_lc, sheetname=ta_tab, skipr=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61TTpXoOP1_3"
      },
      "source": [
        "# CSV\n",
        "Old version\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPOdk01b-pnD",
        "outputId": "bf7c8c24-a1cc-42cf-9321-ca8673683d36"
      },
      "source": [
        "def find_diffs_between_files_lc(path_OLD, path_NEW,new_file, old_file, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', kind_col='', k='', am_cols = [],\\\n",
        "                             cols_int=[], cols_date=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "     \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "    \n",
        "    def date_parser(df, column,format, type_date):\n",
        "        if format == 'mix':\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "            return df[column]\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    am_cols = lower_str(am_cols)\n",
        "    cols_int = lower_str(cols_int)\n",
        "    cols_date = lower_str(cols_date)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        col_order_lc = [\"Contract ID - NEW\",\"FIN ID\",\"Contract Crncy\",\"Contract Type\",\"Freq.\",\"Freq. Unit\",\"Indexation\",\\\n",
        "                        \"Index upon request\",\"Counterpart ID\",\"Counterpart\",\"LC Amount CZK\\nyearly\",\"Amount in EUR yearly (Actual)\",\\\n",
        "                        \"Payment terms Code\",\"VAT Subject\",\"% of VAT\",\"Contr. Start date\",\"Contr. 1st End\",\"Contr. Term End\",\\\n",
        "                        \"Sublease Consession\",\"Renewal Option\",\"Unique Key\",\"Count of Key\",\"Count of ContrBct \",\\\n",
        "                        \"Remarks - Consistency check\",\"Residual Period in years\",\"Scoping classification\",\"DAS sites\",\\\n",
        "                        \"DAS ownership\",\"31_12_9999\",\"Unique Key1\",'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021',\\\n",
        "                        \"Check LC amount\",\"Updated Item\\nAmount yearly\",'Contr. 1st End as of 31.05.2021','Contr. 1st End as of 30.06.2021',\\\n",
        "                        \"Check Contr. 1st End\",\"Updated Item\\n1st End date\",\"Comment\",\"Comment date\",\"Date\"]\n",
        "        col_order_lc = lower_str(col_order_lc)\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=col_order_lc, encoding='latin2')\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD = df_OLD.sort_values(by='contract id - new')\n",
        "        \n",
        "        lista = []\n",
        "\n",
        "        df_OLD['sites'] = df_OLD['contract id - new'].copy()\n",
        "        for i in range(len(df_OLD['contract id - new'])):\n",
        "            if str(df_OLD.iloc[i]['contract id - new']) in lista:           \n",
        "                df_OLD.loc[i, 'sites'] = str(df_OLD.iloc[i]['contract id - new'])+\"_\"+str(lista.count(str(df_OLD.iloc[i]['contract id - new'])))\n",
        "            else:  \n",
        "                df_OLD.loc[i, 'sites']= str(df_OLD.iloc[i]['contract id - new'])+\"_0\"\n",
        "\n",
        "            lista.append(str(df_OLD.iloc[i]['contract id - new']))   \n",
        "\n",
        "        print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD[k].astype(str)+df_OLD['count of contrbct '].astype(str)+df_OLD[\"count of key\"].astype(str)+df_OLD[\"counterpart\"].astype(str)\n",
        "        df_OLD = df_OLD.fillna('')\n",
        "        #df_OLD = df_OLD.apply(unidecode)\n",
        "\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "        \n",
        "        \"\"\"New\"\"\"\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, header=0, names=col_order_lc, encoding='latin2')\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_NEW = df_NEW.sort_values(by='contract id - new')\n",
        "        \n",
        "        lista = []\n",
        "\n",
        "        df_NEW['sites'] = df_NEW['contract id - new'].copy()\n",
        "        for i in range(len(df_NEW['contract id - new'])):\n",
        "            if str(df_NEW.iloc[i]['contract id - new']) in lista:           \n",
        "                df_NEW.loc[i, 'sites'] = str(df_NEW.iloc[i]['contract id - new'])+\"_\"+str(lista.count(str(df_NEW.iloc[i]['contract id - new'])))\n",
        "            else:  \n",
        "                df_NEW.loc[i, 'sites']= str(df_NEW.iloc[i]['contract id - new'])+\"_0\"\n",
        "\n",
        "            lista.append(str(df_NEW.iloc[i]['contract id - new']))   \n",
        "\n",
        "        print([i for i in list(df_NEW['sites']) if list(df_NEW['sites']).count(i)>1])\n",
        "\n",
        "\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD[k].astype(str)+df_OLD['count of contrbct '].astype(str)+df_OLD[\"count of key\"].astype(str)+df_OLD[\"counterpart\"].astype(str)\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "        #df_OLD = df_OLD.apply(unidecode)\n",
        "\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "\n",
        "        #Ajustado só para CZ_Julho, tirei o encoding\n",
        "\n",
        "\n",
        "    else:\n",
        "        col_order_lc = [\"Contract ID - NEW\",\"FIN ID\",\"Contract Crncy\",\"Contract Type\",\"Freq.\",\"Freq. Unit\",\"Indexation\",\\\n",
        "                        \"Index upon request\",\"Counterpart ID\",\"Counterpart\",\"LC Amount CZK\\nyearly\",\"Amount in EUR yearly (Actual)\",\\\n",
        "                        \"Payment terms Code\",\"VAT Subject\",\"% of VAT\",\"Contr. Start date\",\"Contr. 1st End\",\"Contr. Term End\",\\\n",
        "                        \"Sublease Consession\",\"Renewal Option\",\"Unique Key\",\"Count of Key\",\"Count of ContrBct \",\\\n",
        "                        \"Remarks - Consistency check\",\"Residual Period in years\",\"Scoping classification\",\"DAS sites\",\\\n",
        "                        \"DAS ownership\",\"31_12_9999\",\"Unique Key1\",'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021',\\\n",
        "                        \"Check LC amount\",\"Updated Item\\nAmount yearly\",'Contr. 1st End as of 31.05.2021','Contr. 1st End as of 30.06.2021',\\\n",
        "                        \"Check Contr. 1st End\",\"Updated Item\\n1st End date\",\"Comment\",\"Comment date\",\"Date\"]\n",
        "        col_order_lc = lower_str(col_order_lc)\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=col_order_lc, encoding='latin2')\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD = df_OLD.sort_values(by='contract id - new')\n",
        "        \n",
        "        lista = []\n",
        "\n",
        "        df_OLD['sites'] = df_OLD['contract id - new'].copy()\n",
        "        for i in range(len(df_OLD['contract id - new'])):\n",
        "            if str(df_OLD.iloc[i]['contract id - new']) in lista:           \n",
        "                df_OLD.loc[i, 'sites'] = str(df_OLD.iloc[i]['contract id - new'])+\"_\"+str(lista.count(str(df_OLD.iloc[i]['contract id - new'])))\n",
        "            else:  \n",
        "                df_OLD.loc[i, 'sites']= str(df_OLD.iloc[i]['contract id - new'])+\"_0\"\n",
        "\n",
        "            lista.append(str(df_OLD.iloc[i]['contract id - new']))   \n",
        "\n",
        "        print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD[k].astype(str)+df_OLD['count of contrbct '].astype(str)+df_OLD[\"count of key\"].astype(str)+df_OLD[\"counterpart\"].astype(str)\n",
        "        df_OLD = df_OLD.fillna('')\n",
        "        #df_OLD = df_OLD.apply(unidecode)\n",
        "\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "\n",
        "        col_names = ['Contract ID - NEW', 'FIN ID', 'Contract Crncy', 'Contract Type', 'Freq.', 'Freq. Unit', 'Indexation',\\\n",
        "                'Index upon request', 'Counterpart ID', 'Counterpart', 'LC Amount CZK\\nyearly', 'Amount in EUR yearly (Actual)', \\\n",
        "                'Payment terms Code', 'VAT Subject', '% of VAT', 'Contr. Start date', 'Contr. 1st End', 'Contr. Term End', \\\n",
        "                'Sublease Consession', 'Renewal Option', 'Unique Key', 'Count of Key', 'Count of ContrBct ', \\\n",
        "                'Remarks - Consistency check', 'Residual Period in years', 'Scoping classification', 'DAS sites', \\\n",
        "                'DAS ownership', '31_12_9999', 'Unique Key1', 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021', \\\n",
        "                'Check LC amount', 'Updated Item\\nAmount yearly', 'Contr. 1st End as of 31.05.2021', \\\n",
        "                'Contr. 1st End as of 30.06.2021', 'Check Contr. 1st End', 'Updated Item\\n1st End date', 'Comment', \\\n",
        "                'Comment date', 'Date']\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,header=0, names=col_names, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.reindex(columns=col_order_lc)\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW = df_NEW.sort_values(by='contract id - new')\n",
        "\n",
        "        lista = []\n",
        "        df_NEW['sites'] = df_NEW['contract id - new'].copy()\n",
        "        for i in range(len(df_NEW['contract id - new'])):\n",
        "            if str(df_NEW.iloc[i]['contract id - new']) in lista:           \n",
        "                df_NEW.loc[i, 'sites'] = str(df_NEW.iloc[i]['contract id - new'])+\"_\"+str(lista.count(str(df_NEW.iloc[i]['contract id - new'])))\n",
        "            else:   \n",
        "                df_NEW.loc[i, 'sites'] = str(df_NEW.iloc[i]['contract id - new'])+\"_0\"\n",
        "\n",
        "            lista.append(str(df_NEW.iloc[i]['contract id - new']))   \n",
        "\n",
        "        #print([i for i in list(df_NEW['sites']) if list(df_NEW['sites']).count(i)>1])\n",
        "        df_NEW = replace_values(df_NEW, am_cols, 0)\n",
        "        for col in am_cols:\n",
        "            #df[col] = df[col].fillna(0)\n",
        "            df_NEW[col] = df_NEW[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "\n",
        "        df_NEW = replace_values(df_NEW,bill_cols)\n",
        "        for i in cols_date:\n",
        "            df_NEW[i] = date_parser(df_NEW, i, 'mix', \"%d/%m/%Y\")\n",
        "        df_NEW = df_NEW.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "        #print(df_NEW.info())\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "\n",
        "        try:\n",
        "            df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        except:\n",
        "            print('falha')\n",
        "        #print(list(df_NEW.columns)==(df_OLD.columns))\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    \n",
        "    n = 10\n",
        "    lista = []\n",
        "    for site in new_copy['sites']:\n",
        "        chunks = [site[i:i+n] for i in range(0, len(site), n)]\n",
        "        lista.append(chunks[0])\n",
        "    new_copy['sites_code'] = lista\n",
        "    new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} {old_file} vs {new_file}.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                             end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "\n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    ws.delete_cols(2)\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "lc_col = ['FIN ID', 'Counterpart', 'LC Amount CZK\\nyearly','Contr. Start date', 'Contr. 1st End']\n",
        "path_lc_input = '/content/LC_Input_CzechRepublic_20210831.csv'\n",
        "lc_old = '/content/LC_Input_CzechRepublic_20210630_True-up.csv'\n",
        "lc_save = '/content/CZ_LC'\n",
        "lc_tab = 'Final data_Lease'\n",
        "dates_lc = ['Contr. Start date', 'Contr. 1st End', 'Contr. 1st End as of 31.05.2021', 'Contr. 1st End as of 30.06.2021']\n",
        "interger_lc = []\n",
        "amount_lc = ['LC Amount CZK\\nyearly']\n",
        "lc_bill_cols = ['Contract ID - NEW','Counterpart']\n",
        "lc_nn = 'celfinet_20210831.csv'\n",
        "lc_on = 'LC_20210630_True-up.csv'\n",
        "find_diffs_between_files_lc(lc_old, path_lc_input,lc_nn,lc_on, 'FIN ID', lc_col,status_col='', path_save=lc_save,type_file='csv', kind='ta',\\\n",
        "                            kind_col = 'contract id - new', k='contract type',am_cols=amount_lc,\\\n",
        "                           cols_int=[], cols_date=dates_lc, sheetname=lc_tab, skipr=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n",
            "\n",
            "New Rows:     []\n",
            "Dropped Rows: []\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iogYMZwc_dxz"
      },
      "source": [
        "# UIS Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kplnCM5DM9s"
      },
      "source": [
        "def find_diffs_between_files_uis(path_OLD, path_NEW, index_col, uis_cols, \\\n",
        "                             path_save, old_name, new_name, type_file='mix', sheetname='', skipr=0, skipc=0):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, uis_cols):\n",
        "\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        #df = df[uis_cols]\n",
        "        #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    #uis_cols = lower_str(uis_cols)\n",
        "    #index_col = index_col.lower()\n",
        "    # type_file = type_file.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD, sheet_name = sheetname,  skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = df_OLD.drop([0,0])\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,  skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = df_NEW.drop([0,0]) \n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "    else:\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,uis_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,uis_cols)\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "    print(len(sharedCols))\n",
        "    print(sharedCols)\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0,thresh=len(new_copy.columns[1:]))\n",
        "    new_copy = new_copy.dropna(axis=1)\n",
        "    print(new_copy)\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - [{old_name}] vs [{new_name}].xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "col_uis = ['Site_ID (Alphanumeric, Alphabetical or Numeric)',\n",
        "       'Site Categorization as Phase 1 or Phase 2 Site',\n",
        "       'BTS site applicable charge (Annual)',\n",
        "       'Commercials for sites beyond 10% cap of critical sites (Annual)',\n",
        "       'Total charges to be applied for subsequent active sharing arrangement',\n",
        "       'Fixed component of energy bill', 'Variable component of energy bill',\n",
        "       'Energy consumption (for metered site)',\n",
        "       'Agreed unit price (for metered sites)',\n",
        "       'Operator’s active energy charge through estimated model',\n",
        "       'Capex incurred by TowerCo under clause 29 to be reimbursed by Operator',\n",
        "       'Energy reduction measures - Final value to be charged',\n",
        "       'Legacy site upgrades Or BBU capacity upgrade - Final value to be charged',\n",
        "       'BTS site - Site service order withdrawal - Final value to be charged',\n",
        "       'Energy infrastructure enhancements - Final value to be charged',\n",
        "       'One-off power related costs - Final value to be charged',\n",
        "       'Space blocking - Unutilized sites\\nIf unutilized then select Yes ',\n",
        "       'Decommissioning charges - Equipment removal cost - Final value to be charged',\n",
        "       'Site exit fee - Site ID part of delta',\n",
        "       'Site exit fee - Remaining term of site IDs in delta',\n",
        "       'Site power availability - Site IDs where service credits are applicable',\n",
        "       'Site power availability - Total hours applicable ',\n",
        "       'Site power availability - Unavailable time',\n",
        "       'Site power availability - Count of repeat incidents (if applicable)',\n",
        "       'Site cooling - Final value associated with Service Credit applicable',\n",
        "       'Site access - Final value associated with Service Credit applicable',\n",
        "       'Incident resolution - Final value associated with Service Credit applicable',\n",
        "       'BTS sites - Deployment time - Delay',\n",
        "       'Site modification Completion time - Delay',\n",
        "       'Delay In RSCR site modification completion Time',\n",
        "       'Delay In RSCR BTS site completion time',\n",
        "       'Additional Customer Discount percentage applicable (Number only)',\n",
        "       'Delay in Site Modification Projects', 'Delay in BTS Projects',\n",
        "       'Excess of Upgrade Capital Expenditure over Threshold']\n",
        "       \n",
        "pathuis_n = '/content/UserInput_CzechRepublic_20210831.xlsx'\n",
        "pathuis_o = '/content/UserInput_CzechRepublic_20210630 TrueUp.xlsx'\n",
        "sheetuis='SiteLevel'\n",
        "skiprows=2\n",
        "to_save = '/content/CZ_UIS'\n",
        "old_n = 'UIS - 20210630 TrueUp.xlsx'\n",
        "new_n = \"UIS - 20210831.xlsx\"\n",
        "\n",
        "find_diffs_between_files_uis(pathuis_o, pathuis_n, 'Site_ID (Alphanumeric, Alphabetical or Numeric)', col_uis, \\\n",
        "                        to_save, old_n, new_n, type_file='excel', sheetname=sheetuis, skipr=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}