{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cz_comparisons.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7RrQDzfHrUvZ0r9iGQqam"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXmIHnYMdaUO"
      },
      "source": [
        "!pip install unidecode\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "from unidecode import unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-PK7UUdeRmK"
      },
      "source": [
        "TowerDB Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvNojQ9vePh0"
      },
      "source": [
        "def find_diffs_between_files_1(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, cols_conv, type_file='mix',status_col='', \\\n",
        "                             kind='tw', dates_cols=[], kind_col='',type_date=\"%d/%m/%Y\", sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col,cols_conv, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col] \n",
        "            #df.columns = lower_str(list(df.columns))\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            for col in cols_conv:\n",
        "                df[col] = df[col].astype(str).apply(unidecode)\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df = df.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "            #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']]\n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format, type_date):\n",
        "        for column in columns:\n",
        "            if format == 'mix':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    cols_conv = lower_str(cols_conv)\n",
        "    dates_cols = lower_str(dates_cols)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        date_parser(df_OLD, dates_cols,format , type_date)\n",
        "        df_OLD = fit_df(df_OLD,index_col,cols_conv,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols,format , type_date)\n",
        "        df_NEW = fit_df(df_NEW,index_col,cols_conv,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='ISO-8859-1').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        #df_OLD = df_OLD[lower_str(bill_cols)]\n",
        "        df_OLD = fit_df(df_OLD, index_col, cols_conv,kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='ISO-8859-1').fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,cols_conv, kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD) header=0, names=cols_old,\n",
        "        df_OLD = pd.read_csv(path_OLD,encoding='ISO-8859-1').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col,cols_conv, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        #print(df_NEW.columns)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols,format , type_date)\n",
        "        df_NEW = fit_df(df_NEW, index_col,cols_conv, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                            end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "bill_col= ['Code (Duplicate)',\\\n",
        "            \"Site Status\",\\\n",
        "            \"Categorization by Transmission Sys\",\\\n",
        "            \"Categorization by Site Type\",\\\n",
        "            'Strategic Macro Sites',\\\n",
        "            'Critical Sites',\\\n",
        "            'Case A Core Site',\\\n",
        "            'Macro Site - Transmission Hub Site',\\\n",
        "            'Macro Site - Transmission Hub Site with/without Shelters',\\\n",
        "            'Transmission Sites',\\\n",
        "            'Room Configuration',\\\n",
        "            'Power Supply',\\\n",
        "            'Air Conditioning',\\\n",
        "            'Active Sharing Arrangements involving the Operator',\\\n",
        "            'VF-CZ Demerger phase',\\\n",
        "            \"Billing Trigger date \",\\\n",
        "            'Strategic_Site_Bucket',\\\n",
        "            'Critical Site Bucket',\\\n",
        "            'Wip_Site',\\\n",
        "            'Bts_Site',\\\n",
        "            'Sites_As_Metered_Estimated',\\\n",
        "            'Subsequent_Sharing_Arrangement',\\\n",
        "            \"First_Active_Sharing_Deployment_Type\",\\\n",
        "            \"First_Active_Sharing_Start_Date\",\\\n",
        "            \"First_Active_Sharing_End_Date\",\\\n",
        "            \"Decommissioned_Site\",\\\n",
        "            \"Transfer_Date_Of_Consent_Required_Sites\",\\\n",
        "            \"Date_Of_Equipment_Removal\"]\n",
        "pathtw = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "pathold = '/content/TowerDB_CzechRepublic_20210731.csv'\n",
        "tw_save = '/content/CZ_TW'\n",
        "tab = 'Tower DB'\n",
        "row = 3\n",
        "old_n = 'twdb_last.csv'\n",
        "new_n = 'twdb_current.xlsx'\n",
        "cols_con = ['Province','Municipality','Address']\n",
        "dates = ['Billing Trigger date ','infrastructure ready (existing)/ to be ready (new)',\\\n",
        "         'First_Active_Sharing_Start_Date',\\\n",
        "         'First_Active_Sharing_End_Date', 'Transfer_Date_Of_Consent_Required_Sites',\\\n",
        "         'Date_Of_Equipment_Removal']  \n",
        "\n",
        "find_diffs_between_files_1(pathold, pathtw, 'Code (Duplicate)', bill_col,tw_save, old_n,new_n,cols_con,'mix',\\\n",
        "                           status_col=\"Site Status\" , kind='tw', dates_cols=dates , kind_col='', sheetname=tab, skipr = row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4154LwReeL_Y"
      },
      "source": [
        "TA Input Comparisons\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBOQombqmQSB",
        "outputId": "1b70a685-3bc4-412b-d30c-3e90ac53d613"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', kind_col='', k='', am_cols = [],\\\n",
        "                             cols_int=[], cols_date=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "     \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "    \n",
        "    def date_parser(df, column,format, type_date):\n",
        "        if format == 'mix':\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "            return df[column]\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        #Ajustado só para CZ_Julho, tirei o encoding\n",
        "        df_NEW = pd.read_csv(path_NEW).fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    else:\n",
        "        ta_cols = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', \\\n",
        "           'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', \\\n",
        "           'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', \\\n",
        "           'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "           'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', \\\n",
        "           'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', \\\n",
        "           'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "           'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x','SiteType',\\\n",
        "           '1.028', 'Unique Tenant', 'Unique Tenant Count', 'unamed', 'unamed_2']\n",
        "        ta_cols = lower_str(ta_cols)\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=ta_cols)\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        \n",
        "        df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD['site name']+df_OLD['count of contracts'].astype(str)+df_OLD['service type'].astype(str)\n",
        "        df_OLD = df_OLD.fillna('')\n",
        "        for in_col in ['counterpart','annual amount in czk', 'amount in eur', 'index', 'counterpart_1']:\n",
        "            df_OLD[in_col] = df_OLD[in_col].apply(unidecode)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "\n",
        "        col = ['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type',\\\n",
        "                'Contract start date', 'Contract end date', 'Status of contract',\\\n",
        "                'Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart',\\\n",
        "                'Classification of Tenant', 'Annual amount - billing currency',\\\n",
        "                'Billing Currency', 'Annual Amount in CZK', 'Amount in EUR',\\\n",
        "                'Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index',\\\n",
        "                'Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key',\\\n",
        "                'Classification', 'Residual Period in Years', 'Remarks',\\\n",
        "                'Count of unique key', 'Count of contracts', 'Scoping classification',\\\n",
        "                'DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month',\\\n",
        "                'Updated Item', 'Comment', 'Counterpart ID_1', 'Counterpart_1', 'x',\\\n",
        "                'SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count','Quota of Unique Key','unamed']\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,header=0, names=col, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.reindex(columns=ta_cols)\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW['sites'] = df_NEW[index_col].astype(str)+df_NEW[kind_col].astype(str)+df_NEW['site name']+df_NEW['count of contracts'].astype(str)+df_NEW['service type'].astype(str)\n",
        "        #print([i for i in list(df_NEW['sites']) if list(df_NEW['sites']).count(i)>1])\n",
        "        df_NEW = replace_values(df_NEW, am_cols, 0)\n",
        "        for col in am_cols:\n",
        "            #df[col] = df[col].fillna(0)\n",
        "            df_NEW[col] = df_NEW[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "\n",
        "        for col in cols_int:\n",
        "            df_NEW[col] = df_NEW[col].fillna(0)\n",
        "            df_NEW[col] = df_NEW[col].astype(int)\n",
        "\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "        for in_col in ['counterpart','annual amount in czk', 'amount in eur', 'index', 'counterpart_1']:\n",
        "            df_NEW[in_col] = df_NEW[in_col].apply(unidecode)\n",
        "        df_NEW = replace_values(df_NEW,bill_cols)\n",
        "        for i in cols_date:\n",
        "            df_NEW[i] = date_parser(df_NEW, i, 'mix', \"%d/%m/%Y\")\n",
        "        df_NEW = df_NEW.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "        #print(df_NEW.info())\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "\n",
        "        try:\n",
        "            df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        except:\n",
        "            print('falha')\n",
        "        #print(list(df_NEW.columns)==(df_OLD.columns))\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    \n",
        "    n = 11\n",
        "    lista = []\n",
        "    for site in new_copy['sites']:\n",
        "        chunks = [site[i:i+n] for i in range(0, len(site), n)]\n",
        "        lista.append(chunks[0])\n",
        "    new_copy['sites_code'] = lista\n",
        "    new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} old_file vs new_file.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                             end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "\n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    ws.delete_cols(2)\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "ta_cols = ['Tenant Agreement ID - NEW', 'Code','Contract start date', 'Contract end date','Counterpart',\\\n",
        "           'Classification of Tenant',]\n",
        "path_ta_input = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "sheet = 'Tenant '\n",
        "ta_old = '/content/TA_Input_CzechRepublic_20210731.csv'\n",
        "ta_save = '/content/CZ_TA'\n",
        "old_ta = 'TA_old.csv'\n",
        "new_ta = 'TA_new.xlsx'\n",
        "#find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, \"\", ta_save,'csv', 'ta')\n",
        "\n",
        "\n",
        "#cols_con_ta = ['Province','Municipality','Address']\n",
        "dates = lower_str(['Contract start date', 'Contract end date', 'Update in month'])\n",
        "amount_ta = lower_str(['Annual Amount in CZK', 'Amount in EUR'])\n",
        "interger = lower_str(['annual amount - billing currency','Residual Period in Years','Count of unique key', 'Count of contracts', 'Unique Tenant Count'])\n",
        "ta_bill_cols = lower_str(['Tenant Agreement ID - NEW','Contract start date','Contract end date',\\\n",
        "                'Counterpart','Classification of Tenant'])\n",
        "\n",
        "find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, \\\n",
        "                         status_col='', path_save=ta_save, type_file='mix',kind='ta',\\\n",
        "                         kind_col='tenant agreement id - new',k='service type',am_cols = amount_ta,\\\n",
        "                         cols_int=interger, cols_date=dates, sheetname=sheet)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['CBUD1Gnot available yetJHBUD0.0Lease', 'BOJVR5not available yetBOJVR0.0Lease', '111790not available yetPRSLA0.0Lease', '170140not available yetCKDOM0.0Lease', '750430not available yetA3CHE0.0Lease', '750120not available yetA3CHU0.0Lease']\n",
            "Dropped Rows: ['1610902400000063ULPOD1.0Energy', '1610902400000063ULPOD0.0Lease']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWDk2R5x-nin"
      },
      "source": [
        "LC Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPOdk01b-pnD"
      },
      "source": [
        "def find_diffs_between_files_lc(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', kind_col='', k='', am_cols = [],\\\n",
        "                             cols_int=[], cols_date=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "     \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "    \n",
        "    def date_parser(df, column,format, type_date):\n",
        "        if format == 'mix':\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "            return df[column]\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan', ' ', 'not available yet']\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            df[column] = df[column].fillna(0)\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "        return df\n",
        "\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    am_cols = lower_str(am_cols)\n",
        "    cols_int = lower_str(cols_int)\n",
        "    cols_date = lower_str(cols_date)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        #Ajustado só para CZ_Julho, tirei o encoding\n",
        "        df_NEW = pd.read_csv(path_NEW).fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    else:\n",
        "        col_order_lc = [\"Contract ID - NEW\",\"FIN ID\",\"Contract Crncy\",\"Contract Type\",\"Freq.\",\"Freq. Unit\",\"Indexation\",\\\n",
        "                        \"Index upon request\",\"Counterpart ID\",\"Counterpart\",\"LC Amount CZK\\nyearly\",\"Amount in EUR yearly (Actual)\",\\\n",
        "                        \"Payment terms Code\",\"VAT Subject\",\"% of VAT\",\"Contr. Start date\",\"Contr. 1st End\",\"Contr. Term End\",\\\n",
        "                        \"Sublease Consession\",\"Renewal Option\",\"Unique Key\",\"Count of Key\",\"Count of ContrBct \",\\\n",
        "                        \"Remarks - Consistency check\",\"Residual Period in years\",\"Scoping classification\",\"DAS sites\",\\\n",
        "                        \"DAS ownership\",\"31_12_9999\",\"Unique Key1\",'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021',\\\n",
        "                        \"Check LC amount\",\"Updated Item\\nAmount yearly\",'Contr. 1st End as of 31.05.2021','Contr. 1st End as of 30.06.2021',\\\n",
        "                        \"Check Contr. 1st End\",\"Updated Item\\n1st End date\",\"Comment\",\"Comment date\",\"Date\"]\n",
        "        col_order_lc = lower_str(col_order_lc)\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=col_order_lc)\n",
        "        #df_OLD = fit_df(df_OLD,bill_cols)\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = df_OLD.dropna(subset=[index_col], axis=0)\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        df_OLD = df_OLD.sort_values(by='contract id - new')\n",
        "        \n",
        "        lista = []\n",
        "        df_OLD['sites'] = df_OLD['contract id - new'].copy()\n",
        "        for i in range(len(df_OLD['contract id - new'])):\n",
        "            if str(df_OLD.iloc[i]['contract id - new']) in lista:           \n",
        "                df_OLD.loc[i, 'sites'] = str(df_OLD.iloc[i]['contract id - new'])+\"_\"+str(lista.count(str(df_OLD.iloc[i]['contract id - new'])))\n",
        "            else:  \n",
        "                df_OLD.loc[i, 'sites']= str(df_OLD.iloc[i]['contract id - new'])+\"_0\"\n",
        "\n",
        "            lista.append(str(df_OLD.iloc[i]['contract id - new']))   \n",
        "\n",
        "        print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "\n",
        "        #df_OLD['sites'] = df_OLD[index_col].astype(str)+df_OLD[kind_col].astype(str)+df_OLD[k].astype(str)+df_OLD['count of contrbct '].astype(str)+df_OLD[\"count of key\"].astype(str)+df_OLD[\"counterpart\"].astype(str)\n",
        "        df_OLD = df_OLD.fillna('')\n",
        "        #print([i for i in list(df_OLD['sites']) if list(df_OLD['sites']).count(i)>1])\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_OLD = df_OLD.set_index('sites').fillna('')\n",
        "\n",
        "        col_names = ['Contract ID - NEW', 'FIN ID', 'Contract Crncy', 'Contract Type', 'Freq.', 'Freq. Unit', 'Indexation',\\\n",
        "                'Index upon request', 'Counterpart ID', 'Counterpart', 'LC Amount CZK\\nyearly', 'Amount in EUR yearly (Actual)', \\\n",
        "                'Payment terms Code', 'VAT Subject', '% of VAT', 'Contr. Start date', 'Contr. 1st End', 'Contr. Term End', \\\n",
        "                'Sublease Consession', 'Renewal Option', 'Unique Key', 'Count of Key', 'Count of ContrBct ', \\\n",
        "                'Remarks - Consistency check', 'Residual Period in years', 'Scoping classification', 'DAS sites', \\\n",
        "                'DAS ownership', '31_12_9999', 'Unique Key1', 'LC amount as of 31.05.2021', 'LC amount as of 30.06.2021', \\\n",
        "                'Check LC amount', 'Updated Item\\nAmount yearly', 'Contr. 1st End as of 31.05.2021', \\\n",
        "                'Contr. 1st End as of 30.06.2021', 'Check Contr. 1st End', 'Updated Item\\n1st End date', 'Comment', \\\n",
        "                'Comment date', 'Date']\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname,header=0, names=col_names, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW.reindex(columns=col_order_lc)\n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW = df_NEW.sort_values(by='contract id - new')\n",
        "\n",
        "        lista = []\n",
        "        df_NEW['sites'] = df_NEW['contract id - new'].copy()\n",
        "        for i in range(len(df_NEW['contract id - new'])):\n",
        "            if str(df_NEW.iloc[i]['contract id - new']) in lista:           \n",
        "                df_NEW.loc[i, 'sites'] = str(df_NEW.iloc[i]['contract id - new'])+\"_\"+str(lista.count(str(df_NEW.iloc[i]['contract id - new'])))\n",
        "            else:   \n",
        "                df_NEW.loc[i, 'sites'] = str(df_NEW.iloc[i]['contract id - new'])+\"_0\"\n",
        "\n",
        "            lista.append(str(df_NEW.iloc[i]['contract id - new']))   \n",
        "\n",
        "        print(df_NEW['sites'])\n",
        "        #df_NEW['sites'] = [str(site)+\"_\"+str(i) for i, site in enumerate(df_NEW['contract id - new'])]\n",
        "\n",
        "        #df_NEW['sites'] = df_NEW[index_col].astype(str)+df_NEW[kind_col].astype(str)+df_NEW[k].astype(str)+df_NEW['count of contrbct '].astype(str)+df_NEW[\"counterpart\"].astype(str)+df_NEW[\"counterpart\"].astype(str)\n",
        "        print([i for i in list(df_NEW['sites']) if list(df_NEW['sites']).count(i)>1])\n",
        "        df_NEW = replace_values(df_NEW, am_cols, 0)\n",
        "        for col in am_cols:\n",
        "            #df[col] = df[col].fillna(0)\n",
        "            df_NEW[col] = df_NEW[col].astype(int).apply(lambda x: f'{x:,}')\n",
        "\n",
        "        df_NEW = df_NEW.fillna('')\n",
        "\n",
        "        df_NEW = replace_values(df_NEW,bill_cols)\n",
        "        for i in cols_date:\n",
        "            df_NEW[i] = date_parser(df_NEW, i, 'mix', \"%d/%m/%Y\")\n",
        "        df_NEW = df_NEW.applymap(lambda x: x.encode('unicode_escape').decode('ISO-8859-1') if isinstance(x, str) else x)\n",
        "        #print(df_NEW.info())\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "\n",
        "        try:\n",
        "            df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        except:\n",
        "            print('falha')\n",
        "        #print(list(df_NEW.columns)==(df_OLD.columns))\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    \n",
        "    n = 10\n",
        "    lista = []\n",
        "    for site in new_copy['sites']:\n",
        "        chunks = [site[i:i+n] for i in range(0, len(site), n)]\n",
        "        lista.append(chunks[0])\n",
        "    new_copy['sites_code'] = lista\n",
        "    new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} old_file vs new_file.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                             end_color='00FF0000', fill_type='solid')\n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "\n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    ws.delete_cols(2)\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "lc_col = ['FIN ID', 'Counterpart', 'LC Amount CZK\\nyearly','Contr. Start date', 'Contr. 1st End']\n",
        "path_lc_input = '/content/TowerDB_FY21-06 June-Skylon VF CZ_v1.xlsx'\n",
        "lc_old = '/content/LC_Input_CzechRepublic_20210731.csv'\n",
        "lc_save = '/content/CZ_LC'\n",
        "ta_tab = 'Final data_Lease'\n",
        "dates_lc = ['Contr. Start date', 'Contr. 1st End', 'Contr. 1st End as of 31.05.2021', 'Contr. 1st End as of 30.06.2021']\n",
        "interger_lc = []\n",
        "amount_lc = ['LC Amount CZK\\nyearly']\n",
        "lc_bill_cols = ['Contract ID - NEW','Counterpart']\n",
        "\"\"\"find_diffs_between_files(ta_old, path_ta_input, 'Code', ta_cols, \\\n",
        "                         status_col='', path_save=ta_save, type_file='mix',kind='ta',\\\n",
        "                         kind_col='tenant agreement id - new',k='service type',am_cols = amount_ta,\\\n",
        "                         cols_int=interger, cols_date=dates, sheetname=sheet)\"\"\"\n",
        "find_diffs_between_files_lc(lc_old, path_lc_input, 'FIN ID', lc_col,status_col='', path_save=lc_save,type_file='mix', kind='ta',\\\n",
        "                            kind_col = 'contract id - new', k='contract type',am_cols=amount_lc,\\\n",
        "                           cols_int=[], cols_date=dates_lc, sheetname=ta_tab, skipr=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kplnCM5DM9s"
      },
      "source": [
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}