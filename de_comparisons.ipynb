{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "de_comparisons.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8xl+Ty4vzVUsy0HlMbg0m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emerenan/xlsx_validation/blob/main/de_comparisons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lMRvOc9gy8c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf_a_t1pDbHO"
      },
      "source": [
        "# TowerDB Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz0ifGfgA81C",
        "outputId": "e8d866ef-c386-4548-ddf5-7873b6f07a37"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save,old_file,new_file, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    import re\n",
        "    from openpyxl import Workbook, styles\n",
        "    from openpyxl.styles import PatternFill, Font\n",
        "    from openpyxl.styles.differential import DifferentialStyle\n",
        "    from openpyxl.formatting.rule import Rule\n",
        "\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        #df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        dates_tw = ['first_active_sharing_arrangement_start_date','first_active_sharing_arrangement_end_date', 'date_of_equipment_removal']\n",
        "        for date_col in dates_tw:\n",
        "            lista = []\n",
        "            df[date_col] = df[date_col].replace([' '], '')\n",
        "            df[date_col] = df[date_col].fillna('')\n",
        "            for i in df[date_col]:\n",
        "                dates_format = re.match(r'\\d{2}\\/\\d{2}\\/\\d{4}', str(i)) or re.match(r'\\d{2}\\-\\d{2}\\-\\d{4}', str(i))\n",
        "                not_date_format = not re.match(r'\\d{2}\\/\\d{2}\\/\\d{4}', str(i)) or not re.match(r'\\d{2}\\-\\d{2}\\-\\d{4}', str(i))\n",
        "                if i !='' and not_date_format:\n",
        "                    #print(towerdb[towerdb[date_col]==i][['identification - site key', date_col]])\n",
        "                    lista.append(f'{i:%d/%m/%Y}')\n",
        "                \n",
        "                elif dates_format:\n",
        "                    lista.append(i)\n",
        "                else:\n",
        "                    lista.append('')\n",
        "            df[date_col] = lista\n",
        "        ints = ['security class', 'postal code']\n",
        "        for i in ints:\n",
        "            df[i] = df[i].fillna(0)\n",
        "            df[i] = df[i].replace('', 0)\n",
        "            df[i] = list(map(int, df[i]))\n",
        "\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW[index_col] = df_NEW[index_col].astype(str)\n",
        "        df_NEW['sites'] = df_NEW[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save}_{old_file} vs {new_file}.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ20000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "        \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ20000', header_style)\n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "    \n",
        "\n",
        "bill_cols = [\"Code\",\\\n",
        "            \"Categorization by Transmission Sys\",\\\n",
        "            \"Categorization by Transmission Sys (sub-cluster)\",\\\n",
        "            \"Categorization by Site Type\",\"Categorization by Indoor / Outdoor Site (Antennas)\",\\\n",
        "            \"Categorization by Strategic Site\",\\\n",
        "            \"Categorization by Critical Site\",\\\n",
        "            \"Site Cluster\",\\\n",
        "            \"Strategic Site Bucket\",\\\n",
        "            \"Bts_sites\",\\\n",
        "            \"CriticalSite_Beyond_10\",\\\n",
        "            \"Sites As Metered Estimated\",\\\n",
        "            \"Active Sharing Arrangement\",\\\n",
        "            \"Indoor_Site_Any_Climate_Control\",\\\n",
        "            \"Outdoor_Site_With_Dc_Power\",\\\n",
        "            \"Rfai_Date\",\\\n",
        "            \"decommissioned site\",\\\n",
        "            \"Subsequent_Active_Sharing_Arrangement\",\\\n",
        "            \"First_Active_Sharing_Deployment_Type\",\\\n",
        "            \"First_Active_Sharing_Arrangement_Start_Date\",\\\n",
        "            \"First_Active_Sharing_Arrangement_End_Date\",\\\n",
        "            'Site Status (TIMS)']\n",
        "pathnew = '/content/TowerDB_Germany_20210731.csv'\n",
        "pathold = '/content/TowerDB_Germany_20210630.csv'\n",
        "to_save = '/content/DE_TW'\n",
        "old_n = '20210630.csv'\n",
        "new_n = '20210731.csv'\n",
        "\n",
        "find_diffs_between_files(pathold, pathnew, 'Code', bill_cols, 'Site Status (TIMS)', to_save,old_n, new_n, 'mix','tw')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['0861 W', '3207 O', '5331 B', '5866 O', '6214 S', '6282 O', '7963 H', 'B194 M']\n",
            "Dropped Rows: ['1130 M', '2890 W', 'MRT112', 'MRT125', 'MRT126', 'MRT147']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39CV7wLTTamf"
      },
      "source": [
        "Special Case - Excel File(August)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBwBOLxgI4Ed"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save,old_file,new_file, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    import re\n",
        "    from openpyxl import Workbook, styles\n",
        "    from openpyxl.styles import PatternFill, Font\n",
        "    from openpyxl.styles.differential import DifferentialStyle\n",
        "    from openpyxl.formatting.rule import Rule\n",
        "\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        #df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        dates_tw = ['first_active_sharing_arrangement_start_date','first_active_sharing_arrangement_end_date', 'date_of_equipment_removal']\n",
        "        for date_col in dates_tw:\n",
        "            lista = []\n",
        "            df[date_col] = df[date_col].replace([' '], '')\n",
        "            df[date_col] = df[date_col].fillna('')\n",
        "            for i in df[date_col]:\n",
        "                dates_format = re.match(r'\\d{2}\\/\\d{2}\\/\\d{4}', str(i)) or re.match(r'\\d{2}\\-\\d{2}\\-\\d{4}', str(i))\n",
        "                not_date_format = not re.match(r'\\d{2}\\/\\d{2}\\/\\d{4}', str(i)) or not re.match(r'\\d{2}\\-\\d{2}\\-\\d{4}', str(i))\n",
        "                if i !='' and not_date_format:\n",
        "                    #print(towerdb[towerdb[date_col]==i][['identification - site key', date_col]])\n",
        "                    lista.append(f'{i:%d/%m/%Y}')\n",
        "                \n",
        "                elif dates_format:\n",
        "                    lista.append(i)\n",
        "                else:\n",
        "                    lista.append('')\n",
        "            df[date_col] = lista\n",
        "        ints = ['security class', 'postal code']\n",
        "        for i in ints:\n",
        "            df[i] = df[i].fillna(0)\n",
        "            df[i] = df[i].replace('', 0)\n",
        "            df[i] = list(map(int, df[i]))\n",
        "\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = path_NEW\n",
        "        df_NEW[index_col] = df_NEW[index_col].astype(str)\n",
        "        df_NEW = df_NEW.drop_duplicates(subset=index_col)\n",
        "        df_NEW['sites'] = df_NEW[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = str(df_OLD.loc[row,col])\n",
        "                value_NEW = str(df_NEW.loc[row,col])\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save}_{old_file} vs {new_file}.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ20000', highlight)\n",
        "    \n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "        \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ20000', header_style)\n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "    \n",
        "path_excel = '/content/Output - Skylon - TowerDB - Germany - Release6.3 20210726_Billing_ReducedScope_sharedwithCelfinet_v03 (1).xlsx'\n",
        "#towerdb = pd.read_csv(path_towerdb,encoding='latin')\n",
        "guideline = lower_str(['Code','Site Name','Macro Region','Region','Province','Municipality','Inhabitants','Address','Latitude','Longitude',\\\n",
        "             'Categorization by Transmission Sys','Categorization by Transmission Sys (sub-cluster)','Categorization by Site Type',\\\n",
        "             'Categorization by Indoor / Outdoor Site (Antennas)','Additional Site Type information','Categorization by Inhabitants',\\\n",
        "             'Rural/ Suburban/ Urban','Categorization by CONNECTIVITY','Technology VOD','POD ID','Energy Consumption current FY',\\\n",
        "             'Actual Energy Cost current FY','Infrastructure ready (existing)/ to be ready (new)','Infrastructure to be shared by',\\\n",
        "             'Counterpart','# of Lease Contracts','Current annual lease fees','Current energy lease fees','Current annual other fees',\\\n",
        "             'Total Annual Lease','(Average) residual duration','Maturity Cluster','ExCo rep. Avg Annual Lease costs',\\\n",
        "             'Total Energy Cost currrent FY (Energy provider + LL)','VOD (y/n)','Deutsche Telekom','Annual Fee per Tenant MNO1',\\\n",
        "             'Annual Energy Fee MNO1','Annual Maintenance Fee MNO1','Other Services Fee MNO1','Total Hosting Fee & Services MNO1',\\\n",
        "             'Residual duration MNO1','Maturity Clusters MNO1','Telefonica','Annual Fee per Tenant MNO2','Annual Energy Fee MNO2',\\\n",
        "             'Annual Maintenance Fee MNO2','Other Services Fee MNO2','Total Hosting Fee & Services MNO2','Residual duration MNO2',\\\n",
        "             'Maturity Clusters MNO2','E-Plus','Annual Fee per Tenant MNO3','Annual Energy Fee MNO3','Annual Maintenance Fee MNO3',\\\n",
        "             'Other Services Fee MNO3','Total Hosting Fee & Services','Residual duration MNO3','Maturity Clusters','# of OTMOs',\\\n",
        "             'Annual Fee from OTMOs','Annual Energy Fee from OTMOs','Annual Maintenance Fee OTMOs','Other Services Fee OTMOs',\\\n",
        "             'Total Hosting Fee & Services OTMOs','Average residual duration OTMOs','Maturity Clusters OTMOs','Total # of 3rd Party Tenants',\\\n",
        "             'Annual Fee from 3rd Party Tenants','Annual Energy Fee from 3rd Party Tenants','Annual Maintenance Fee from 3rd Party Tenants',\\\n",
        "             'Other Services Fee from 3rd Party Tenants','Total Hosting Fee & Services from 3rd Party Tenants',\\\n",
        "             'Weighted Average residual duration','Macro Cluster Tenancy','Macro Cluster Lease / Freeholds & Surface Right','Macro Cluster 1',\\\n",
        "             'Sites w/ at least a DDS (Lease Contract Type)','# of Tenants','Categorization by Tenant combination',\\\n",
        "             'Categorization by Type of Passive contracts','Categorization by Land/Surface ownership','In/Out',\\\n",
        "             'No of 3rd Party Tenants (PoP count)','Top cities classification','Type of Structure','Categorization by shared usage (contractual)',\\\n",
        "             'Categorization by Shared Usage (PoP count)','Security Class','Categorization by Sites with Single/Multiple Tower(s)',\\\n",
        "             'Categorization by Special Site/PLC','Categorization by Chimney-location','Categorization by Antenna Constraint',\\\n",
        "             'Categorization by 5G Roll-out Site','Categorization by Authority Site','Categorization by Transport-Sammler',\\\n",
        "             'Categorization by Strategic Site','Categorization by Dual Use Site','Categorization by Sublease Right','GBT Reserve Space','State',\\\n",
        "             'Postal code','Tower Height','Floor Space','Ground Register .1','Ground Register .2','Ground Register .3',\\\n",
        "             'Categorization by Critical Site','Site Cluster','infrastructure to be dismantled by','decommissioned site',\\\n",
        "             'Strategic Site Bucket','Bts_sites','CriticalSite_Beyond_10','Sites As Metered Estimated','Active Sharing Arrangement',\\\n",
        "             'Subsequent_Active_Sharing_Arrangement','First_Active_Sharing_Deployment_Type','First_Active_Sharing_Arrangement_Start_Date',\\\n",
        "             'First_Active_Sharing_Arrangement_End_Date','Indoor_Site_Any_Climate_Control','Outdoor_Site_With_Dc_Power','Rfai_Date',\\\n",
        "             'Cluster for non-enterprise DAS Sites','Site Status (TIMS)','Date_Of_Equipment_Removal'])\n",
        "\n",
        "new = pd.read_excel(path_excel, sheet_name='TowerDB', skiprows=12)\n",
        "new = new.iloc[:, 2:]\n",
        "new.columns = lower_str(list(new.columns))\n",
        "\n",
        "path_towerdb = '/content/TowerDB_Germany_20210731.csv'\n",
        "towerdb = pd.read_csv(path_towerdb,encoding='latin')\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "\n",
        "columns_to_merge = ['code', 'annual maintenance fee mno3', 'categorization by critical site', 'e-plus', 'categorization by antenna constraint', 'ground register .3', 'criticalsite_beyond_10', 'energy consumption current fy', 'first_active_sharing_deployment_type', 'indoor_site_any_climate_control', 'total hosting fee & services', 'subsequent_active_sharing_arrangement', 'categorization by indoor / outdoor site (antennas)', 'outdoor_site_with_dc_power', 'additional site type information', 'annual fee per tenant mno3', 'total energy cost currrent fy (energy provider + ll)', 'rfai_date', 'cluster for non-enterprise das sites', 'residual duration mno3', 'other services fee mno3', 'actual energy cost current fy', 'annual energy fee mno3', 'ground register .2', 'first_active_sharing_arrangement_start_date', 'ground register .1', 'first_active_sharing_arrangement_end_date', 'maturity clusters']\n",
        "df_merge = towerdb[columns_to_merge]\n",
        "new = pd.merge(new, df_merge, how='left', on='code')\n",
        "\n",
        "new = new[guideline]\n",
        "\n",
        "dates_tw = ['first_active_sharing_arrangement_start_date','first_active_sharing_arrangement_end_date', 'date_of_equipment_removal']\n",
        "for i in dates_tw:\n",
        "    new[i] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in new[i]]\n",
        "\n",
        "ints = ['actual energy cost current fy', 'counterpart', '# of lease contracts', 'current energy lease fees',\\\n",
        "        'total annual lease', 'deutsche telekom', 'annual energy fee mno1', 'total hosting fee & services mno1', 'telefonica',\\\n",
        "        'annual energy fee mno2', '# of otmos', 'annual energy fee from otmos', 'total hosting fee & services otmos',\\\n",
        "        'total # of 3rd party tenants', 'annual energy fee from 3rd party tenants', 'annual maintenance fee from 3rd party tenants',\\\n",
        "        'other services fee from 3rd party tenants', 'total hosting fee & services from 3rd party tenants',\\\n",
        "        'no of 3rd party tenants (pop count)', 'security class']\n",
        "for i in ints:\n",
        "    new[i] = new[i].fillna(0)\n",
        "    new[i] = list(map(int, new[i]))\n",
        "\n",
        "bill_col = lower_str([\"Code\",\"Categorization by Transmission Sys\",\"Categorization by Transmission Sys (sub-cluster)\",\\\n",
        "             \"Categorization by Site Type\",\\\n",
        "             \"Categorization by Strategic Site\",\"Categorization by Critical Site\",\"Site Cluster\",\\\n",
        "             \"Strategic Site Bucket\",\"Bts_sites\",\"CriticalSite_Beyond_10\",\"Sites As Metered Estimated\",\"Active Sharing Arrangement\",\\\n",
        "             \"Indoor_Site_Any_Climate_Control\",\"Outdoor_Site_With_Dc_Power\",\"Rfai_Date\",\"decommissioned site\",\\\n",
        "             \"Subsequent_Active_Sharing_Arrangement\",\"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Arrangement_Start_Date\",\\\n",
        "             \"First_Active_Sharing_Arrangement_End_Date\"])\n",
        "\n",
        "for i in bill_col:\n",
        "    new[i] = new[i].replace(['N/A', 'n/a',\"0\", 0, '-'], '')\n",
        "\n",
        "pathold = '/content/TowerDB_Germany_20210731.csv'\n",
        "to_save = '/content/DE_TW'\n",
        "old_n = '20210731.csv'\n",
        "new_n = 'Release6.3.xlsx' \n",
        "\n",
        "find_diffs_between_files(pathold, new, 'Code', bill_col, 'Site Status (TIMS)', to_save,old_n, new_n, 'else','tw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLXK-rEes90Z"
      },
      "source": [
        "UIS Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS3z1G2Vs9H_",
        "outputId": "2cc35115-0b42-4131-e0eb-92f8ae9c330b"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', dates_cols=[], status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    import re\n",
        "    from openpyxl import Workbook, styles\n",
        "    from openpyxl.styles import PatternFill, Font\n",
        "    from openpyxl.styles.differential import DifferentialStyle\n",
        "    from openpyxl.formatting.rule import Rule\n",
        "\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        kind = kind.lower()\n",
        "        if kind == 'ta':\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col] \n",
        "            #df.columns = lower_str(list(df.columns))\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "        t_col = type_dates.lower()\n",
        "        if format == 1:\n",
        "            type_date = \"%d/%m/%Y\"\n",
        "        else:\n",
        "            type_date = \"%d-%m-%Y\"\n",
        "        for column in lower_str(columns):\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,engine='python').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, engine='python').fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD) header=0, names=cols_old,\n",
        "        df_OLD = pd.read_csv(path_OLD, engine='python').fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        date_parser(df_NEW, dates_cols, 1, 'mixed')\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ25000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ25000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "uis_inmonth = '/content/UserInput_Germany_20210831_InMonth.xlsx'\n",
        "uis_old = '/content/UserInput_Germany_20210731_InMonth.xlsx'\n",
        "uis_trueup = '/content/UserInput_Germany_20210630_TrueUps.xlsx'\n",
        "sheet = 'SiteLevel'\n",
        "uis_index = 'Site_ID (Alphanumeric)'\n",
        "to_uis = '/content/DE_UIS_SiteLevel'\n",
        "old_uis = '20210731.xlsx'\n",
        "new_uis = '20210831_InMonth.xlsx'\n",
        "bill = []\n",
        "find_diffs_between_files(uis_trueup, uis_inmonth, uis_index, bill, to_uis, old_uis,new_uis,'excel',dates_cols=[],status_col='',\\\n",
        "                         kind='',kind_col='', sheetname=sheet, skipr=2, skipc=0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['0818 W', '0859 D', '1248 W', '1289 H', '2788 S', '4983 W', '5791 O', '7860 S', 'B003 M', 'MRT265']\n",
            "Dropped Rows: ['0644 S', '1130 M', '1620 S', '2890 W', '2965 S', '3394 S', '3597 S', '4512 S', '5478 S', '5660 S', '7117 S', '7299 H', 'D074 W', 'MRT10', 'MRT112', 'MRT126', 'MRT147']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCSBqwt1WEpA"
      },
      "source": [
        "# TA Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL4b97k6WC31",
        "outputId": "7eb4e9f3-edc4-4102-bee7-b25c2c4c92ff"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    from openpyxl import Workbook, styles\n",
        "    from openpyxl.styles import PatternFill, Font\n",
        "    from openpyxl.styles.differential import DifferentialStyle\n",
        "    from openpyxl.formatting.rule import Rule\n",
        "    import re\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col] + df[kind_col]\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        \"\"\"if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\"\"\"\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "        \"\"\"if kind=='ta':\n",
        "            #new_sites['site_code'] = [i.endswith('MEO') or i.endswith('NOS')]\n",
        "            new_copy['sites_code'] = [re.sub('MEO$|NOS$', '', str(x)) for x in new_copy['sites']]\n",
        "            new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\"\"\"\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "ta_bills = ['Code','Tenant','Starting date', 'Expiring date']\n",
        "path_ta_new = '/content/Output - Skylon - TowerDB - Germany - Release6.3 20210726_Billing_ReducedScope_sharedwithCelfinet_v03 (1).xlsx'\n",
        "path_ta_old = '/content/TA_Input_Germany_20210731.csv'\n",
        "ta_save = '/content/TA_DE'\n",
        "old = '20210630.csv'\n",
        "new = '20210731.csv'\n",
        "\"\"\"(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0)\"\"\"\n",
        "find_diffs_between_files(path_ta_old, path_ta_new, 'Tenant Agreement', ta_bills, ta_save, old, new,type_file='csv', status_col='',\\\n",
        "                         kind='ta', kind_col='Tenant')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     []\n",
            "Dropped Rows: ['/2500006069Telefónica Germany GmbH & Co. OHG', '2400003988/2500006284Landratsamt Niederschlesischer', '/2500006914EWE NETZ GmbH', '/2400000811Deutsche Funkturm GmbH', '/2400000810Deutsche Funkturm GmbH', '/2400000812Telekom Deutschland']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiCop-EHXeHC"
      },
      "source": [
        "LC Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJUePpkbXfaP"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col] + df[kind_col]\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        \"\"\"if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\"\"\"\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "        \"\"\"if kind=='ta':\n",
        "            #new_sites['site_code'] = [i.endswith('MEO') or i.endswith('NOS')]\n",
        "            new_copy['sites_code'] = [re.sub('MEO$|NOS$', '', str(x)) for x in new_copy['sites']]\n",
        "            new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\"\"\"\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "lc_bills = ['Lease Contract ID',' Annual lease fees ','Starting date', 'Expiring date']\n",
        "path_lc_new = '/content/LC_Input_Germany_20210731.csv'\n",
        "path_lc_old = '/content/LC_Input_Germany_20210630.csv'\n",
        "lc_save = '/content/LC_DE'\n",
        "old = '20210630.csv'\n",
        "new = '20210731.csv'\n",
        "\"\"\"(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0)\"\"\"\n",
        "find_diffs_between_files(path_lc_old, path_lc_new, 'Lease Contract ID', lc_bills, lc_save, old, new,type_file='csv', status_col='',\\\n",
        "                         kind='ta', kind_col='Counterpart')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}