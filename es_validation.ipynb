{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "es_validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvIYrVtJZRyS5rwwUcRJ2P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emerenan/xlsx_validation/blob/main/es_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4FndupnMVhY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index):\n",
        "    \"\"\"\n",
        "    Params:\\n\n",
        "    path: parth of file in the computer.\\n\n",
        "    n_skiprows: Number of rows to delete in the original file,.\\n\n",
        "    columns_to_convert: Columns to convert the data general type. \\n\n",
        "    n_skipcolumn: Columns to skip in the original file. \\n\n",
        "    endrow = pass 0 to read everything, 1 to count entire\n",
        "    columns_order: List of columns names in specific order to pass in the engine.\\n\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    \"\"\"cont = 0\n",
        "    for i in df.iloc[:,site_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[:cont, :]\n",
        "    #df.columns = columns_order\"\"\"\n",
        "    \n",
        "    # convert intery columns to integer \n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].fillna(0)\n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].astype('int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def check_df(df, error_msg):\n",
        "    if df == None or df.empty:\n",
        "        return f'{error_msg}'\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def count_duplicates(lista):\n",
        "    count_dict = {}\n",
        "    for entry in lista:\n",
        "        if entry in count_dict.keys():\n",
        "            count_dict[entry] += 1\n",
        "        else:\n",
        "            count_dict[entry] = 1\n",
        "    \n",
        "    duplicates = {}\n",
        "    for k, v in count_dict.items():\n",
        "        if v > 1:\n",
        "            duplicates[k] = v\n",
        "    return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "def defining_df(df, column_range, number_col):\n",
        "    cont = 0\n",
        "    for i in df.iloc[:,number_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[0:cont, :]\n",
        "    return df\n",
        "\n",
        "#old version\n",
        "def check_columns(table, output_columns):\n",
        "    \"\"\"\n",
        "    Check the total of number of missing columns and the missing columns in passed table.\\n\n",
        "\n",
        "    Params:\\n\n",
        "    table: contain the columns to be check.\\n\n",
        "    output_columns: columns structure at the final file. \n",
        "\n",
        "    Returns:\\n\n",
        "    Number of missing columns and a list that contains the name os missing columns.\n",
        "    \"\"\"\n",
        "    \"\"\"\"\n",
        "    countries = ['DE':{'towerDB':[],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':[]}\\,\n",
        "                 'HU',\n",
        "                 'IE',\n",
        "                 'RO',\n",
        "                 'PT',\n",
        "                 'ES',\n",
        "                 'CZ': {'towerDB':[\"Code (Duplicate)\",\"Site Status\",\"VF - In scope / out of scope (Generalised scoping)\",\"Site in Skylon scope Actual (From Site List Sheet )\",\"Legacy Site Code(Duplicate)\",\"TIMS Site Code\",\"Legacy Site Code\",\"Site Name\",\"Macro Region\",\"Province\",\"Municipality\",\"Inhabitants\",\"Address\",\"Ground Register\",\"Altitude\",\"Latitude\",\"Longitude\",\"Categorization by Inhabitants\",\"Categorization by Transmission Sys\",\"Categorization by Site Type\",\"Categorization by Transmission Sys (subcluster)\",\"Other internal Categorization 1 (Identify ACQ Sites)\",\"Other internal Categorization 2 Energy provider (Eon/ LL)\",\"DAS+Macro\",\"DAS (Yes/ No)\",\"DAS Ownership (Complete/ Partial/ 3rd Party)\",\"Active/ Passive DAS\",\"# of remote units/ radiating points\",\"Type of Structure\",\"Distance highest antenna to ground level\",\"GBT Tower height\",\"POD ID\",\"Energy Consumption LTM (kwh)\",\"Annual Energy cost LTM (Euros)\",\"Infrastructure ready (existing)/ to be ready (new)\",\"Infrastructure to be dismantled by\",\"Radio equipments to be deactivated by\",\"Infrastructure to be shared by\",\"Technology VOD\",\"Fibre / Microwave\",\"Vertical passive structure owner\",\"Room configuration (detailed)\",\"Shelter passive structure ownership\",\"Type of Air Conditioning\",\"Number of cabinets (Full Capacity)\",\"Number of Antenna (Full Capacity)\",\"Number of MW (Full Capacity)\",\"Counterpart\",\"# of Lease Contracts\",\"Current annual lease fees \",\"Current other fees (Maintenance)\",\"Current other fees\",\"(Average) residual duration - Lease contract\",\"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\"# of Tenants Agreements\",\"Current Total Annual Hosting Fees\",\"Tenant (name/ID) MNO1 (Česká telekomunikační infrastruktura a.s.)\",\"Annual Fee per Tenant MNO1\",\"Annual Energy Fee MNO1\",\"Annual Maintenance Fee MNO1\",\"Other Services Fee MNO1\",\"Residual duration MNO1 (Years)\",\"Tenant (name/ID) MNO2 (T-Mobile Czech Republic a.s.)\",\"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\"Annual Maintenance Fee MNO2\",\"Other Services Fee MNO2\",\"Residual duration MNO2 (days)\",\"Tenant (name/ID) MNO3\",\"Annual Fee per Tenant MNO3\",\"Annual Energy Fee MNO3\",\"Annual Maintenance Fee MNO3\",\"Other Services Fee MNO3\",\"Residual duration MNO3\",\"# of OTMOs\",\"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\"Annual Maintenance Fee OTMOs\",\"Other Services Fee OTMOs\",\"Average residual duration (days)\",\"Check\",\"Strategic Macro Sites\",\"Critical Sites\",\"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\"Transmission Sites\",\"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\"VF-CZ Demerger phase\",\"EVO Location [FAR Site ID] \",\"Billing Trigger date \",\\\n",
        "                 \"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\"Wip_Site\",\"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', 'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', 'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index','Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', 'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', 'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month','Updated Item', 'Comment', 'Counterpart_extra_1', 'Counterpart_extra_2', 'x','SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count', 'not_def_1']}\\,\n",
        "                 }\n",
        "                 'GR']\n",
        "                 \"\"\"\n",
        "    total_received = len(table.columns)\n",
        "    number_missing_columns = 0\n",
        "    missing_columns = []\n",
        "    #Counting of missing columns       \n",
        "    #if country in contries:      \n",
        "    for columns in output_columns:\n",
        "        if columns.lower() not in [labels.lower() for labels in table]:\n",
        "            number_missing_columns +=1\n",
        "            missing_columns.append(columns)\n",
        "    \n",
        "    return total_received, number_missing_columns, missing_columns\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    \"\"\"\n",
        "    for i in bill_cols:\n",
        "        if i not in twdb_col:\n",
        "            col_miss.append(i)\"\"\"\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", 0, '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "      \n",
        "def date_parser(df, columns, format, type_dates):\n",
        "    t_col = type_dates.lower()\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Refactorar esse codigo para receber todas as colunas num dic\n",
        "# Sendo as keys=columns e values= picklist for each column\n",
        "def check_date_columns(df, df_index, columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                if date_format.match(df_dates.loc[de_site,de_column]) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "                    else:\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            return df_de\n",
        "        else: \n",
        "            print('No one columns with incorrect date format!')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                if date_format.match(df_dates.loc[de_site,de_column]) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "                    else:\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            return df_de\n",
        "        else: \n",
        "            print('No one columns with incorrect date format!')\n",
        "\n",
        "def check_amounts(df_check, df_index, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "        \n",
        "def check_picklist(df,df_index,df_cols, picklist_dict):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        df:\n",
        "        df_index: \n",
        "        picklist_dict:\n",
        "        nultiIndex: More than one column to check\n",
        "    \"\"\"\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in picklist_dict.keys(): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            col_values = [i.lower() for i in picklist_dict[column]]\n",
        "            if not value.lower() in col_values or pd.isnull(value):\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "                else:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    #df = df_errors.dropna()   \n",
        "    df = df_errors.dropna(how='all', axis=1) \n",
        "    \n",
        "    return df\n",
        "\n",
        "def check_picklist_3(df,df_index, picklist_dict):\n",
        "    log = {}\n",
        "    for column in picklist_dict:\n",
        "        df_aux = df.copy()\n",
        "        new = df_aux[column].isin(picklist_dict[column])\n",
        "        #new = df_aux[column].apply(lambda x: x in picklist_dict[column])\n",
        "        # Aceita somento os valores que não estão na picklist\n",
        "        indexes = df.index[new == False].tolist()\n",
        "        #if len(indexes)>0:\n",
        "        if column not in log:log[column]=[]\n",
        "        log[column]=log[column]+indexes\n",
        "    #print(log.keys())\n",
        "\n",
        "    newDict ={}\n",
        "    df1 = pd.DataFrame()\n",
        "    for key,value in log.items():\n",
        "        for val in value:  \n",
        "            ID=df.iloc[val][df_index]\n",
        "            if ID in newDict:\n",
        "                newDict[ID].append(key)\n",
        "            else:\n",
        "                newDict[ID] = [key]\n",
        "        \n",
        "    logs = pd.DataFrame.from_dict(newDict, orient='index')\n",
        "    return logs\n",
        "\n",
        "#check on air foi trocado pelo check_picklist\n",
        "def check_on_air_sites(df, df_index, df_cols):\n",
        "\n",
        "    df_check = df[df_cols]\n",
        "    df_check['sites'] = df_check[df_index]\n",
        "    df_check = df_check.set_index('sites')\n",
        "    #df_check = replace_values(df_check, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in df_cols: \n",
        "            if df_check.loc[site,df_index] not in df_errors.index:\n",
        "                df_errors.loc[site,df_index] = df_check.loc[site,df_index]\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "            else:\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    df = df_errors[df_errors.iloc[:,1:]]\n",
        "    df = df.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    #df = df_errors.dropna(how='all', axis=1)   \n",
        "    return df\n",
        "\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [str(i) for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uip = [str(i) for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index].sort_values() if i not in uip_list]\n",
        "    bts_out_uip = pd.DataFrame(bts_out_uip, columns=['Bts_Sites_Out_UIP_File'])\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "\n",
        "    return new_sites, bts_out_uip, df_site_bts[[tw_index, bts_col, bill_col]]\n",
        "    \"\"\"Restruturar o script em CZ, DE, PT\"\"\"\n",
        "\n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    return df\n",
        "\n",
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[wip_tw]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, wip_tw, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "    \"\"\"Reestrurar os script em PT, DE, CZ\"\"\"\n",
        "    # Falta os outros países\n",
        "\n",
        "def check_decommissioned(df,df_index, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    return filtered[[df_index, decom_col, doer_col]]\n",
        "    \"\"\"Ajustar para CZ, DE, PT\"\"\"\n",
        "    \n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]') < current_date)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "        else:\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]').empty)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "    \"\"\"Ajustar para CZ, DE, PT, IE\"\"\"\n",
        "\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col, status_col):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)]\n",
        "        return filtered[[tw_index, status_col, doer_col]]  \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "                                                              \n",
        "def check_mom_bts(df_tw, tw_index, tw_col, df_msa,msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    return filtered[[tw_index, tw_col]]         \n",
        "\n",
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "\n",
        "        filtered = df[df[start_date] > df[end_date]]\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "    t1 = ['pt', 'de', 'cz', 'ie', 'es', 'ro', 'hu']\n",
        "    if country.lower() in t1:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        if decom_col != \"\":\n",
        "            tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "            decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "\n",
        "        \n",
        "            return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "        else:\n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "            return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "        decomiss_sites_in_uip = []\n",
        "        if set(tw_decomiss).intersection(uip_sites):\n",
        "            decomiss_sites_in_uip = [i for i in tw_decomiss if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(decomiss_sites_in_uip, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip = [i for i in df_uip['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Yes'][tw_index]\n",
        "        critical = []\n",
        "        if set(uip).intersection(bts_tw_critical):\n",
        "            critical = [i for i in bts_tw_critical if i not in uip]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% out in UIP File'])\n",
        "\n",
        "        return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    return df_commercial_diffs\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() \n",
        "\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, old_file, new_file, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = fit_df(path_OLD,bill_cols)\n",
        "        df_NEW = fit_df(path_NEW,bill_cols)\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_file}) vs ({new_file}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "187ZOjvNX27i"
      },
      "source": [
        "Looking for differences betwwnn older and newest files in towerdb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZmQg7JrX2S7"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', \n",
        "                             status_col='', kind='tw', dates=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col):\n",
        "        #print(df.head(1))\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "        t_col = type_dates.lower()\n",
        "        if format == 1:\n",
        "            type_date = \"%d/%m/%Y\"\n",
        "        else:\n",
        "            type_date = \"%d-%m-%Y\"\n",
        "        for column in lower_str(columns):\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new).fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,encoding='latin1').fillna('')\n",
        "        cols_old = lower_str(list(df_OLD.columns))\n",
        "        df_OLD.columns = cols_old\n",
        "        df_OLD = fit_df(df_OLD, index_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        for i in dates:\n",
        "            df_NEW[i] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) \\\n",
        "                         and not isinstance(date_obj, str) else date_obj for date_obj in df_NEW[i]]\n",
        "            df_NEW[i] = df_NEW[i].astype(str)\n",
        "            \n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW[index_col] = df_NEW[index_col].astype(str)\n",
        "        df_NEW['sites'] = df_NEW[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        #df_NEW = df_NEW[cols_old]\n",
        "\n",
        "        #df_NEW = fit_df(df_NEW, index_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "    \n",
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "pathold = '/content/TowerDB_Spain_20210731.csv'\n",
        "to_save = '/content/TW_ES'\n",
        "sheet= 'Enhanced TowerDB'\n",
        "skipr = 7\n",
        "skipc = 3\n",
        "old_n = 'TWDB_20210731.csv'\n",
        "new_n = 'TWDB_20210809.xlsx'\n",
        "bill_cols = ['Code',\\\n",
        "             'Inhabitants',\\\n",
        "             'Categorization by Transmission Sys',\\\n",
        "             'Categorization by Site Type',\\\n",
        "             'Vodafone equipment giving Active Sharing to Orange',\\\n",
        "             'Bundled_Sites_Yes_No',\\\n",
        "\t\t\t 'Wip_Site',\\\n",
        "\t\t\t 'Region_For_Tax_Calculation',\\\n",
        "\t\t\t 'Indoor_Outdoor_Categorization',\\\n",
        "\t\t\t 'Bts_Site',\\\n",
        "\t\t\t 'Sites_As_Metered_Estimated',\\\n",
        "\t\t\t 'Indoor_Site_Any_Climate_Control',\\\n",
        "\t\t\t 'Outdoor_Site_With__Power',\\\n",
        "\t\t\t 'Bundled_Site_Categorization',\\\n",
        "\t\t\t 'Strategic_Site',\\\n",
        "\t\t\t 'Strategic_Site_Bucket',\\\n",
        "\t\t\t 'Critical_Site',\\\n",
        "\t\t\t 'CriticalSite_Beyond_10',\\\n",
        "\t\t\t 'Active_Sharing_Arrangement',\\\n",
        "\t\t\t 'Orange_Crossed_Site',\\\n",
        "\t\t\t 'Das_Classification',\\\n",
        "\t\t\t 'Macro_Core_Site_Yes_No',\\\n",
        "\t\t\t 'Macro_Transmission_Hub_Yes_No',\\\n",
        "\t\t\t 'Macro_Transmission_Hub_With_Shelters_Without_Shelters',\\\n",
        "\t\t\t 'Transmission_With_Shelters_Without_Shelters',\\\n",
        "\t\t\t 'Subsequent_Sharing_Arrangement',\\\n",
        "\t\t\t 'First_Active_Sharing_Deployment_Type',\\\n",
        "             'First_Active_Sharing_Start_Date',\\\n",
        "             'First_Active_Sharing_End_Date',\\\n",
        "\t\t\t 'Decommissioned_Site only for VF',\\\n",
        "\t\t\t 'Sites_Fall_Under_2400',\\\n",
        "\t\t\t 'Macro_CoreA_CoreB',\\\n",
        "\t\t\t 'Billing Trigger date',\n",
        "             ]\n",
        "\n",
        "dates_tw = ['First_Active_Sharing_Start_Date','First_Active_Sharing_End_Date']\n",
        "\"\"\"(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', \n",
        "                             status_col='', kind='tw', dates=[], sheetname='', skipr=0, skipc=0)\"\"\"\n",
        "find_diffs_between_files(pathold, pathtw, 'Code', bill_cols,to_save, old_n, new_n, type_file='mix',status_col='On air / Active', \n",
        "                         kind='tw',dates=dates_tw, sheetname=sheet, skipr=7, skipc=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMNqI3p4S8M1"
      },
      "source": [
        "Adding new columns for UIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lulong55S_QC"
      },
      "source": [
        "# Lendo o Ficheiro de input\n",
        "path_uip=''\n",
        "sheet = 'SiteLevel'\n",
        "site_level = pd.read_excel(path_uip,sheet, header=[0,1,2])\n",
        "\n",
        "head_1 = pd.MultiIndex.from_product([[''],['Numeric (in months)'],['Delay in Site Modification Projects', 'Delay in BTS Projects']])\n",
        "df_1 = pd.DataFrame(columns=head_1)\n",
        "\n",
        "head_2 = pd.MultiIndex.from_product([[''],['Numeric (in €)'],['Excess of Upgrade Capital Expenditure over Threshold']])\n",
        "df_2 = pd.DataFrame(columns=head_2)\n",
        "\n",
        "site_level = pd.concat([site_level, df_1, df_2], axis=1)\n",
        "site_level.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFXAXm2TPPu2"
      },
      "source": [
        "Doing all checks in Towerdb File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mJZs4Ek2HyU",
        "outputId": "e096185d-cad7-4d4b-c57b-9f6ddaecf9f7"
      },
      "source": [
        "pathmsa = '/content/TowerDB_Spain_20210731.csv'\n",
        "msa = pd.read_csv(pathmsa, encoding='latin')\n",
        "msa.columns.to_list()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (112) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Code',\n",
              " 'Site Name',\n",
              " 'Macro Region',\n",
              " 'Region',\n",
              " 'Province',\n",
              " 'Municipality',\n",
              " 'Inhabitants',\n",
              " 'Address',\n",
              " 'Latitude',\n",
              " 'Longitude',\n",
              " 'Altitude',\n",
              " 'Categorization by Transmission Sys',\n",
              " 'Categorization by Transmission Sys_1',\n",
              " 'Categorization by Transmission Sys (sub-cluster)',\n",
              " 'MSA Type updated',\n",
              " 'Categorization by Site Type',\n",
              " 'Categorization by inhabitants',\n",
              " 'Rural/ Suburban/ Urban',\n",
              " 'Categorization by CONNECTIVITY as per MSA Site List 31/03',\n",
              " 'POD ID',\n",
              " \"Energy Consumption feb'20-jan´21\",\n",
              " \"Actual Energy cost feb'20-jan´21\",\n",
              " 'Infrastructure ready (existing)/ to be ready (new)',\n",
              " 'Infrastructure to be shared by',\n",
              " 'Counterpart',\n",
              " '# of Lease Contracts',\n",
              " 'Current annual lease fees ',\n",
              " 'Current energy lease fees ',\n",
              " 'Current annual other fees ',\n",
              " 'Total Annual Lease',\n",
              " '(Average) residual duration',\n",
              " 'Maturity Cluster',\n",
              " 'ExCo rep. Avg Annual Lease costs',\n",
              " 'Total Energy Cost (Energy provider + LL)',\n",
              " 'VOD (y/n)',\n",
              " 'ORANGE',\n",
              " 'Annual Fee per Tenant MNO1',\n",
              " 'Annual Energy Fee MNO1',\n",
              " 'Annual Maintenance Fee MNO1',\n",
              " 'Other Services Fee MNO1',\n",
              " 'Total Hosting Fee & Services',\n",
              " 'Residual duration MNO1',\n",
              " 'Maturity Clusters',\n",
              " 'Telefonica',\n",
              " 'Annual Fee per Tenant MNO2',\n",
              " 'Annual Energy Fee MNO2',\n",
              " 'Annual Maintenance Fee MNO2',\n",
              " 'Other Services Fee MNO2',\n",
              " 'Total Hosting Fee & Services.1',\n",
              " 'Residual duration MNO2',\n",
              " 'Maturity Clusters.1',\n",
              " 'YOIGO',\n",
              " 'Annual Fee per Tenant MNO3',\n",
              " 'Annual Energy Fee MNO3',\n",
              " 'Annual Maintenance Fee MNO3',\n",
              " 'Other Services Fee MNO3',\n",
              " 'Total Hosting Fee & Services.2',\n",
              " 'Residual duration MNO3',\n",
              " 'Maturity Clusters.2',\n",
              " '# of OTHERs',\n",
              " 'Annual Fee from OTHERs',\n",
              " 'Annual Energy Fee from OTHERs',\n",
              " 'Annual Maintenance Fee OTHERs',\n",
              " 'Other Services Fee OTHERs',\n",
              " 'Total Hosting Fee & Services.3',\n",
              " 'Average residual duration',\n",
              " 'Maturity Clusters.3',\n",
              " 'Total # of 3rd Party Tenants',\n",
              " 'Annual Fee from 3rd Party Tenants',\n",
              " 'Annual Energy Fee from 3rd Party Tenants',\n",
              " 'Annual Maintenance Fee from 3rd Party Tenants',\n",
              " 'Other Services Fee from 3rd Party Tenants',\n",
              " 'Total Hosting Fee & Services from 3rd Party Tenants',\n",
              " 'Waighted Average residual duration',\n",
              " 'Macro Cluster Tenancy',\n",
              " 'Macro Cluster Lease / Freeholds & Surface Right',\n",
              " 'Macro Cluster 1',\n",
              " 'Sites w/ at list a DDS (Lease Contract Type)',\n",
              " '# of Tenants',\n",
              " 'Categorization by Tenant combination',\n",
              " 'Categorization by Type of Passive contracts',\n",
              " 'Categorization by Land/Surface ownership',\n",
              " 'Zero Cost Ground Lease',\n",
              " 'In/Out',\n",
              " 'Active Sharing. VF Equipment & OSP Equipment',\n",
              " 'Core sites',\n",
              " 'Vodafone Office /Vodafone Shop sites',\n",
              " 'Freehold sites',\n",
              " 'X',\n",
              " 'VF Only Rural/Urban',\n",
              " 'VF Only GBT/RTT',\n",
              " 'VF Only Zone',\n",
              " 'on air / active',\n",
              " \"Estimated + Real Energy Consumption nov'19-oct´20\",\n",
              " \"Estimated + Real Actual Energy cost nov'19-oct'20\",\n",
              " 'Vodafone equipment giving Active Sharing to Orange',\n",
              " 'Bts_Site',\n",
              " 'Sites_As_Metered_Estimated',\n",
              " 'Indoor_Site_Any_Climate_Control',\n",
              " 'Outdoor_Site_With__Power',\n",
              " 'Bundled_Sites_Yes_No',\n",
              " 'Bundled_Site_Categorization',\n",
              " 'Strategic_Site',\n",
              " 'Strategic_Site_Bucket',\n",
              " 'Critical_Site',\n",
              " 'CriticalSite_Beyond_10',\n",
              " 'Wip_Site',\n",
              " 'Active_Sharing_Arrangement',\n",
              " 'Subsequent_Sharing_Arrangement',\n",
              " 'First_Active_Sharing_Deployment_Type',\n",
              " 'First_Active_Sharing_Start_Date',\n",
              " 'First_Active_Sharing_End_Date',\n",
              " 'Billing Trigger date',\n",
              " 'Decommissioned_Site only for VF',\n",
              " 'Sites_Fall_Under_2400',\n",
              " 'Region_For_Tax_Calculation',\n",
              " 'Orange_Crossed_Site',\n",
              " 'Indoor_Outdoor_Categorization',\n",
              " 'Das_Classification',\n",
              " 'Macro_Core_Site_Yes_No',\n",
              " 'Macro_CoreA_CoreB',\n",
              " 'Macro_Transmission_Hub_Yes_No',\n",
              " 'Macro_Transmission_Hub_With_Shelters_Without_Shelters',\n",
              " 'Transmission_With_Shelters_Without_Shelters',\n",
              " 'Date of equipment removal (from MAR´21)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe3gnYrg1iFF",
        "outputId": "eb94600b-b0f6-4cec-ef49-086983b92705"
      },
      "source": [
        "towerdb.columns.to_list()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Code',\n",
              " 'Site Name',\n",
              " 'Macro Region',\n",
              " 'Region',\n",
              " 'Province',\n",
              " 'Municipality',\n",
              " 'Inhabitants',\n",
              " 'Address',\n",
              " 'Latitude',\n",
              " 'Longitude',\n",
              " 'Altitude',\n",
              " 'Categorization by Transmission Sys',\n",
              " 'Categorization by Transmission Sys.1',\n",
              " 'Categorization by Transmission Sys (sub-cluster)',\n",
              " 'MSA Type updated',\n",
              " 'Categorization by Site Type',\n",
              " 'Categorization by inhabitants',\n",
              " 'Rural/ Suburban/ Urban',\n",
              " 'Categorization by CONNECTIVITY as per MSA Site List 31/03',\n",
              " 'POD ID',\n",
              " \"Energy Consumption feb'20-jan´21\",\n",
              " \"Actual Energy cost feb'20-jan´21\",\n",
              " 'Infrastructure ready (existing)/ to be ready (new)',\n",
              " 'Infrastructure to be shared by',\n",
              " 'Counterpart',\n",
              " '# of Lease Contracts',\n",
              " 'Current annual lease fees ',\n",
              " 'Current energy lease fees ',\n",
              " 'Current annual other fees ',\n",
              " 'Total Annual Lease',\n",
              " '(Average) residual duration',\n",
              " 'Maturity Cluster',\n",
              " 'ExCo rep. Avg Annual Lease costs',\n",
              " 'Total Energy Cost (Energy provider + LL)',\n",
              " 'VOD (y/n)',\n",
              " 'ORANGE',\n",
              " 'Annual Fee per Tenant MNO1',\n",
              " 'Annual Energy Fee MNO1',\n",
              " 'Annual Maintenance Fee MNO1',\n",
              " 'Other Services Fee MNO1',\n",
              " 'Total Hosting Fee & Services',\n",
              " 'Residual duration MNO1',\n",
              " 'Maturity Clusters',\n",
              " 'Telefonica',\n",
              " 'Annual Fee per Tenant MNO2',\n",
              " 'Annual Energy Fee MNO2',\n",
              " 'Annual Maintenance Fee MNO2',\n",
              " 'Other Services Fee MNO2',\n",
              " 'Total Hosting Fee & Services.1',\n",
              " 'Residual duration MNO2',\n",
              " 'Maturity Clusters.1',\n",
              " 'YOIGO',\n",
              " 'Annual Fee per Tenant MNO3',\n",
              " 'Annual Energy Fee MNO3',\n",
              " 'Annual Maintenance Fee MNO3',\n",
              " 'Other Services Fee MNO3',\n",
              " 'Total Hosting Fee & Services.2',\n",
              " 'Residual duration MNO3',\n",
              " 'Maturity Clusters.2',\n",
              " '# of OTHERs',\n",
              " 'Annual Fee from OTHERs',\n",
              " 'Annual Energy Fee from OTHERs',\n",
              " 'Annual Maintenance Fee OTHERs',\n",
              " 'Other Services Fee OTHERs',\n",
              " 'Total Hosting Fee & Services.3',\n",
              " 'Average residual duration',\n",
              " 'Maturity Clusters.3',\n",
              " 'Total # of 3rd Party Tenants',\n",
              " 'Annual Fee from 3rd Party Tenants',\n",
              " 'Annual Energy Fee from 3rd Party Tenants',\n",
              " 'Annual Maintenance Fee from 3rd Party Tenants',\n",
              " 'Other Services Fee from 3rd Party Tenants',\n",
              " 'Total Hosting Fee & Services from 3rd Party Tenants',\n",
              " 'Waighted Average residual duration',\n",
              " 'Macro Cluster Tenancy',\n",
              " 'Macro Cluster Lease / Freeholds & Surface Right',\n",
              " 'Macro Cluster 1',\n",
              " 'Sites w/ at list a DDS (Lease Contract Type)',\n",
              " '# of Tenants',\n",
              " 'Categorization by Tenant combination',\n",
              " 'Categorization by Type of Passive contracts',\n",
              " 'Categorization by Land/Surface ownership',\n",
              " 'Zero Cost Ground Lease',\n",
              " 'In/Out',\n",
              " 'Active Sharing. VF Equipment & OSP Equipment',\n",
              " 'Core sites',\n",
              " 'Vodafone Office /Vodafone Shop sites',\n",
              " 'Freehold sites',\n",
              " 'X',\n",
              " 'VF Only Rural/Urban',\n",
              " 'VF Only GBT/RTT',\n",
              " 'VF Only Zone',\n",
              " 'on air / active',\n",
              " \"Estimated + Real Energy Consumption nov'19-oct´20\",\n",
              " \"Estimated + Real Actual Energy cost nov'19-oct'20\",\n",
              " 'Vodafone equipment giving Active Sharing to Orange',\n",
              " 'Bts_Site',\n",
              " 'Sites_As_Metered_Estimated',\n",
              " 'Indoor_Site_Any_Climate_Control',\n",
              " 'Outdoor_Site_With__Power',\n",
              " 'Bundled_Sites_Yes_No',\n",
              " 'Bundled_Site_Categorization',\n",
              " 'Strategic_Site',\n",
              " 'Strategic_Site_Bucket',\n",
              " 'Critical_Site',\n",
              " 'CriticalSite_Beyond_10',\n",
              " 'Wip_Site',\n",
              " 'Active_Sharing_Arrangement',\n",
              " 'Subsequent_Sharing_Arrangement',\n",
              " 'First_Active_Sharing_Deployment_Type',\n",
              " 'First_Active_Sharing_Start_Date',\n",
              " 'First_Active_Sharing_End_Date',\n",
              " 'Billing Trigger date',\n",
              " 'Decommissioned_Site only for VF',\n",
              " 'Sites_Fall_Under_2400',\n",
              " 'Region_For_Tax_Calculation',\n",
              " 'Orange_Crossed_Site',\n",
              " 'Indoor_Outdoor_Categorization',\n",
              " 'Das_Classification',\n",
              " 'Macro_Core_Site_Yes_No',\n",
              " 'Macro_CoreA_CoreB',\n",
              " 'Macro_Transmission_Hub_Yes_No',\n",
              " 'Macro_Transmission_Hub_With_Shelters_Without_Shelters',\n",
              " 'Transmission_With_Shelters_Without_Shelters',\n",
              " 'Date of equipment removal (from MAR´21)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV5q5ZwHQAgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4fbb2b5-b04f-4ff5-c606-10b75cca4e32"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "sheet= 'Enhanced TowerDB'\n",
        "skipr = 7\n",
        "skipc = 3\n",
        "towerdb = read_files(pathtw, sheet, skipr, skipc, 'Code')\n",
        "towerdb = towerdb.rename(columns={'Categorization by Transmission Sys.1': 'Categorization by Transmission Sys_1'})\n",
        "#towerdb.columns = lower_str(list(towerdb.columns))\n",
        "\n",
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = 'Code'\n",
        "tw_doer = 'Date of equipment removal (from MAR´21)'\n",
        "tw_status = 'on air / active'\n",
        "tw_bts = 'Bts_Site'\n",
        "tw_bill = 'Billing Trigger date'\n",
        "tw_wip = 'Wip_Site'\n",
        "tw_decom = 'Decommissioned_Site only for VF'\n",
        "#w_amount = 'Lease Contract - Current annual lease fee'\n",
        "tw_critical = 'CriticalSite_Beyond_10'\n",
        "\n",
        "pathmsa = '/content/TowerDB_Spain_20210731.csv'\n",
        "msa = pd.read_csv(pathmsa, encoding='latin')\n",
        "#msa.columns = lower_str(list(msa.columns))\n",
        "msa_index = 'Code'\n",
        "msa_bts = 'Bts_Site'\n",
        "msa_doer = 'Date of equipment removal (from MAR´21)'\n",
        "msa_status = 'on air / active'\n",
        "msa_bill = 'Billing Trigger date'\n",
        "msa_wip = 'Wip_Site'\n",
        "msa_decom = 'Decommissioned_Site only for VF'\n",
        "#msa_amount = 'Lease Contract - Current annual lease fee'\n",
        "msa_critical = 'CriticalSite_Beyond_10'\n",
        "\n",
        "col_order = list(msa.columns)\n",
        "\n",
        "towerdb = towerdb[col_order]\n",
        "#towerdb = towerdb.fillna('')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (112) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_CmMZIVVWBB"
      },
      "source": [
        "Check columns received looking for missing columns that is gonna be used in rating engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wblfWhtLTeQX"
      },
      "source": [
        "\"\"\"Check Columns Received\"\"\"\n",
        "df_cols = check_columns_received(towerdb, col_order)\n",
        "df_cols\n",
        "#No columns missing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAGJs483VchY"
      },
      "source": [
        "First Check - Dates Formats (dd/mm/YYYY) \n",
        "\n",
        "Columns: Date of equipment removal (from MAR´21)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "2U68LurB1Bkp",
        "outputId": "b410b9a3-6621-492e-c2af-ade1e903cc30"
      },
      "source": [
        "towerdb[tw_bill]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Billing Trigger date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5e8a804e7076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtowerdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtw_bill\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Billing Trigger date'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEZFbjGvVzZ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "ce260bec-b2d9-40a9-ddc8-44ab4a80d561"
      },
      "source": [
        "\"\"\"You need to convert all values in cols for string format to check\"\"\"\n",
        "def date_parser(df, columns, format, type_dates):\n",
        "    t_col = type_dates.lower()\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Columns to functions\n",
        "dates_bill = [tw_index, tw_bill]\n",
        "dates_doer = [tw_index, tw_doer]\n",
        "#Columns to parser\n",
        "bill=[tw_bill]\n",
        "doer=[tw_doer]\n",
        "\n",
        "date_parser(towerdb, bill, \"%d/%m/%Y\", 'mixed')  #não precisar checkar o bill dos sites in service\n",
        "date_parser(towerdb, doer, \"%d/%m/%Y\", 'mixed')\n",
        "\n",
        "actives_1 = towerdb[towerdb['on air / active']=='In Service']\n",
        "no_actives_1 = towerdb[~(towerdb[tw_status]=='In Service')]\n",
        "\n",
        "#Checking columns for errors\n",
        "actives_dates_errors = check_date_columns(actives_1, tw_index, dates_bill, 2) \n",
        "# Actives sites with blank billing trigger date\n",
        "no_actives_dates_errors = check_date_columns(no_actives_1, tw_index, dates_doer, 2) \n",
        "# No Actives sites with blank Date of Equipament Removal"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Billing Trigger date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-27064f332e26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdoer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtw_doer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdate_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtowerdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%d/%m/%Y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#não precisar checkar o bill dos sites in service\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdate_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtowerdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%d/%m/%Y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-27064f332e26>\u001b[0m in \u001b[0;36mdate_parser\u001b[0;34m(df, columns, format, type_dates)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt_col\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mixed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdate_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_obj\u001b[0m\u001b[0;34m)\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdate_obj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate_obj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Billing Trigger date'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "MI1wLJ8g0y54",
        "outputId": "6226ad2d-9894-44c6-f489-2c3f9cc68f7c"
      },
      "source": [
        "#Checking columns for errors\n",
        "actives_1 = towerdb[towerdb['on air / active']=='In Service']\n",
        "no_actives_1 = towerdb[~(towerdb[tw_status]=='In Service')]\n",
        "\n",
        "actives_dates_errors = check_date_columns(actives_1, tw_index, dates_bill, 2) \n",
        "# Actives sites with blank billing trigger date\n",
        "no_actives_dates_errors = check_date_columns(no_actives_1, tw_index, dates_doer, 2) \n",
        "actives_dates_errors"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3f2aae56e353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mno_actives_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtowerdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtowerdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtw_status\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'In Service'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mactives_dates_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_date_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactives_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates_bill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Actives sites with blank billing trigger date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mno_actives_dates_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_date_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_actives_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates_doer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dates_bill' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YQEtZ0Ni7Vm"
      },
      "source": [
        "Second Check - TW Amount value General (xxx.xx)\n",
        "\n",
        "Column(s): ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtpdwTwDi7Fd"
      },
      "source": [
        "amount_cols = [tw_index, \"\"\"???\"\"\"]\n",
        "df_amount_errors = check_amounts(actives, tw_index, amount_cols, 1)\n",
        "#No one error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2LrHKSGjj9t"
      },
      "source": [
        "Thirth - Check Picklist values All sites\n",
        "\n",
        "Do this check in all sites\n",
        "\n",
        "Check the picklist for each case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoS6fLsXjykD"
      },
      "source": [
        "picklist_tw_general = {\n",
        "    'Categorization by Transmission Sys' : ['Macro', 'Public DAS']\n",
        "}\n",
        "pick_col_general = ['Code', 'Categorization by Transmission Sys']\n",
        "\n",
        "df_general_pick = check_picklist(towerdb, tw_index, pick_col_general, picklist_tw_general)\n",
        "df_general_pick\n",
        "# Tem sites com e sem a flag filtrar na planilha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v4oVgDimF4S"
      },
      "source": [
        "Fourth Check - Remove \"N/A\", \"0\" or \"-\" values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWlMEo9LLbDn"
      },
      "source": [
        "towerdb = replace_values(towerdb, col_order, value=\"\")\n",
        "actives = replace_values(actives, col_order, value=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uoBfMK4mZt_"
      },
      "source": [
        "Fifth Check MoM Sites (BTS, decomissoned...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J7-o7qSmZUE"
      },
      "source": [
        "\"\"\"Falta Coluna de Flag Indicating BTS Site no twerdb recebido\"\"\"\n",
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "df_mom_bts = check_mom_bts(actives, tw_index, tw_bts, msa, msa_index, msa_bts)\n",
        "# No one error df_mom_bts\n",
        "\n",
        "decomiss = towerdb[towerdb[tw_status]=='Decommissioned']\n",
        "df_mom_decom = check_mom_bts(decomiss, tw_index, tw_decom, msa, msa_index, msa_decom)\n",
        "# No one error df_mom_decom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kyAO221L8Uu"
      },
      "source": [
        "Check Picklist and dates formats for In service sites\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOzkwTx1MGa6"
      },
      "source": [
        "picklis_dict = {\n",
        "    'Categorization by Transmission Sys': ['Macro', 'Public DAS'],\n",
        "    'Categorization by Site Type': ['DAS passive','GBT','RTT'],\n",
        "    'Sites_As_Metered_Estimated': ['Estimated Model','Metered Model'],\n",
        "    'Infrastructure ready (existing)/ to be ready (new)': [''],\n",
        "    'Indoor_Site_Any_Climate_Control': ['No','Yes; Indoor Air Conditioning','Yes; Indoor Air Conditioning and Free Air cooling / Free cooling units'],\n",
        "    'Bts_Site': ['Yes', 'No'],\n",
        "    'Strategic_Site': ['Yes', 'No'],\n",
        "    'Strategic_Site_Bucket': ['Yes - first 460', ''],\n",
        "    'Critical_Site': ['Yes', 'No'],\n",
        "    'CriticalSite_Beyond_10': ['Within 10%','Non Critical'],\n",
        "    'Wip_Site': ['Yes', 'No'],\n",
        "    'Decommissioned_Site only for VF': ['Yes', 'No'],\n",
        "    'First_Active_Sharing_Deployment_Type': ['']\n",
        "}\n",
        "\n",
        "pick_cols = ['Code','Categorization by Transmission Sys','Categorization by Site Type','Sites_As_Metered_Estimated',\\\n",
        "             'Infrastructure ready (existing)/ to be ready (new)','Indoor_Site_Any_Climate_Control','Bts_Site',\\\n",
        "             'Strategic_Site','Strategic_Site_Bucket','Critical_Site','CriticalSite_Beyond_10','Wip_Site',\\\n",
        "             'Decommissioned_Site only for VF','First_Active_Sharing_Deployment_Type' ]\n",
        "actives = actives.fillna('')\n",
        "df_in_service_picklist = check_picklist(actives, tw_index, pick_cols, picklis_dict)\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQj9BsQKSHfZ"
      },
      "source": [
        "#check dates in columns\n",
        "start_dates = ['First_Active_Sharing_Start_Date']\n",
        "\n",
        "date_parser(actives, start_dates, \"%d/%m/%Y\", 'no')\n",
        "actives['First_Active_Sharing_End_Date'] = actives['First_Active_Sharing_End_Date'].fillna('')\n",
        "in_service_cols = ['Code', 'First_Active_Sharing_Start_Date', 'First_Active_Sharing_End_Date']\n",
        "df_in_service_dates = check_date_columns(actives, tw_index, in_service_cols, 2)\n",
        "# tem errors em df_in_service_dates sites ativos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF9HkIPBZ3va"
      },
      "source": [
        "Fifth Check BTS Flagged(Billing Trigger and Commercial)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsLeIodqVW4c"
      },
      "source": [
        "status = 'Yes'\n",
        "df_bts_flagged = check_tw_bill_doer(towerdb, tw_index,tw_bill, tw_bts, status, 'doer')\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePYDr-9PcqOg"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "path_uip = '/content/UserInput_Spain_202106 v1.xlsx'\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites(Annual)']\n",
        "uip = pd.read_excel(path_uip ,sheet_name='SiteLevel',usecols=[0,1,2],skiprows=2)\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [i for i in msa[msa_index]]\n",
        "tw_sites = [i for i in towerdb[tw_index]]\n",
        "uip_sites = [i for i in uip['Site_ID']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eow-oO-Ndhxl"
      },
      "source": [
        "new_sites, bts_out_uip, df_bts_errors = check_new_sites(towerdb, tw_index, tw_bts, tw_bill, msa_sites, tw_sites, uip_sites)\n",
        "# New sites = 132\n",
        "# Error on BTS sites out of UIP File"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgLjHpQFLupb"
      },
      "source": [
        "Sixtith Check on air sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYF3LphxLvIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91aa003c-0e34-4afb-8b0f-fd068f775788"
      },
      "source": [
        "df_on_air = check_on_air_sites(actives, tw_index, pick_cols)\n",
        "# NO Errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:303: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb7vr-l6nO3Z"
      },
      "source": [
        "Check Wip SItes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgxfEk5qnKdd"
      },
      "source": [
        "wip_out_tw_list, df_wip_and_bts_flagged = check_wip(towerdb,tw_index, tw_wip, tw_bts, msa, msa_index, msa_wip)\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZpkKgsNn5vR"
      },
      "source": [
        "Check Decomissioned sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it4pR9Srn1zw"
      },
      "source": [
        "df_decom_sites = check_decommissioned(towerdb, tw_index, tw_decom, tw_doer)\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL92Kkk1osOj"
      },
      "source": [
        "Check Doer columns for in service sites\n",
        "\n",
        "Should not to be in past or different of blank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPbkQQC8oroV"
      },
      "source": [
        "#COluna Doer tem valores fora do formato\n",
        "df_doer = check_tw_bill_doer(actives, tw_index, tw_doer, tw_bts, 'Yes', 'doer')\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPWyxksEyBQp"
      },
      "source": [
        "*Tenth* - Check UIP Towerdb matches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOub5bqnxKqo"
      },
      "source": [
        "in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical = check_uip_tw(towerdb,tw_index, tw_status, \\\n",
        "                                                              tw_decom, tw_bts,tw_critical, \\\n",
        "                                                              uip, uip_sites, 'gr')\n",
        "# in_service_uip_sites sites out UIP Files\n",
        "# bts_sites_out_uip BTS sites out UIP FIles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVELEIlf0EGZ"
      },
      "source": [
        "Commercial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRSJ-p5Vz836"
      },
      "source": [
        "\n",
        "path_before = '/content/UserInput_Spain_20210630.xlsx'\n",
        "names = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', 'Input_Value',\\\n",
        "        'Description/Instruction', 'Frequency of Update']\n",
        "merge_cols = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type', \\\n",
        "              'Description/Instruction', 'Frequency of Update']\n",
        "cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2','Data_Type','Input_Value_actual',\\\n",
        "                'Input_Value_before','Equal Values','Description/Instruction', 'Frequency of Update']\n",
        "df_commercial_diffs = check_commercial(path_uip, path_before, 'Input_Value', names, merge_cols, cols_ordered)\n",
        "#NO errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze_QyJXE7l9o",
        "outputId": "596b65a3-c722-466d-933a-2693bbb48040"
      },
      "source": [
        "df_list = [actives_dates_errors, no_actives_dates_errors,  bts_out_uip]\n",
        "sheetnames = ['Actives Sites without Billin trigger Date', 'Not In Service without DOER','BTS Flagged in Towerdb out of UIP File']\n",
        "\n",
        "path = '/content/towerdb_es_errors.xlsx'\n",
        "general_log_erros(df_list, sheetnames, path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/openpyxl/workbook/child.py:102: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
            "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13M_kfik0zsh"
      },
      "source": [
        "Lc Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqwvl8K60yKr"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210708.xlsx'\n",
        "sheet= 'LC_INPUT_SPAIN'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "lc = pd.read_excel(pathtw, sheet)\n",
        "\n",
        "lc_cols = ['Código', '   Importe anual']\n",
        "lc['Importe anual'] = lc['   Importe anual'].astype(str)\n",
        "\n",
        "#Python interpretou como float a coluna de Import Anual\n",
        "df_lc_amount = check_amounts(lc, 'Código', lc_cols)\n",
        "# No errors\n",
        "\n",
        "df_lc_dates = check_lc_ta_dates(lc,'Código', 'Inicio','Fin vigen/')\n",
        "#NO erros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzMgwJR6iZ62"
      },
      "source": [
        "df_lc_amount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAHLU4YMwwpi"
      },
      "source": [
        "Looking for differences between newest and oldest LC file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXLR_77BucBD"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210708.xlsx'\n",
        "pathold = '/content/TowerDB_Spain_20210617 v2.xlsx'\n",
        "to_save_lc = '/content/lc_'\n",
        "sheet_lc= 'LC_INPUT_SPAIN'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "lc_bill = ['Código', '   Importe anual','Inicio','Fin vigen/']\n",
        "find_diffs_between_files(pathold, pathtw, 'Código', lc_bill,'',to_save_lc, 'excel','lc',sheet_lc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4jp7suZ1BAQ"
      },
      "source": [
        "TA OSP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br86vCWY1Aia"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210708.xlsx'\n",
        "sheet= 'TA_Input_Lease_OSP_Spain'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "ta_bill = ['Código', 'Importe anual','Inicio','F/cie/tec/']\n",
        "ta_osp = pd.read_excel(pathtw, sheet)\n",
        "\n",
        "ta_osp = ta_osp.fillna('')\n",
        "ta_osp = ta_osp[ta_osp['Inicio']!=\"\"]\n",
        "\n",
        "\n",
        "ta_osp_cols = ['Código', 'Importe anual']  \n",
        "\n",
        "ta_osp['Importe anual'] = ta_osp['Importe anual'].astype(str)\n",
        "\n",
        "df_ta_osp_amount = check_amounts(ta_osp, 'Código', ta_osp_cols)\n",
        "# No errors\n",
        "\n",
        "df_ta_osp_dates = check_lc_ta_dates(ta_osp,'Código', 'Inicio','F/cie/tec/')\n",
        "#NO erros\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cigSffhh1K8w"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210708.xlsx'\n",
        "sheet= 'TA_Input_Lease_TME_Spain'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "ta_tme = ta_tme.fillna('')\n",
        "ta_tme = ta_tme[ta_tme['Inicio']!=\"\"]\n",
        "ta_tme = pd.read_excel(pathtw, sheet)\n",
        "\n",
        "\n",
        "ta_tme_cols = ['Código', 'Importe anual']  \n",
        "\n",
        "ta_tme['Importe anual'] = ta_tme['Importe anual'].astype(str)\n",
        "df_ta_tme_amount = check_amounts(ta_tme, 'Código', ta_tme_cols)\n",
        "# No errors\n",
        "\n",
        "df_ta_tme_dates = check_lc_ta_dates(ta_tme,'Código', 'Inicio','F/cie/tec/')\n",
        "#NO erros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXtLlVp91MJL"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210708.xlsx'\n",
        "sheet= 'TA_Input_Lease_MM_Spain'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "ta_mm = pd.read_excel(pathtw, sheet)\n",
        "\n",
        "ta_mm = ta_mm.fillna('')\n",
        "ta_mm = ta_mm[ta_mm['Inicio']!=\"\"]\n",
        "\n",
        "ta_mm_cols = ['Código', 'Importe anual']  \n",
        "\n",
        "ta_mm['Importe anual'] = ta_mm['Importe anual'].astype(str)\n",
        "df_ta_mm_amount = check_amounts(ta_mm, 'Código', ta_mm_cols)\n",
        "# No errors\n",
        "\n",
        "df_ta_mm_dates = check_lc_ta_dates(ta_mm,'Código', 'Inicio','F/cie/tec/')\n",
        "#NO erros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vquhmwrK1MX7"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210708.xlsx'\n",
        "sheet= 'TA_Input_Lease_Others_Spain'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "ta_others = pd.read_excel(pathtw, sheet)\n",
        "\n",
        "ta_others = ta_others.fillna('')\n",
        "ta_others = ta_others[ta_others['Inicio']!=\"\"]\n",
        "\n",
        "ta_o_cols = ['Código', 'Importe anual']\n",
        "\n",
        "ta_others['Importe anual'] = ta_others['Importe anual'].astype(str)\n",
        "df_ta_o_amount = check_amounts(ta_others, 'Código', ta_o_cols)\n",
        "# No errors\n",
        "\n",
        "df_ta_o_dates = check_lc_ta_dates(ta_others,'Código', 'Inicio','F/cie/tec/')\n",
        "#NO erros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNvIvZOUPvZ7"
      },
      "source": [
        "Looking for differences between newest and oldest TA files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9GJuqnRwlf3"
      },
      "source": [
        "sheet_osp = 'TA_Input_Lease_OSP_Spain'\n",
        "to_save_osp = '/content/ta_osp_'\n",
        "sheet_tme = 'TA_Input_Lease_TME_Spain'\n",
        "to_save_tme = '/content/ta_tme_'\n",
        "sheet_mm = 'TA_Input_Lease_MM_Spain'\n",
        "to_save_mm = '/content/ta_mm_'\n",
        "sheet_others = 'TA_Input_Lease_Others_Spain'\n",
        "to_save_others = '/content/ta_others_'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "ta_bill = ['Código', 'Importe anual','Inicio','F/cie/tec/']\n",
        "find_diffs_between_files(pathold, pathtw, 'Código', ta_bill,'', to_save_osp, 'excel','ta',sheet_osp)\n",
        "find_diffs_between_files(pathold, pathtw, 'Código', ta_bill, to_save_tme, 'excel',sheet_tme)\n",
        "find_diffs_between_files(pathold, pathtw, 'Código', ta_bill, to_save_mm, 'excel',sheet_mm)\n",
        "find_diffs_between_files(pathold, pathtw, 'Código', ta_bill, to_save_others, 'excel',sheet_others)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}