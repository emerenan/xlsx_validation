{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "es_validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMwAMh3rAxWbN9AAWQStkO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4FndupnMVhY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index):\n",
        "    \"\"\"\n",
        "    Params:\\n\n",
        "    path: parth of file in the computer.\\n\n",
        "    n_skiprows: Number of rows to delete in the original file,.\\n\n",
        "    columns_to_convert: Columns to convert the data general type. \\n\n",
        "    n_skipcolumn: Columns to skip in the original file. \\n\n",
        "    endrow = pass 0 to read everything, 1 to count entire\n",
        "    columns_order: List of columns names in specific order to pass in the engine.\\n\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    \"\"\"cont = 0\n",
        "    for i in df.iloc[:,site_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[:cont, :]\n",
        "    #df.columns = columns_order\"\"\"\n",
        "    \n",
        "    # convert intery columns to integer \n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].fillna(0)\n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].astype('int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def check_df(df, error_msg):\n",
        "    if df == None or df.empty:\n",
        "        return f'{error_msg}'\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def count_duplicates(lista):\n",
        "    count_dict = {}\n",
        "    for entry in lista:\n",
        "        if entry in count_dict.keys():\n",
        "            count_dict[entry] += 1\n",
        "        else:\n",
        "            count_dict[entry] = 1\n",
        "    \n",
        "    duplicates = {}\n",
        "    for k, v in count_dict.items():\n",
        "        if v > 1:\n",
        "            duplicates[k] = v\n",
        "    return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "def defining_df(df, column_range, number_col):\n",
        "    cont = 0\n",
        "    for i in df.iloc[:,number_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[0:cont, :]\n",
        "    return df\n",
        "\n",
        "#old version\n",
        "def check_columns(table, output_columns):\n",
        "    \"\"\"\n",
        "    Check the total of number of missing columns and the missing columns in passed table.\\n\n",
        "\n",
        "    Params:\\n\n",
        "    table: contain the columns to be check.\\n\n",
        "    output_columns: columns structure at the final file. \n",
        "\n",
        "    Returns:\\n\n",
        "    Number of missing columns and a list that contains the name os missing columns.\n",
        "    \"\"\"\n",
        "    \"\"\"\"\n",
        "    countries = ['DE':{'towerDB':[],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':[]}\\,\n",
        "                 'HU',\n",
        "                 'IE',\n",
        "                 'RO',\n",
        "                 'PT',\n",
        "                 'ES',\n",
        "                 'CZ': {'towerDB':[\"Code (Duplicate)\",\"Site Status\",\"VF - In scope / out of scope (Generalised scoping)\",\"Site in Skylon scope Actual (From Site List Sheet )\",\"Legacy Site Code(Duplicate)\",\"TIMS Site Code\",\"Legacy Site Code\",\"Site Name\",\"Macro Region\",\"Province\",\"Municipality\",\"Inhabitants\",\"Address\",\"Ground Register\",\"Altitude\",\"Latitude\",\"Longitude\",\"Categorization by Inhabitants\",\"Categorization by Transmission Sys\",\"Categorization by Site Type\",\"Categorization by Transmission Sys (subcluster)\",\"Other internal Categorization 1 (Identify ACQ Sites)\",\"Other internal Categorization 2 Energy provider (Eon/ LL)\",\"DAS+Macro\",\"DAS (Yes/ No)\",\"DAS Ownership (Complete/ Partial/ 3rd Party)\",\"Active/ Passive DAS\",\"# of remote units/ radiating points\",\"Type of Structure\",\"Distance highest antenna to ground level\",\"GBT Tower height\",\"POD ID\",\"Energy Consumption LTM (kwh)\",\"Annual Energy cost LTM (Euros)\",\"Infrastructure ready (existing)/ to be ready (new)\",\"Infrastructure to be dismantled by\",\"Radio equipments to be deactivated by\",\"Infrastructure to be shared by\",\"Technology VOD\",\"Fibre / Microwave\",\"Vertical passive structure owner\",\"Room configuration (detailed)\",\"Shelter passive structure ownership\",\"Type of Air Conditioning\",\"Number of cabinets (Full Capacity)\",\"Number of Antenna (Full Capacity)\",\"Number of MW (Full Capacity)\",\"Counterpart\",\"# of Lease Contracts\",\"Current annual lease fees \",\"Current other fees (Maintenance)\",\"Current other fees\",\"(Average) residual duration - Lease contract\",\"(Average) residual duration - Maintenance contract\",\"(Average) residual duration - Lease & Maintenance both\",\"# of Tenants Agreements\",\"Current Total Annual Hosting Fees\",\"Tenant (name/ID) MNO1 (Česká telekomunikační infrastruktura a.s.)\",\"Annual Fee per Tenant MNO1\",\"Annual Energy Fee MNO1\",\"Annual Maintenance Fee MNO1\",\"Other Services Fee MNO1\",\"Residual duration MNO1 (Years)\",\"Tenant (name/ID) MNO2 (T-Mobile Czech Republic a.s.)\",\"Annual Fee per Tenant MNO2\",\"Annual Energy Fee MNO2\",\"Annual Maintenance Fee MNO2\",\"Other Services Fee MNO2\",\"Residual duration MNO2 (days)\",\"Tenant (name/ID) MNO3\",\"Annual Fee per Tenant MNO3\",\"Annual Energy Fee MNO3\",\"Annual Maintenance Fee MNO3\",\"Other Services Fee MNO3\",\"Residual duration MNO3\",\"# of OTMOs\",\"Annual Fee from OTMOs\",\"Annual Energy Fee from OTMOs\",\"Annual Maintenance Fee OTMOs\",\"Other Services Fee OTMOs\",\"Average residual duration (days)\",\"Check\",\"Strategic Macro Sites\",\"Critical Sites\",\"Case A Core Site\",\"Macro Site - Transmission Hub Site\",\"Macro Site - Transmission Hub Site with/without Shelters\",\"Transmission Sites\",\"Room Configuration\",\"Power Supply\",\"Air Conditioning\",\"Active Sharing Arrangements involving the Operator\",\"VF-CZ Demerger phase\",\"EVO Location [FAR Site ID] \",\"Billing Trigger date \",\\\n",
        "                 \"Strategic_Site_Bucket\",\"Critical Site Bucket\",\"Subsequent_Sharing_Arrangement\",\"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Start_Date\",\"First_Active_Sharing_End_Date\",\"Decommissioned_Site\",\"Wip_Site\",\"Bts_Site\",\"Transfer_Date_Of_Consent_Required_Sites\",\"Sites_As_Metered_Estimated\",\"Date_Of_Equipment_Removal\"],\\\n",
        "                       'lc': [],\\\n",
        "                       'ta':['Tenant Agreement ID - NEW', 'Code', 'Site Name', 'Service Type','Contract start date', 'Contract end date', 'Status of contract','Renewal Option', 'Comments', 'Counterpart ID', 'Counterpart','Classification of Tenant', 'Annual amount - billing currency','Billing Currency', 'Annual Amount in CZK', 'Amount in EUR','Terms of Payment', 'Frequency', 'Indexed YES/NO', 'Index','Percentage', 'VAT Subject YES/NO', 'Percentage (VAT)', 'Unique Key','Classification', 'Residual Period in Years', 'Remarks','Count of unique key', 'Count of contracts', 'Scoping classification','DAS sites', 'DAS ownership', '31-Mar-21', 'Update in month','Updated Item', 'Comment', 'Counterpart_extra_1', 'Counterpart_extra_2', 'x','SiteType', '1.028', 'Unique Tenant', 'Unique Tenant Count', 'not_def_1']}\\,\n",
        "                 }\n",
        "                 'GR']\n",
        "                 \"\"\"\n",
        "    total_received = len(table.columns)\n",
        "    number_missing_columns = 0\n",
        "    missing_columns = []\n",
        "    #Counting of missing columns       \n",
        "    #if country in contries:      \n",
        "    for columns in output_columns:\n",
        "        if columns.lower() not in [labels.lower() for labels in table]:\n",
        "            number_missing_columns +=1\n",
        "            missing_columns.append(columns)\n",
        "    \n",
        "    return total_received, number_missing_columns, missing_columns\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    \"\"\"\n",
        "    for i in bill_cols:\n",
        "        if i not in twdb_col:\n",
        "            col_miss.append(i)\"\"\"\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", 0, '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "      \n",
        "def date_parser(df, columns, format, type_dates='normal'):\n",
        "    t_col = type_dates.lower()\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Refactorar esse codigo para receber todas as colunas num dic\n",
        "# Sendo as keys=columns e values= picklist for each column\n",
        "def check_date_columns(df, df_index, columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                if date_format.match(df_dates.loc[de_site,de_column]) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "                    else:\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            return df_de\n",
        "        else: \n",
        "            print('No one columns with incorrect date format!')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                if date_format.match(df_dates.loc[de_site,de_column]) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "                    else:\n",
        "                        df_de.loc[de_site,de_column] = 'Incorret Format or Blank Value'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            return df_de\n",
        "        else: \n",
        "            print('No one columns with incorrect date format!')\n",
        "\n",
        "def check_amounts(df_check, df_index, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "        \n",
        "def check_picklist(df,df_index,df_status, df_cols, picklist_dict):\n",
        "\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        columns = [i.lower() for i in picklist_dict.keys()]\n",
        "        for column in set(columns): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            #print(value)\n",
        "            if not value in set(picklist_dict[column]) or pd.isnull(value):\n",
        "                #print(set(picklist_dict[column]))\n",
        "\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "                else:\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "\n",
        "    #df = df_errors.dropna()   \n",
        "    df_errors = df_errors.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    if len(df_errors)>0:\n",
        "        df = df[[df_index, df_status]]\n",
        "        df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "        df_errors = df_errors.reset_index()\n",
        "    else:\n",
        "        print('\\nNo one Picklist Error Founded!\\n')\n",
        "    return df_errors\n",
        "\n",
        "def check_picklist_3(df,df_index, picklist_dict):\n",
        "    log = {}\n",
        "    for column in picklist_dict:\n",
        "        df_aux = df.copy()\n",
        "        new = df_aux[column].isin(picklist_dict[column])\n",
        "        #new = df_aux[column].apply(lambda x: x in picklist_dict[column])\n",
        "        # Aceita somento os valores que não estão na picklist\n",
        "        indexes = df.index[new == False].tolist()\n",
        "        #if len(indexes)>0:\n",
        "        if column not in log:log[column]=[]\n",
        "        log[column]=log[column]+indexes\n",
        "    #print(log.keys())\n",
        "\n",
        "    newDict ={}\n",
        "    df1 = pd.DataFrame()\n",
        "    for key,value in log.items():\n",
        "        for val in value:  \n",
        "            ID=df.iloc[val][df_index]\n",
        "            if ID in newDict:\n",
        "                newDict[ID].append(key)\n",
        "            else:\n",
        "                newDict[ID] = [key]\n",
        "        \n",
        "    logs = pd.DataFrame.from_dict(newDict, orient='index')\n",
        "    return logs\n",
        "\n",
        "#check on air foi trocado pelo check_picklist\n",
        "def check_on_air_sites(df, df_index, df_cols):\n",
        "\n",
        "    df_check = df[df_cols]\n",
        "    df_check['sites'] = df_check[df_index]\n",
        "    df_check = df_check.set_index('sites')\n",
        "    #df_check = replace_values(df_check, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in df_cols: \n",
        "            if df_check.loc[site,df_index] not in df_errors.index:\n",
        "                df_errors.loc[site,df_index] = df_check.loc[site,df_index]\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "            else:\n",
        "                if df_check.loc[site,column] == 0:\n",
        "                    df_errors.loc[site,column] = 'Incorret picklist value or Blank Value'\n",
        "    df = df_errors[df_errors.iloc[:,1:]]\n",
        "    df = df.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    #df = df_errors.dropna(how='all', axis=1)   \n",
        "    return df\n",
        "\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [str(i) for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uip = [str(i) for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index].sort_values() if i not in uip_list]\n",
        "    bts_out_uip = pd.DataFrame(bts_out_uip, columns=['Bts_Sites_Out_UIP_File'])\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "\n",
        "    return new_sites, bts_out_uip, df_site_bts[[tw_index, bts_col, bill_col]]\n",
        "    \"\"\"Restruturar o script em CZ, DE, PT\"\"\"\n",
        "\n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    return df\n",
        "\n",
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[wip_tw]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, wip_tw, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "    \"\"\"Reestrurar os script em PT, DE, CZ\"\"\"\n",
        "    # Falta os outros países\n",
        "\n",
        "def check_decommissioned(df,df_index, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    filtered = df[(df[decom_col]=='Yes')&(df[doer_col]==\"\")]\n",
        "    return filtered[[df_index, decom_col, doer_col]]\n",
        "    \"\"\"Ajustar para CZ, DE, PT\"\"\"\n",
        "    \n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[date_col].empty:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]') < current_date)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "        else:\n",
        "            filtered = df_tw[(df_tw[status_col]==status)&(df_tw[date_col].astype('datetime64[ns]').empty)]\n",
        "            return filtered[[tw_index, status_col, date_col]] \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "    \"\"\"Ajustar para CZ, DE, PT, IE\"\"\"\n",
        "\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col, status_col):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)]\n",
        "        return filtered[[tw_index, status_col, doer_col]]  \n",
        "    else:\n",
        "        print('Nothing wrong in services sites!')\n",
        "                                                              \n",
        "def check_mom_bts(df_tw, tw_index, tw_col, df_msa,msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[tw_col]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    return filtered[[tw_index, tw_col]]         \n",
        "\n",
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "    df[start_date] = pd.to_datetime(df[start_date])\n",
        "    df[end_date] = pd.to_datetime(df[end_date], errors='coerce')\n",
        "    filtered = df.loc[pd.to_datetime(df[start_date]) > df[end_date], [tw_index, start_date,end_date]]\n",
        "    print(filtered)\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "    t1 = ['pt', 'de', 'cz', 'ie', 'es', 'ro', 'hu']\n",
        "    if country.lower() in t1:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        if decom_col != \"\":\n",
        "            tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "            decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "\n",
        "        \n",
        "            return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "        else:\n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIP File'])\n",
        "            return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIP File!'])\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "        decomiss_sites_in_uip = []\n",
        "        if set(tw_decomiss).intersection(uip_sites):\n",
        "            decomiss_sites_in_uip = [i for i in tw_decomiss if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(decomiss_sites_in_uip, columns=['Decomissioned Site in UIP File'])\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIP File'])\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip = [i for i in df_uip['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Yes'][tw_index]\n",
        "        critical = []\n",
        "        if set(uip).intersection(bts_tw_critical):\n",
        "            critical = [i for i in bts_tw_critical if i not in uip]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% out in UIP File'])\n",
        "\n",
        "        return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names)\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names )\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols)\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    return df_commercial_diffs\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() \n",
        "\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, old_file, new_file, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = fit_df(path_OLD,bill_cols)\n",
        "        df_NEW = fit_df(path_NEW,bill_cols)\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_file}) vs ({new_file}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "187ZOjvNX27i"
      },
      "source": [
        "Looking for differences betwwnn older and newest files in towerdb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZmQg7JrX2S7"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', \n",
        "                             status_col='', kind='tw', dates=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col):\n",
        "        #print(df.head(1))\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "        t_col = type_dates.lower()\n",
        "        if format == 1:\n",
        "            type_date = \"%d/%m/%Y\"\n",
        "        else:\n",
        "            type_date = \"%d-%m-%Y\"\n",
        "        for column in lower_str(columns):\n",
        "            if t_col == 'mixed':\n",
        "                df[column] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) \\\n",
        "                            and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "            else:\n",
        "                df[column] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new).fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,encoding='latin1').fillna('')\n",
        "        cols_old = lower_str(list(df_OLD.columns))\n",
        "        df_OLD.columns = cols_old\n",
        "        df_OLD = fit_df(df_OLD, index_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        for i in dates:\n",
        "            df_NEW[i] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) \\\n",
        "                         and not isinstance(date_obj, str) else date_obj for date_obj in df_NEW[i]]\n",
        "            df_NEW[i] = df_NEW[i].astype(str)\n",
        "            \n",
        "        df_NEW = df_NEW.dropna(subset=[index_col], axis=0)\n",
        "        df_NEW[index_col] = df_NEW[index_col].astype(str)\n",
        "        df_NEW['sites'] = df_NEW[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df_NEW = df_NEW.set_index('sites').fillna('')\n",
        "        #df_NEW = df_NEW[cols_old]\n",
        "\n",
        "        #df_NEW = fit_df(df_NEW, index_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "    \n",
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "pathold = '/content/TowerDB_Spain_20210731.csv'\n",
        "to_save = '/content/TW_ES'\n",
        "sheet= 'Enhanced TowerDB'\n",
        "skipr = 7\n",
        "skipc = 3\n",
        "old_n = 'TWDB_20210731.csv'\n",
        "new_n = 'TWDB_20210809.xlsx'\n",
        "bill_cols = ['Code',\\\n",
        "             'Inhabitants',\\\n",
        "             'Categorization by Transmission Sys',\\\n",
        "             'Categorization by Site Type',\\\n",
        "             'Vodafone equipment giving Active Sharing to Orange',\\\n",
        "             'Bundled_Sites_Yes_No',\\\n",
        "\t\t\t 'Wip_Site',\\\n",
        "\t\t\t 'Region_For_Tax_Calculation',\\\n",
        "\t\t\t 'Indoor_Outdoor_Categorization',\\\n",
        "\t\t\t 'Bts_Site',\\\n",
        "\t\t\t 'Sites_As_Metered_Estimated',\\\n",
        "\t\t\t 'Indoor_Site_Any_Climate_Control',\\\n",
        "\t\t\t 'Outdoor_Site_With__Power',\\\n",
        "\t\t\t 'Bundled_Site_Categorization',\\\n",
        "\t\t\t 'Strategic_Site',\\\n",
        "\t\t\t 'Strategic_Site_Bucket',\\\n",
        "\t\t\t 'Critical_Site',\\\n",
        "\t\t\t 'CriticalSite_Beyond_10',\\\n",
        "\t\t\t 'Active_Sharing_Arrangement',\\\n",
        "\t\t\t 'Orange_Crossed_Site',\\\n",
        "\t\t\t 'Das_Classification',\\\n",
        "\t\t\t 'Macro_Core_Site_Yes_No',\\\n",
        "\t\t\t 'Macro_Transmission_Hub_Yes_No',\\\n",
        "\t\t\t 'Macro_Transmission_Hub_With_Shelters_Without_Shelters',\\\n",
        "\t\t\t 'Transmission_With_Shelters_Without_Shelters',\\\n",
        "\t\t\t 'Subsequent_Sharing_Arrangement',\\\n",
        "\t\t\t 'First_Active_Sharing_Deployment_Type',\\\n",
        "             'First_Active_Sharing_Start_Date',\\\n",
        "             'First_Active_Sharing_End_Date',\\\n",
        "\t\t\t 'Decommissioned_Site only for VF',\\\n",
        "\t\t\t 'Sites_Fall_Under_2400',\\\n",
        "\t\t\t 'Macro_CoreA_CoreB',\\\n",
        "\t\t\t 'Billing Trigger date',\n",
        "             ]\n",
        "\n",
        "dates_tw = ['First_Active_Sharing_Start_Date','First_Active_Sharing_End_Date']\n",
        "\"\"\"(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', \n",
        "                             status_col='', kind='tw', dates=[], sheetname='', skipr=0, skipc=0)\"\"\"\n",
        "find_diffs_between_files(pathold, pathtw, 'Code', bill_cols,to_save, old_n, new_n, type_file='mix',status_col='On air / Active', \n",
        "                         kind='tw',dates=dates_tw, sheetname=sheet, skipr=7, skipc=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMNqI3p4S8M1"
      },
      "source": [
        "Adding new columns for UIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lulong55S_QC"
      },
      "source": [
        "# Lendo o Ficheiro de input\n",
        "path_uip=''\n",
        "sheet = 'SiteLevel'\n",
        "site_level = pd.read_excel(path_uip,sheet, header=[0,1,2])\n",
        "\n",
        "head_1 = pd.MultiIndex.from_product([[''],['Numeric (in months)'],['Delay in Site Modification Projects', 'Delay in BTS Projects']])\n",
        "df_1 = pd.DataFrame(columns=head_1)\n",
        "\n",
        "head_2 = pd.MultiIndex.from_product([[''],['Numeric (in €)'],['Excess of Upgrade Capital Expenditure over Threshold']])\n",
        "df_2 = pd.DataFrame(columns=head_2)\n",
        "\n",
        "site_level = pd.concat([site_level, df_1, df_2], axis=1)\n",
        "site_level.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFXAXm2TPPu2"
      },
      "source": [
        "Doing all checks in Towerdb File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mJZs4Ek2HyU"
      },
      "source": [
        "pathmsa = '/content/TowerDB_Spain_20210731.csv'\n",
        "msa = pd.read_csv(pathmsa, encoding='latin')\n",
        "msa.columns.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe3gnYrg1iFF"
      },
      "source": [
        "msa.columns.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV5q5ZwHQAgA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "4b25e32d-b3d6-4228-df71-8c156dd19212"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "sheet= 'Enhanced TowerDB'\n",
        "skipr = 7\n",
        "skipc = 3\n",
        "towerdb = read_files(pathtw, sheet, skipr, skipc, 'Code')\n",
        "towerdb = towerdb.rename(columns={'Categorization by Transmission Sys.1': 'Categorization by Transmission Sys_1'})\n",
        "#towerdb.columns = lower_str(list(towerdb.columns))\n",
        "\n",
        "\"\"\"Defining variables which is gonna be reusable in checks\"\"\"\n",
        "tw_index = 'Code'\n",
        "tw_doer = 'Date of equipment removal (from MAR´21)'\n",
        "tw_status = 'on air / active'\n",
        "tw_bts = 'Bts_Site'\n",
        "tw_bill = 'Billing Trigger date'\n",
        "tw_wip = 'Wip_Site'\n",
        "tw_decom = 'Decommissioned_Site only for VF'\n",
        "#w_amount = 'Lease Contract - Current annual lease fee'\n",
        "tw_critical = 'CriticalSite_Beyond_10'\n",
        "\n",
        "pathmsa = '/content/TowerDB_Spain_20210731.csv'\n",
        "msa = pd.read_csv(pathmsa, encoding='latin')\n",
        "#msa.columns = lower_str(list(msa.columns))\n",
        "msa_index = 'Code'\n",
        "msa_bts = 'Bts_Site'\n",
        "msa_doer = 'Date of equipment removal (from MAR´21)'\n",
        "msa_status = 'on air / active'\n",
        "msa_bill = 'Billing Trigger date'\n",
        "msa_wip = 'Wip_Site'\n",
        "msa_decom = 'Decommissioned_Site only for VF'\n",
        "#msa_amount = 'Lease Contract - Current annual lease fee'\n",
        "msa_critical = 'CriticalSite_Beyond_10'\n",
        "\n",
        "col_order = list(msa.columns)\n",
        "\n",
        "towerdb = towerdb[col_order]\n",
        "#towerdb = towerdb.fillna('')\n",
        "\n",
        "ints = ['Inhabitants', 'Altitude', '# of Lease Contracts', 'ORANGE', 'Telefonica', 'YOIGO']\n",
        "for i in ints:\n",
        "    towerdb[i] = towerdb[i].fillna(0)\n",
        "    towerdb[i] = list(map(int, towerdb[i]))\n",
        "\n",
        "dates_tw = ['First_Active_Sharing_Start_Date', 'First_Active_Sharing_End_Date', 'Billing Trigger date']\n",
        "for i in dates_tw:\n",
        "    towerdb[i] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in towerdb[i]]\n",
        "    \n",
        "towerdb['Date of equipment removal (from MAR´21)'] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) and not isinstance(date_obj, str) else '' for date_obj in towerdb['Date of equipment removal (from MAR´21)']]\n",
        "\n",
        "sites = [i for i in towerdb[towerdb['Sites_As_Metered_Estimated']=='Estimated model'][tw_index]]\n",
        "towerdb.loc[towerdb[tw_index].isin(sites), 'Sites_As_Metered_Estimated'] = 'Estimated Model'\n",
        "\n",
        "towerdb = towerdb.fillna('')\n",
        "towerdb.head(3)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (112) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>Site Name</th>\n",
              "      <th>Macro Region</th>\n",
              "      <th>Region</th>\n",
              "      <th>Province</th>\n",
              "      <th>Municipality</th>\n",
              "      <th>Inhabitants</th>\n",
              "      <th>Address</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>Altitude</th>\n",
              "      <th>Categorization by Transmission Sys</th>\n",
              "      <th>Categorization by Transmission Sys_1</th>\n",
              "      <th>Categorization by Transmission Sys (sub-cluster)</th>\n",
              "      <th>MSA Type updated</th>\n",
              "      <th>Categorization by Site Type</th>\n",
              "      <th>Categorization by inhabitants</th>\n",
              "      <th>Rural/ Suburban/ Urban</th>\n",
              "      <th>Categorization by CONNECTIVITY as per MSA Site List 31/03</th>\n",
              "      <th>POD ID</th>\n",
              "      <th>Energy Consumption feb'20-jan´21</th>\n",
              "      <th>Actual Energy cost feb'20-jan´21</th>\n",
              "      <th>Infrastructure ready (existing)/ to be ready (new)</th>\n",
              "      <th>Infrastructure to be shared by</th>\n",
              "      <th>Counterpart</th>\n",
              "      <th># of Lease Contracts</th>\n",
              "      <th>Current annual lease fees</th>\n",
              "      <th>Current energy lease fees</th>\n",
              "      <th>Current annual other fees</th>\n",
              "      <th>Total Annual Lease</th>\n",
              "      <th>(Average) residual duration</th>\n",
              "      <th>Maturity Cluster</th>\n",
              "      <th>ExCo rep. Avg Annual Lease costs</th>\n",
              "      <th>Total Energy Cost (Energy provider + LL)</th>\n",
              "      <th>VOD (y/n)</th>\n",
              "      <th>ORANGE</th>\n",
              "      <th>Annual Fee per Tenant MNO1</th>\n",
              "      <th>Annual Energy Fee MNO1</th>\n",
              "      <th>Annual Maintenance Fee MNO1</th>\n",
              "      <th>Other Services Fee MNO1</th>\n",
              "      <th>...</th>\n",
              "      <th>Core sites</th>\n",
              "      <th>Vodafone Office /Vodafone Shop sites</th>\n",
              "      <th>Freehold sites</th>\n",
              "      <th>X</th>\n",
              "      <th>VF Only Rural/Urban</th>\n",
              "      <th>VF Only GBT/RTT</th>\n",
              "      <th>VF Only Zone</th>\n",
              "      <th>on air / active</th>\n",
              "      <th>Estimated + Real Energy Consumption nov'19-oct´20</th>\n",
              "      <th>Estimated + Real Actual Energy cost nov'19-oct'20</th>\n",
              "      <th>Vodafone equipment giving Active Sharing to Orange</th>\n",
              "      <th>Bts_Site</th>\n",
              "      <th>Sites_As_Metered_Estimated</th>\n",
              "      <th>Indoor_Site_Any_Climate_Control</th>\n",
              "      <th>Outdoor_Site_With__Power</th>\n",
              "      <th>Bundled_Sites_Yes_No</th>\n",
              "      <th>Bundled_Site_Categorization</th>\n",
              "      <th>Strategic_Site</th>\n",
              "      <th>Strategic_Site_Bucket</th>\n",
              "      <th>Critical_Site</th>\n",
              "      <th>CriticalSite_Beyond_10</th>\n",
              "      <th>Wip_Site</th>\n",
              "      <th>Active_Sharing_Arrangement</th>\n",
              "      <th>Subsequent_Sharing_Arrangement</th>\n",
              "      <th>First_Active_Sharing_Deployment_Type</th>\n",
              "      <th>First_Active_Sharing_Start_Date</th>\n",
              "      <th>First_Active_Sharing_End_Date</th>\n",
              "      <th>Billing Trigger date</th>\n",
              "      <th>Decommissioned_Site only for VF</th>\n",
              "      <th>Sites_Fall_Under_2400</th>\n",
              "      <th>Region_For_Tax_Calculation</th>\n",
              "      <th>Orange_Crossed_Site</th>\n",
              "      <th>Indoor_Outdoor_Categorization</th>\n",
              "      <th>Das_Classification</th>\n",
              "      <th>Macro_Core_Site_Yes_No</th>\n",
              "      <th>Macro_CoreA_CoreB</th>\n",
              "      <th>Macro_Transmission_Hub_Yes_No</th>\n",
              "      <th>Macro_Transmission_Hub_With_Shelters_Without_Shelters</th>\n",
              "      <th>Transmission_With_Shelters_Without_Shelters</th>\n",
              "      <th>Date of equipment removal (from MAR´21)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>M_NUDONORTE</td>\n",
              "      <td>Zona 1</td>\n",
              "      <td>Zona Centro</td>\n",
              "      <td>Madrid</td>\n",
              "      <td>MADRID</td>\n",
              "      <td>3334730</td>\n",
              "      <td>C/Colindres, s/n (Canal de Isabel II)</td>\n",
              "      <td>40.4847</td>\n",
              "      <td>-3.68206</td>\n",
              "      <td>737</td>\n",
              "      <td>Macro</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Macro Site</td>\n",
              "      <td>GBT</td>\n",
              "      <td>7. Mayor 1.000.000</td>\n",
              "      <td>URBAN</td>\n",
              "      <td>Fibre</td>\n",
              "      <td>ES0021000004884226TN</td>\n",
              "      <td>38593.5</td>\n",
              "      <td>4071.74</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Canal de Isabel II Gestión, S.A.</td>\n",
              "      <td>1</td>\n",
              "      <td>7657.56</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7657.56</td>\n",
              "      <td>1.25205</td>\n",
              "      <td>&lt;= 5 years</td>\n",
              "      <td>7657.56</td>\n",
              "      <td>4071.74</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>FALSO</td>\n",
              "      <td></td>\n",
              "      <td>1URBAN</td>\n",
              "      <td>1GBT</td>\n",
              "      <td>1Madrid</td>\n",
              "      <td>In Service</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Estimated model</td>\n",
              "      <td>Yes; Indoor Air Conditioning</td>\n",
              "      <td>DC</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>Yes</td>\n",
              "      <td>Within 10%</td>\n",
              "      <td>No</td>\n",
              "      <td>No Active Sharing</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>None</td>\n",
              "      <td>No</td>\n",
              "      <td>Indoor</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>Non Core</td>\n",
              "      <td>No</td>\n",
              "      <td>Non Transmission Hub Site</td>\n",
              "      <td>Non Transmission Site</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>M_ATALAYAM30</td>\n",
              "      <td>Zona 1</td>\n",
              "      <td>Zona Centro</td>\n",
              "      <td>Madrid</td>\n",
              "      <td>MADRID</td>\n",
              "      <td>3334730</td>\n",
              "      <td>Avda San Luis 77</td>\n",
              "      <td>40.4728</td>\n",
              "      <td>-3.66317</td>\n",
              "      <td>722</td>\n",
              "      <td>Macro</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Macro Site (Transmission Hub Site)</td>\n",
              "      <td>RTT</td>\n",
              "      <td>7. Mayor 1.000.000</td>\n",
              "      <td>URBAN</td>\n",
              "      <td>Fibre</td>\n",
              "      <td>ES0022000007592111AM</td>\n",
              "      <td>50635</td>\n",
              "      <td>5984.89</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Lyntia Networks S.A.U.</td>\n",
              "      <td>13</td>\n",
              "      <td>37262.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37262.4</td>\n",
              "      <td>1.898</td>\n",
              "      <td>&lt;= 5 years</td>\n",
              "      <td>37262.4</td>\n",
              "      <td>5984.89</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>FALSO</td>\n",
              "      <td></td>\n",
              "      <td>1URBAN</td>\n",
              "      <td>1RTT</td>\n",
              "      <td>1Madrid</td>\n",
              "      <td>In Service</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Estimated model</td>\n",
              "      <td>Yes; Indoor Air Conditioning</td>\n",
              "      <td>DC</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>Yes</td>\n",
              "      <td>Within 10%</td>\n",
              "      <td>No</td>\n",
              "      <td>No Active Sharing</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>None</td>\n",
              "      <td>No</td>\n",
              "      <td>Indoor</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>Non Core</td>\n",
              "      <td>Yes</td>\n",
              "      <td>With shelters</td>\n",
              "      <td>Non Transmission Site</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>M_PALOMASNII</td>\n",
              "      <td>Zona 1</td>\n",
              "      <td>Zona Centro</td>\n",
              "      <td>Madrid</td>\n",
              "      <td>MADRID</td>\n",
              "      <td>3334730</td>\n",
              "      <td>C/ Peonias, 2</td>\n",
              "      <td>40.4505</td>\n",
              "      <td>-3.61605</td>\n",
              "      <td>645</td>\n",
              "      <td>Macro</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Macro Site</td>\n",
              "      <td>RTT</td>\n",
              "      <td>7. Mayor 1.000.000</td>\n",
              "      <td>URBAN</td>\n",
              "      <td>Microwave</td>\n",
              "      <td>ES0022000007592119AJ</td>\n",
              "      <td>64776.8</td>\n",
              "      <td>7522.33</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>CONSTRUC HISPANO ARGENTINA S.A.</td>\n",
              "      <td>1</td>\n",
              "      <td>32000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32000</td>\n",
              "      <td>0.432877</td>\n",
              "      <td>&lt;= 5 years</td>\n",
              "      <td>32000</td>\n",
              "      <td>7522.33</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>FALSO</td>\n",
              "      <td></td>\n",
              "      <td>0URBAN</td>\n",
              "      <td>0RTT</td>\n",
              "      <td>0Madrid</td>\n",
              "      <td>In Service</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Estimated model</td>\n",
              "      <td>Yes; Indoor Air Conditioning</td>\n",
              "      <td>DC</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>Non Critical</td>\n",
              "      <td>No</td>\n",
              "      <td>No Active Sharing</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>None</td>\n",
              "      <td>No</td>\n",
              "      <td>Indoor</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>Non Core</td>\n",
              "      <td>No</td>\n",
              "      <td>Non Transmission Hub Site</td>\n",
              "      <td>Non Transmission Site</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 125 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Code  ... Date of equipment removal (from MAR´21)\n",
              "0     1  ...                                        \n",
              "1     2  ...                                        \n",
              "2     3  ...                                        \n",
              "\n",
              "[3 rows x 125 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_CmMZIVVWBB"
      },
      "source": [
        "Check columns received looking for missing columns that is gonna be used in rating engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wblfWhtLTeQX"
      },
      "source": [
        "\"\"\"Check Columns Received\"\"\"\n",
        "df_cols = check_columns_received(towerdb, col_order)\n",
        "df_cols\n",
        "#No columns missing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAGJs483VchY"
      },
      "source": [
        "First Check - Dates Formats (dd/mm/YYYY) \n",
        "\n",
        "Columns: Date of equipment removal (from MAR´21)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEZFbjGvVzZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d5ed02-53cf-4cb2-831d-54d0c0c39b73"
      },
      "source": [
        "\"\"\"You need to convert all values in cols for string format to check\"\"\"\n",
        "def check_date_columns(df, df_index,status_col,columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            #df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "\n",
        "def date_parser(df, columns, format, type_dates):\n",
        "    t_col = type_dates.lower()\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Columns to functions\n",
        "dates_bill = [tw_index, tw_bill]\n",
        "dates_doer = [tw_index, tw_doer]\n",
        "#Columns to parser\n",
        "bill=[tw_bill]\n",
        "doer=[tw_doer]\n",
        "\n",
        "date_parser(towerdb, bill, \"%d/%m/%Y\", 'no')  #não precisar checkar o bill dos sites in service\n",
        "date_parser(towerdb, doer, \"%d/%m/%Y\", 'mixed')\n",
        "\n",
        "actives_1 = towerdb[towerdb['on air / active']=='In Service']\n",
        "no_actives_1 = towerdb[~(towerdb[tw_status]=='In Service')]\n",
        "\n",
        "#Checking columns for errors\n",
        "actives_dates_errors = check_date_columns(actives_1, tw_index, tw_status, dates_bill, 2) \n",
        "# Actives sites with blank billing trigger date\n",
        "no_actives_dates_errors = check_date_columns(no_actives_1, tw_index,tw_status, dates_doer, 2) \n",
        "# No Actives sites with blank Date of Equipament Removal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:167: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:167: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "MI1wLJ8g0y54",
        "outputId": "61cee5c9-8146-4995-d320-bb0ba05005d4"
      },
      "source": [
        "#Checking columns for errors\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>Billing Trigger date</th>\n",
              "      <th>on air / active</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9089</th>\n",
              "      <td>184368</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9090</th>\n",
              "      <td>181945</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9091</th>\n",
              "      <td>1806</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9092</th>\n",
              "      <td>129431</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9093</th>\n",
              "      <td>24224</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>In Service</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9094 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Code Billing Trigger date on air / active\n",
              "0          1          Blank Value      In Service\n",
              "1          2          Blank Value      In Service\n",
              "2          3          Blank Value      In Service\n",
              "3          4          Blank Value      In Service\n",
              "4          5          Blank Value      In Service\n",
              "...      ...                  ...             ...\n",
              "9089  184368          Blank Value      In Service\n",
              "9090  181945          Blank Value      In Service\n",
              "9091    1806          Blank Value      In Service\n",
              "9092  129431          Blank Value      In Service\n",
              "9093   24224          Blank Value      In Service\n",
              "\n",
              "[9094 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "UAiUcn0K5xSe",
        "outputId": "68c3df3f-a3e0-420a-9a2a-0ace14d43a2f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>Date of equipment removal (from MAR´21)</th>\n",
              "      <th>on air / active</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>53060</td>\n",
              "      <td>Incorret picklist value: Not removed</td>\n",
              "      <td>Decommissioned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>74808</td>\n",
              "      <td>Incorret picklist value: Not removed</td>\n",
              "      <td>Decommissioned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>173929</td>\n",
              "      <td>Incorret picklist value: Not removed</td>\n",
              "      <td>Decommissioned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>181769</td>\n",
              "      <td>Incorret picklist value: Not removed</td>\n",
              "      <td>WIP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>199722</td>\n",
              "      <td>Incorret picklist value: Not removed</td>\n",
              "      <td>WIP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>13848</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>16132</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>25746</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>69521</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>145463</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>164 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Code Date of equipment removal (from MAR´21) on air / active\n",
              "0     53060    Incorret picklist value: Not removed  Decommissioned\n",
              "1     74808    Incorret picklist value: Not removed  Decommissioned\n",
              "2    173929    Incorret picklist value: Not removed  Decommissioned\n",
              "3    181769    Incorret picklist value: Not removed             WIP\n",
              "4    199722    Incorret picklist value: Not removed             WIP\n",
              "..      ...                                     ...             ...\n",
              "159   13848                             Blank Value      Dismantled\n",
              "160   16132                             Blank Value      Dismantled\n",
              "161   25746                             Blank Value      Dismantled\n",
              "162   69521                             Blank Value      Dismantled\n",
              "163  145463                             Blank Value      Dismantled\n",
              "\n",
              "[164 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2LrHKSGjj9t"
      },
      "source": [
        "Thirth - Check Picklist values All sites\n",
        "\n",
        "Do this check in all sites\n",
        "\n",
        "Check the picklist for each case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdgNaUiU6EdE"
      },
      "source": [
        "def check_picklist(df,df_index,df_status, df_cols, picklist_dict):\n",
        "\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        columns = [i for i in picklist_dict.keys()]\n",
        "        for column in set(columns): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            #print(value)\n",
        "            if not value in set(picklist_dict[column]) or pd.isnull(value):\n",
        "                #print(set(picklist_dict[column]))\n",
        "\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "                else:\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "\n",
        "    #df = df_errors.dropna()   \n",
        "    df_errors = df_errors.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    if len(df_errors)>0:\n",
        "        df = df[[df_index, df_status]]\n",
        "        df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "        df_errors = df_errors.reset_index()\n",
        "        df_errors = df_errors[df_errors[df_status]!='Dismantled']\n",
        "    else:\n",
        "        print('\\nNo one Picklist Error Founded!\\n')\n",
        "    return df_errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoS6fLsXjykD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "c01482f9-4a90-4aa7-abc8-62a968e478f9"
      },
      "source": [
        "picklist_tw_general = {\n",
        "    'Categorization by Transmission Sys' : ['Macro', 'Public DAS']\n",
        "}\n",
        "pick_col_general = ['Code', 'Categorization by Transmission Sys']\n",
        "\n",
        "df_general_pick = check_picklist(towerdb, tw_index, tw_status, pick_col_general, picklist_tw_general)\n",
        "df_general_pick\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>on air / active</th>\n",
              "      <th>Categorization by Transmission Sys</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Code, on air / active, Categorization by Transmission Sys]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v4oVgDimF4S"
      },
      "source": [
        "Fourth Check - Remove \"N/A\", \"0\" or \"-\" values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWlMEo9LLbDn"
      },
      "source": [
        "towerdb = replace_values(towerdb, col_order, value=\"\")\n",
        "actives = replace_values(actives, col_order, value=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uoBfMK4mZt_"
      },
      "source": [
        "Fifth Check MoM Sites (BTS, decomissoned...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J7-o7qSmZUE"
      },
      "source": [
        "\"\"\"Falta Coluna de Flag Indicating BTS Site no twerdb recebido\"\"\"\n",
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "df_mom_bts = check_mom_bts(actives, tw_index, tw_bts, msa, msa_index, msa_bts)\n",
        "# No one error df_mom_bts\n",
        "\n",
        "decomiss = towerdb[towerdb[tw_status]=='Decommissioned']\n",
        "df_mom_decom = check_mom_bts(decomiss, tw_index, tw_decom, msa, msa_index, msa_decom)\n",
        "# No one error df_mom_decom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kyAO221L8Uu"
      },
      "source": [
        "Check Picklist and dates formats for In service sites\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOzkwTx1MGa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "7cb018b9-80cc-4e51-e786-00634e972a40"
      },
      "source": [
        "picklis_dict = {\n",
        "    'Categorization by Transmission Sys': ['Macro', 'Public DAS'],\n",
        "    'Categorization by Site Type': ['DAS passive','GBT','RTT'],\n",
        "    'Sites_As_Metered_Estimated': ['Estimated Model','Metered Model'],\n",
        "    'Infrastructure ready (existing)/ to be ready (new)': [''],\n",
        "    'Indoor_Site_Any_Climate_Control': ['No','Yes; Indoor Air Conditioning','Yes; Indoor Air Conditioning and Free Air cooling / Free cooling units'],\n",
        "    'Bts_Site': ['Yes', 'No'],\n",
        "    'Strategic_Site': ['Yes', 'No'],\n",
        "    'Strategic_Site_Bucket': ['Yes - first 460', ''],\n",
        "    'Critical_Site': ['Yes', 'No'],\n",
        "    'CriticalSite_Beyond_10': ['Within 10%','Non Critical'],\n",
        "    'Wip_Site': ['Yes', 'No'],\n",
        "    'Decommissioned_Site only for VF': ['Yes', 'No'],\n",
        "    'First_Active_Sharing_Deployment_Type': ['']\n",
        "}\n",
        "\n",
        "pick_cols = ['Code','Categorization by Transmission Sys','Categorization by Site Type','Sites_As_Metered_Estimated',\\\n",
        "             'Infrastructure ready (existing)/ to be ready (new)','Indoor_Site_Any_Climate_Control','Bts_Site',\\\n",
        "             'Strategic_Site','Strategic_Site_Bucket','Critical_Site','CriticalSite_Beyond_10','Wip_Site',\\\n",
        "             'Decommissioned_Site only for VF','First_Active_Sharing_Deployment_Type' ]\n",
        "actives = actives.fillna('')\n",
        "df_in_service_picklist = check_picklist(actives, tw_index, tw_status, pick_cols, picklis_dict)\n",
        "\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>on air / active</th>\n",
              "      <th>Sites_As_Metered_Estimated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: Estimated model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: Estimated model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: Estimated model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: Estimated model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: Estimated model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9092</th>\n",
              "      <td>129431</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: Estimated model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9093</th>\n",
              "      <td>24224</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: Estimated model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9094</th>\n",
              "      <td>176058</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: Estimated model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9095</th>\n",
              "      <td>190594</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: Estimated model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9096</th>\n",
              "      <td>194425</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Incorret picklist value: Estimated model</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9097 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Code on air / active                Sites_As_Metered_Estimated\n",
              "0          1      In Service  Incorret picklist value: Estimated model\n",
              "1          2      In Service  Incorret picklist value: Estimated model\n",
              "2          3      In Service  Incorret picklist value: Estimated model\n",
              "3          4      In Service  Incorret picklist value: Estimated model\n",
              "4          5      In Service  Incorret picklist value: Estimated model\n",
              "...      ...             ...                                       ...\n",
              "9092  129431      In Service  Incorret picklist value: Estimated model\n",
              "9093   24224      In Service  Incorret picklist value: Estimated model\n",
              "9094  176058      In Service  Incorret picklist value: Estimated model\n",
              "9095  190594      In Service  Incorret picklist value: Estimated model\n",
              "9096  194425      In Service  Incorret picklist value: Estimated model\n",
              "\n",
              "[9097 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKbGHTTU-m7q"
      },
      "source": [
        "sites = [i for i in towerdb[towerdb['Sites_As_Metered_Estimated']=='Estimated model'][tw_index]]\n",
        "towerdb.loc[towerdb[tw_index].isin(sites), 'Sites_As_Metered_Estimated'] = 'Estimated Model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQj9BsQKSHfZ"
      },
      "source": [
        "#check dates in columns\n",
        "def check_date_columns(df, df_index,status_col,columns, format=2):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                print(df_dates.loc[de_site,de_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            #df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "start_dates = ['First_Active_Sharing_Start_Date']\n",
        "\n",
        "date_parser(actives, start_dates, \"%d/%m/%Y\", 'mixed')\n",
        "actives['First_Active_Sharing_End_Date'] = actives['First_Active_Sharing_End_Date'].fillna('')\n",
        "in_service_cols = ['Code', 'First_Active_Sharing_Start_Date', 'First_Active_Sharing_End_Date']\n",
        "df_in_service_dates = check_date_columns(actives, tw_index, tw_status, in_service_cols)\n",
        "# tem errors em df_in_service_dates sites ativos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF9HkIPBZ3va"
      },
      "source": [
        "Fifth Check BTS Flagged(Billing Trigger and Commercial)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsLeIodqVW4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "outputId": "2660901d-ced3-4ead-f6cc-b2a72fa4a01d"
      },
      "source": [
        "status = 'Yes'\n",
        "df_bts_flagged = check_tw_bill_doer(towerdb, tw_index,tw_bill, tw_bts, status, 'bill')\n",
        "df_bts_flagged\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>Bts_Site</th>\n",
              "      <th>Billing Trigger date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Code, Bts_Site, Billing Trigger date]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePYDr-9PcqOg"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "path_uip = '/content/UserInput_Spain_202107 v31.xlsx'\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites(Annual)']\n",
        "uip = pd.read_excel(path_uip ,sheet_name='SiteLevel',usecols=[0,1,2],skiprows=2).fillna('')\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [i for i in msa[msa_index]]\n",
        "tw_sites = [i for i in towerdb[tw_index]]\n",
        "uip_sites = [i for i in uip['Site_ID']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eow-oO-Ndhxl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f8f865d9-806a-4a19-b643-9bba573ddf63"
      },
      "source": [
        "\"\"\" BTS sites\"\"\"\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [i for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "    #if not (new_sites.empty or bts_out_uis.empty or df_site_bts.empty):\n",
        "    return new_sites, df_site_bts[[tw_index,status_col, bts_col, bill_col]]\n",
        "\n",
        "new_sites, df_bts_errors = check_new_sites(towerdb, tw_index, tw_bts, tw_bill,tw_status, msa_sites, tw_sites, uip_sites)\n",
        "\n",
        "# New sites = 132\n",
        "# Error on BTS sites out of UIP File"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>New_Sites</th>\n",
              "      <th>on air / active</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>199722</td>\n",
              "      <td>WIP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18524</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24728</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>76139</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>154588</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>13848</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>16132</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>25746</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>69521</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>145463</td>\n",
              "      <td>Dismantled</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>138 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     New_Sites on air / active\n",
              "0       199722             WIP\n",
              "1        18524      Dismantled\n",
              "2        24728      Dismantled\n",
              "3        76139      Dismantled\n",
              "4       154588      Dismantled\n",
              "..         ...             ...\n",
              "133      13848      Dismantled\n",
              "134      16132      Dismantled\n",
              "135      25746      Dismantled\n",
              "136      69521      Dismantled\n",
              "137     145463      Dismantled\n",
              "\n",
              "[138 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "pOiaaRigSI3j",
        "outputId": "6b8fac37-eb21-4fa5-d987-d0cbb813bea3"
      },
      "source": [
        "df_bts_errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>on air / active</th>\n",
              "      <th>Bts_Site</th>\n",
              "      <th>Billing Trigger date</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sites</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>199722</th>\n",
              "      <td>199722</td>\n",
              "      <td>WIP</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Code on air / active Bts_Site Billing Trigger date\n",
              "Sites                                                       \n",
              "199722  199722             WIP      Yes                  NaT"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWLKioNuRru9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgLjHpQFLupb"
      },
      "source": [
        "Sixtith Check on air sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYF3LphxLvIv"
      },
      "source": [
        "df_on_air = check_on_air_sites(actives, tw_index, pick_cols)\n",
        "df_on_air\n",
        "# NO Errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb7vr-l6nO3Z"
      },
      "source": [
        "Check Wip SItes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgxfEk5qnKdd"
      },
      "source": [
        "\"\"\" Wip Sites Check\"\"\"\n",
        "def check_wip(df_tw,tw_index, wip_tw, tw_bts, tw_status, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[wip_tw]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[wip_tw]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index, tw_status,wip_tw, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "wip_out_tw_list, df_wip_and_bts_flagged = check_wip(towerdb,tw_index, tw_wip, tw_bts, msa, msa_index, msa_wip)\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZpkKgsNn5vR"
      },
      "source": [
        "Check Decomissioned sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it4pR9Srn1zw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "outputId": "ac5b5ea3-158f-4e6b-e51f-390715f38715"
      },
      "source": [
        "df_decom_sites = check_decommissioned(towerdb, tw_index, tw_decom, tw_doer)\n",
        "df_decom_sites\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>Decommissioned_Site only for VF</th>\n",
              "      <th>Date of equipment removal (from MAR´21)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Code, Decommissioned_Site only for VF, Date of equipment removal (from MAR´21)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL92Kkk1osOj"
      },
      "source": [
        "Check Doer columns for in service sites\n",
        "\n",
        "Should not to be in past or different of blank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPbkQQC8oroV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "outputId": "707859a3-a311-4f56-de4b-1dde84e76e75"
      },
      "source": [
        "#COluna Doer tem valores fora do formato\n",
        "actives[tw_doer] = actives[tw_doer].replace('Not removed', '')\n",
        "df_doer = check_tw_bill_doer(actives, tw_index, tw_doer, tw_bts, 'Yes', 'doer')\n",
        "df_doer\n",
        "#No errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>Bts_Site</th>\n",
              "      <th>Date of equipment removal (from MAR´21)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Code, Bts_Site, Date of equipment removal (from MAR´21)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPWyxksEyBQp"
      },
      "source": [
        "*Tenth* - Check UIP Towerdb matches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOub5bqnxKqo"
      },
      "source": [
        "def check_uip_tw(df_tw,tw_index, status_tw_col, tw_bts_col, tw_critical_col, df_uip, uip_sites):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    uis_sites_not_in_towerdb = []\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "    if uis_sites_not_in_towerdb:\n",
        "        uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS In Month not active in TowerDB!'])\n",
        "        uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS In Month not active in TowerDB!',\\\n",
        "                                        right_on=tw_index)\n",
        "        uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS In Month not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS In Month!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS In Month!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS In Month!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "\n",
        "    \"\"\"tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "    decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "    decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                    right_on=tw_index)\n",
        "    decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\"\"\"\n",
        "\n",
        "    #Check BTS sites\n",
        "        #Check BTS sites\n",
        "    bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "    uip_bts = [i for i in df_uip[df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]                            \n",
        "    print(bts_sites)\n",
        "    print(uip_bts)\n",
        "    #if not set(bts_sites).intersection(uip_sites):\n",
        "    bts_sites_out_uip = [i for i in uip_bts if i not in bts_sites]\n",
        "    \n",
        "    bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['UIS BTS not in TowerDB(BTS)'])\n",
        "    bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='UIS BTS not in TowerDB(BTS)',\\\n",
        "                                    right_on=tw_index)\n",
        "    bts_sites_out_uip = bts_sites_out_uip[['UIS BTS not in TowerDB(BTS)', status_tw_col]]\n",
        "\n",
        "    #  Check for UIP critical sites \n",
        "    uip_critical = [i for i in df_uip[df_uip['Commercials for sites beyond 10% cap of critical sites(Annual)']!=0]['Site_ID']]\n",
        "    bts_tw_critical = [i for i in df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]]\n",
        "\n",
        "    critical = []\n",
        "    if len(uip_critical) > 0:\n",
        "        #if set(uip_critical).intersection(bts_tw_critical):\n",
        "        critical = [i for i in uip_critical if i not in bts_tw_critical]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                    right_on=tw_index)\n",
        "        critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "    #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "    return uis_sites_not_in_towerdb, in_service_not_in_uis,  bts_sites_out_uip, critical\n",
        "\n",
        "in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical = check_uip_tw(towerdb,tw_index, tw_status, \\\n",
        "                                                              tw_bts,tw_critical, \\\n",
        "                                                              uip, uip_sites)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "MpuuyIyvS9kT",
        "outputId": "0a37902f-852d-41f9-ce69-c3f8d2bc6d2b"
      },
      "source": [
        "in_service_uip_sites"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UIS In Month not active in TowerDB!</th>\n",
              "      <th>on air / active</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1184</td>\n",
              "      <td>Decommissioned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27266</td>\n",
              "      <td>Decommissioned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>52208</td>\n",
              "      <td>Decommissioned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53060</td>\n",
              "      <td>Decommissioned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>74808</td>\n",
              "      <td>Decommissioned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>173929</td>\n",
              "      <td>Decommissioned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>181769</td>\n",
              "      <td>WIP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>95151</td>\n",
              "      <td>WIP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>92098</td>\n",
              "      <td>WIP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>196617</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>19445</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    UIS In Month not active in TowerDB! on air / active\n",
              "0                                  1184  Decommissioned\n",
              "1                                 27266  Decommissioned\n",
              "2                                 52208  Decommissioned\n",
              "3                                 53060  Decommissioned\n",
              "4                                 74808  Decommissioned\n",
              "5                                173929  Decommissioned\n",
              "6                                181769             WIP\n",
              "7                                 95151             WIP\n",
              "8                                 92098             WIP\n",
              "9                                196617             NaN\n",
              "10                                19445             NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "2R0puB_qPtIi",
        "outputId": "afe7dcb1-75f7-41bd-bd60-6876cc28e253"
      },
      "source": [
        "bts_sites_out_uip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UIS BTS not in TowerDB(BTS)</th>\n",
              "      <th>on air / active</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19445</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UIS BTS not in TowerDB(BTS) on air / active\n",
              "0                        19445             NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVELEIlf0EGZ"
      },
      "source": [
        "Commercial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "le6_RE6XRQBm",
        "outputId": "e9bf2774-3114-46ab-e54d-a9b1dfb359af"
      },
      "source": [
        "def check_diffs_v2(path_current, path_last, sheet='Commercial'):\n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def highlight_diff(data, color='yellow'):\n",
        "        attr = 'background-color: {}'.format(color)\n",
        "        other = data.xs('Current', axis='columns', level=-1)\n",
        "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                            index=data.index, columns=data.columns)\n",
        "\n",
        "    _actual = pd.read_excel(path_current,sheet_name=sheet).fillna('')\n",
        "\n",
        "    _before = pd.read_excel(path_last,sheet_name=sheet).fillna('')\n",
        "\n",
        "    df_all = pd.concat([_actual, _before],axis='columns', keys=['Current', 'Last'])\n",
        "    df_final = df_all.swaplevel(axis='columns')[_actual.columns[1:]]\n",
        "\n",
        "    #df_final.style.apply(highlight_diff, axis=None)\n",
        "    if not df_final.empty:\n",
        "        return df_final[(_actual != _before).any(1)].style.apply(highlight_diff, axis=None)\n",
        "    else:\n",
        "        print('\\nNo differences Founded!\\n')\n",
        "        \n",
        "#cols_ordered = ['Charge_Type','Sub_Charge_Type', 'Param1','Param2', 'Data_Type','Input_Value_actual', 'Input_Value_before' ,'Description/Instruction', 'Frequency of Update']\n",
        "df_com = check_diffs_v2( '/content/UserInput_Spain_202107 v31.xlsx', '/content/UserInput_Spain_20210731.xlsx')\n",
        "df_com"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_18728262_faa3_11eb_abbd_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" colspan=2>Sub_charge_Type</th>        <th class=\"col_heading level0 col2\" colspan=2>Param1</th>        <th class=\"col_heading level0 col4\" colspan=2>Param2</th>        <th class=\"col_heading level0 col6\" colspan=2>Data_Type</th>        <th class=\"col_heading level0 col8\" colspan=2>Input_Value</th>        <th class=\"col_heading level0 col10\" colspan=2>Description/Instruction</th>        <th class=\"col_heading level0 col12\" colspan=2>Frequency of Update</th>    </tr>    <tr>        <th class=\"blank level1\" ></th>        <th class=\"col_heading level1 col0\" >Current</th>        <th class=\"col_heading level1 col1\" >Last</th>        <th class=\"col_heading level1 col2\" >Current</th>        <th class=\"col_heading level1 col3\" >Last</th>        <th class=\"col_heading level1 col4\" >Current</th>        <th class=\"col_heading level1 col5\" >Last</th>        <th class=\"col_heading level1 col6\" >Current</th>        <th class=\"col_heading level1 col7\" >Last</th>        <th class=\"col_heading level1 col8\" >Current</th>        <th class=\"col_heading level1 col9\" >Last</th>        <th class=\"col_heading level1 col10\" >Current</th>        <th class=\"col_heading level1 col11\" >Last</th>        <th class=\"col_heading level1 col12\" >Current</th>        <th class=\"col_heading level1 col13\" >Last</th>    </tr></thead><tbody>\n",
              "        </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f1d178b9950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze_QyJXE7l9o",
        "outputId": "4558405b-a6fd-4b65-9ec7-0a2f9e58631d"
      },
      "source": [
        "df_list = [new_sites,actives_dates_errors, no_actives_dates_errors, df_in_service_picklist,df_bts_errors,in_service_uip_sites,bts_sites_out_uip]\n",
        "sheetnames = ['New Sites','Actives Sites without Billin trigger Date', 'Not In Service without DOER','In Service Picklist Erros',\n",
        "              'BTS Flagged in Towerdb out of UIP File','UIS In Month not active in TowerDB!', 'UIS BTS not in TowerDB(BTS)' ]\n",
        "\n",
        "path = '/content/TWDB_ES_Errors_Aug.xlsx'\n",
        "general_log_erros(df_list, sheetnames, path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/openpyxl/workbook/child.py:102: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
            "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13M_kfik0zsh"
      },
      "source": [
        "Lc Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoRilmA1-Pxw"
      },
      "source": [
        "ints = ['Inhabitants', 'Altitude', '# of Lease Contracts', 'ORANGE', 'Telefonica', 'YOIGO']\n",
        "for i in ints:\n",
        "    towerdb[i] = towerdb[i].fillna(0)\n",
        "    towerdb[i] = list(map(int, towerdb[i]))\n",
        "\n",
        "dates_tw = ['First_Active_Sharing_Start_Date', 'First_Active_Sharing_End_Date', 'Billing Trigger date']\n",
        "for i in dates_tw:\n",
        "    towerdb[i] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in towerdb[i]]\n",
        "    \n",
        "towerdb['Date of equipment removal (from MAR´21)'] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) and not isinstance(date_obj, str)\\\n",
        "                                                      else '' for date_obj in towerdb['Date of equipment removal (from MAR´21)']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lupmi1s0Wyvi"
      },
      "source": [
        "def date_parser(df, columns, format, type_dates='normal'):\n",
        "    t_col = type_dates.lower()\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqwvl8K60yKr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "6b06925a-806b-41e9-c661-8f8858208125"
      },
      "source": [
        "def date_parser(df, columns, format, type_dates='normal'):\n",
        "    t_col = type_dates.lower()\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(format) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "            \n",
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "sheet= 'LC_INPUT_SPAIN'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "lc = pd.read_excel(pathtw, sheet)\n",
        "dates_lc = ['Inicio','Fin vigen/']\n",
        "date_parser(lc,dates_lc,\"%d/%m/%Y\")\n",
        "lc = lc.fillna('')\n",
        "lc.head(3)\n",
        "#print([i for i in list(lc['Código']) if list(lc['Código']).count(i)>1])\n",
        "#NO erros"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Zona</th>\n",
              "      <th>Código</th>\n",
              "      <th>Towering</th>\n",
              "      <th>Nombre Loc</th>\n",
              "      <th>Nombre 1</th>\n",
              "      <th>Nº Acreedor</th>\n",
              "      <th>Número de objeto</th>\n",
              "      <th>Por mes</th>\n",
              "      <th>Period.</th>\n",
              "      <th>Importe anual</th>\n",
              "      <th>Fin vigen/</th>\n",
              "      <th>Siguiente</th>\n",
              "      <th>Fin+tardío</th>\n",
              "      <th>PrimFinCon</th>\n",
              "      <th>Inicio</th>\n",
              "      <th>Relación a</th>\n",
              "      <th>Válido de</th>\n",
              "      <th>Validez a</th>\n",
              "      <th>Status actual</th>\n",
              "      <th>NºAlquiler</th>\n",
              "      <th>Alquiler Anterior</th>\n",
              "      <th>Denomin.forma pago</th>\n",
              "      <th>Clase de condición</th>\n",
              "      <th>Objetivo cond.</th>\n",
              "      <th>CeBe</th>\n",
              "      <th>Comentario Central</th>\n",
              "      <th>Comentario Zona</th>\n",
              "      <th>Código Municipio</th>\n",
              "      <th>Provincia</th>\n",
              "      <th>Municipio</th>\n",
              "      <th>Población</th>\n",
              "      <th>Tipología</th>\n",
              "      <th>Crea el</th>\n",
              "      <th>Entorno</th>\n",
              "      <th>Den.cl.contrato</th>\n",
              "      <th>Gpo.autoriz.</th>\n",
              "      <th>Nombre/Dirección</th>\n",
              "      <th>Calle Cf de Red</th>\n",
              "      <th>Calle Volare</th>\n",
              "      <th>FeInFljFin</th>\n",
              "      <th>Cód.postal</th>\n",
              "      <th>Autofactur</th>\n",
              "      <th>Nº teléfono</th>\n",
              "      <th>Número de fax</th>\n",
              "      <th>Dir.cor.elec.</th>\n",
              "      <th>Denominación de contrato</th>\n",
              "      <th>DFórmCálc</th>\n",
              "      <th>IDObjCál</th>\n",
              "      <th>Descr.objeto</th>\n",
              "      <th>Núm.objeto parámetro</th>\n",
              "      <th>ExsRegPror</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>7657.56</td>\n",
              "      <td>30/09/2022</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>01/10/1995</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>2</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>37262.4</td>\n",
              "      <td>31/12/2023</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>01/01/1996</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>32000</td>\n",
              "      <td>05/12/2021</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>06/04/1995</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Zona  Código Towering  ... Descr.objeto Núm.objeto parámetro ExsRegPror\n",
              "0            1           ...                                             \n",
              "1            2           ...                                             \n",
              "2            3           ...                                             \n",
              "\n",
              "[3 rows x 51 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx7ZCqDubz35"
      },
      "source": [
        "list(lc['Código'].value_counts()>1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtpOiwl6W2fn"
      },
      "source": [
        "def check_amounts(df_check, df_index, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            value = df.loc[site,column]\n",
        "            print(value)\n",
        "            if not value.__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "\n",
        "lc_cols = ['Código', '   Importe anual']\n",
        "#lc['Importe anual'] = lc['   Importe anual'].astype(str)\n",
        "#Python interpretou como float a coluna de Import Anual\n",
        "df_lc_amount = check_amounts(lc, 'Código', lc_cols, '.')\n",
        "df_lc_amount\n",
        "# No errors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPAToaUZaAKH"
      },
      "source": [
        "lc['Importe anual'] = lc['   Importe anual'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA9oWJyuW5AC",
        "outputId": "b968c3cb-3ed4-4b3c-a051-35bc85db5cf3"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "    df[start_date] = pd.to_datetime(df[start_date])\n",
        "    df[end_date] = pd.to_datetime(df[end_date], errors='coerce')\n",
        "    filtered = df.loc[pd.to_datetime(df[start_date]) > df[end_date], [tw_index, start_date,end_date]]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "        \n",
        "df_lc_dates = check_lc_ta_dates(lc,'Código', 'Inicio','Fin vigen/')\n",
        "df_lc_dates"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzMgwJR6iZ62"
      },
      "source": [
        "lc.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAHLU4YMwwpi"
      },
      "source": [
        "Looking for differences between newest and oldest LC file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJT6ngWsfl0Y"
      },
      "source": [
        "df=pd.read_csv('/content/LC_Input_Spain_20210731.csv', encoding='ISO-8859-15')\n",
        "df.columns.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXLR_77BucBD"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                            path_save, old_file, new_file, type_file='mix',dates=[], sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        #fit_cols = \n",
        "\n",
        "        #df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    dates = lower_str(dates)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        cols = ['Zona','Código','Towering','Nombre Loc','Nombre 1','Nº Acreedor','Número de objeto','       Por mes','Period.','   Importe anual','Fin vigen/','Siguiente','Fin+tardío','PrimFinCon','Inicio','Relación a','Válido de','Validez a','Status actual','NºAlquiler','Alquiler Anterior','Denomin.forma pago','Clase de condición','Objetivo cond.','CeBe','Comentario Central','Comentario Zona','Código Municipio','Provincia','Municipio','Población','Tipología','Crea el','Entorno','Den.cl.contrato','Gpo.autoriz.','Nombre/Dirección','Calle Cf de Red','Calle Volare','FeInFljFin','Cód.postal','Autofactur','Nº teléfono','Número de fax','Dir.cor.elec.','Denominación de contrato','DFórmCálc','IDObjCál','Descr.objeto','Núm.objeto parámetro','ExsRegPror']\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0,names=cols, encoding='latin')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = df_NEW[lower_str(cols)]\n",
        "        if len(dates)>1:\n",
        "            for column in dates:\n",
        "                df_NEW[column] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in df_NEW[column]]\n",
        "                \n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = fit_df(path_OLD,bill_cols)\n",
        "        df_NEW = fit_df(path_NEW,bill_cols)\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_file}) vs ({new_file}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "pathold = '/content/LC_Input_Spain_20210731.csv'\n",
        "to_save_lc = '/content/ES_LC_'\n",
        "sheet_lc= 'LC_INPUT_SPAIN'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "old_n = '20210731.csv'\n",
        "new_n = '20210809.xlsx'\n",
        "lc_bill = ['Código', '   Importe anual','Inicio','Fin vigen/']\n",
        "dates_lc = ['Inicio','Fin vigen/']\n",
        "\"\"\"path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "   path_save, old_file, new_file, type_file='mix', sheetname='', skipr=0, skipc=0)\"\"\"\n",
        "find_diffs_between_files(pathold, pathtw, 'Código', lc_bill,to_save_lc,old_n,new_n, type_file='mix',dates = dates_lc, sheetname=sheet_lc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4jp7suZ1BAQ"
      },
      "source": [
        "TA OSP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br86vCWY1Aia"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "sheet= 'TA_Input_Lease_OSP_Spain'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "ta_bill = ['Código', 'Importe anual','Inicio','F/cie/tec/']\n",
        "ta_osp = pd.read_excel(pathtw, sheet)\n",
        "\n",
        "ta_osp = ta_osp.fillna('')\n",
        "#ta_osp = ta_osp[ta_osp['Inicio']!=\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6IJ4iHKagBt"
      },
      "source": [
        "ta_osp_cols = ['Código', 'Importe anual']  \n",
        "\n",
        "ta_osp['Importe anual'] = ta_osp['Importe anual'].astype(str)\n",
        "\n",
        "df_ta_osp_amount = check_amounts(ta_osp, 'Código', ta_osp_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sh04VEBaZMX"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "    df[start_date] = pd.to_datetime(df[start_date])\n",
        "    df[end_date] = pd.to_datetime(df[end_date], errors='coerce')\n",
        "    filtered = df.loc[pd.to_datetime(df[start_date]) > df[end_date], [tw_index, start_date,end_date]]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "        \n",
        "df_ta_osp_dates = check_lc_ta_dates(ta_osp,'Código', 'Inicio','F/cie/tec/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cigSffhh1K8w"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "sheet= 'TA_Input_Lease_TME_Spain'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "\n",
        "ta_tme = pd.read_excel(pathtw, sheet)\n",
        "ta_tme = ta_tme.fillna('')\n",
        "#ta_tme = ta_tme[ta_tme['Inicio']!=\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQx4IntFWWsj"
      },
      "source": [
        "ta_tme.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoGuTQEfUamJ"
      },
      "source": [
        "def check_lc_ta_dates(df,tw_index, start_date,end_date):\n",
        "    df[start_date] = pd.to_datetime(df[start_date])\n",
        "    df[end_date] = pd.to_datetime(df[end_date], errors='coerce')\n",
        "    filtered = df.loc[pd.to_datetime(df[start_date]) > df[end_date], [tw_index, start_date,end_date]]\n",
        "    print(filtered)\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index, start_date,end_date]]\n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "df_ta_tme_dates = check_lc_ta_dates(ta_tme,'Código', 'Inicio','F/cie/tec/')\n",
        "df_ta_tme_dates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0KTML1xUfKP"
      },
      "source": [
        "def check_amounts(df_check, df_index,  columns, am_cols, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in am_cols:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                value = str(df.loc[site,column])\n",
        "                if value not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = value\n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_new.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_new.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "                else:\n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_new.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_new.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "            #else:\n",
        "                #print('Contem!')\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "\n",
        "ta_tme_cols = ['Código', 'Importe anual'] \n",
        "am= ['Importe anual']\n",
        "#ta_tme['Importe anual'] = ta_tme['Importe anual'].astype(str)\n",
        "df_ta_tme_amount = check_amounts(ta_tme, 'Código', ta_tme_cols, am)\n",
        "df_ta_tme_amount\n",
        "# No errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXtLlVp91MJL"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "sheet= 'TA_Input_Lease_MM_Spain'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "ta_mm = pd.read_excel(pathtw, sheet)\n",
        "\n",
        "ta_mm = ta_mm.fillna('')\n",
        "ta_mm = ta_mm[ta_mm['Inicio']!=\"\"]\n",
        "\n",
        "ta_mm_cols = ['Código', 'Importe anual']  \n",
        "\n",
        "ta_mm['Importe anual'] = ta_mm['Importe anual'].astype(str)\n",
        "df_ta_mm_amount = check_amounts(ta_mm, 'Código', ta_mm_cols)\n",
        "# No errors\n",
        "\n",
        "df_ta_mm_dates = check_lc_ta_dates(ta_mm,'Código', 'Inicio','F/cie/tec/')\n",
        "#NO erros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vquhmwrK1MX7"
      },
      "source": [
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "sheet= 'TA_Input_Lease_Others_Spain'\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "ta_others = pd.read_excel(pathtw, sheet)\n",
        "\n",
        "ta_others = ta_others.fillna('')\n",
        "ta_others = ta_others[ta_others['Inicio']!=\"\"]\n",
        "\n",
        "ta_o_cols = ['Código', 'Importe anual']\n",
        "\n",
        "ta_others['Importe anual'] = ta_others['Importe anual'].astype(str)\n",
        "df_ta_o_amount = check_amounts(ta_others, 'Código', ta_o_cols)\n",
        "# No errors\n",
        "\n",
        "df_ta_o_dates = check_lc_ta_dates(ta_others,'Código', 'Inicio','F/cie/tec/')\n",
        "#NO erros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNvIvZOUPvZ7"
      },
      "source": [
        "Looking for differences between newest and oldest TA files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9GJuqnRwlf3"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_file, new_file, type_file='mix', dates=[],\n",
        "                             sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        #fit_cols = lower_str(list(df.columns))\n",
        "        #df.columns = fit_cols\n",
        "        #df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        #df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    dates = lower_str(dates)\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        col = ['Zona','Código','Towering','Nombre Loc','Nombre 1','Número de objeto','       Por mes','Period.','Importe anual','F/cie/tec/','Siguiente','Fin+tardío','Válido de','Validez a','Status actual','NºAlquiler','Alquiler Anterior','Den.cl.contrato','Denomin.forma pago','Clase de condición','Objetivo cond.','CeBe','Comentario Central','Comentario Zona','Código Municipio','Provincia','Municipio','Crea el','Inicio','Entorno','Gpo.autoriz.','IDObjDist','ID objeto']\n",
        "        df_OLD = pd.read_csv(path_OLD, header=0, names=col, encoding='latin')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW.columns = lower_str(df_NEW.columns.to_list())\n",
        "        df_NEW = df_NEW[lower_str(col)]\n",
        "        if len(dates)>1:\n",
        "            for column in dates:\n",
        "                df_NEW[column] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in df_NEW[column]]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    else: \n",
        "        df_OLD = fit_df(path_OLD,bill_cols)\n",
        "        df_NEW = fit_df(path_NEW,bill_cols)\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_file}) vs ({new_file}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "pathtw = '/content/TowerDB_Spain_20210809.xlsx'\n",
        "\n",
        "pathoosp =  '/content/TA_Input_Lease_OSP_Spain_20210731.csv'\n",
        "sheet_osp = 'TA_Input_Lease_OSP_Spain'\n",
        "to_save_osp = '/content/TA_OSP'\n",
        "old_osp = '20210731.csv'\n",
        "new_osp = '20210809.xlsx'\n",
        "\n",
        "pathotme = '/content/TA_Input_Lease_TME_Spain_20210731.csv'\n",
        "sheet_tme = 'TA_Input_Lease_TME_Spain'\n",
        "to_save_tme = '/content/TA_TME'\n",
        "old_tme = '20210731.csv'\n",
        "new_tme = '20210809.xlsx'\n",
        "\n",
        "pathomm = '/content/TA_Input_Lease_MM_Spain_20210731.csv'\n",
        "sheet_mm = 'TA_Input_Lease_MM_Spain'\n",
        "to_save_mm = '/content/TA_MM'\n",
        "old_mm = '20210731.csv'\n",
        "new_mm = '20210809.xlsx'\n",
        "\n",
        "pathoot = '/content/TA_Input_Lease_Other_Spain_20210731.csv'\n",
        "sheet_others = 'TA_Input_Lease_Others_Spain'\n",
        "to_save_others = '/content/TA_Others'\n",
        "old_ot = '20210731.csv'\n",
        "new_ot = '20210809.xlsx'\n",
        "\n",
        "skipr = 0\n",
        "skipc = 0\n",
        "ta_bill = ['Código', 'Importe anual','Inicio','F/cie/tec/']\n",
        "\"\"\"(path_OLD, path_NEW, index_col, bill_cols, path_save, old_file, new_file, type_file='mix', sheetname='',\n",
        "                             skipr=0, skipc=0)\"\"\"\n",
        "dates_ta = ['Inicio','F/cie/tec/']\n",
        "find_diffs_between_files(pathoosp, pathtw, 'Código', ta_bill, to_save_osp,old_osp, new_osp, type_file='mix', dates=dates_ta, sheetname=sheet_osp)\n",
        "find_diffs_between_files(pathotme, pathtw, 'Código', ta_bill, to_save_tme,old_tme, new_tme,  type_file='mix', dates=dates_ta,sheetname=sheet_tme)\n",
        "find_diffs_between_files(pathomm, pathtw, 'Código', ta_bill, to_save_mm,old_mm, new_mm, type_file='mix', dates=dates_ta,sheetname=sheet_mm)\n",
        "find_diffs_between_files(pathoot, pathtw, 'Código', ta_bill, to_save_others,old_ot, new_ot, type_file='mix',dates=dates_ta, sheetname=sheet_others)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYIisJ7BvQ3Y"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             path_save, old_name,new_name, type_file='mix', status_col='', kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col].astype(str) + df[kind_col]\n",
        "\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = df_OLD.loc[row,col]\n",
        "                    value_NEW = df_NEW.loc[row,col]\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old,  engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW,header=0, names=cols_new).fillna('')\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "uis_new = '/content/UserInput_Spain_202107 v31.xlsx'\n",
        "uis_old = ''\n",
        "sheet = 'SiteLevel'\n",
        "uis_index = 'Site_ID (Numeric)'\n",
        "to_uis = '/content/ES_UIS_'\n",
        "old_uis = '20210630_True-Up.xlsx'\n",
        "new_uis = '202107 v31.xlsx'\n",
        "bill = []\n",
        "find_diffs_between_files(uis_old, uis_new, uis_index, bill, to_uis, old_uis,new_uis,'excel',status_col='',\\\n",
        "                         kind='',kind_col='', sheetname=sheet, skipr=2, skipc=0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}