{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "de_validations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1beLa5nrAKyE8co1MoPnyTYu_aNS955Hi",
      "authorship_tag": "ABX9TyPxj709lrwSqP5v3QsCdjYg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fJokH_7lGGq"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import re\n",
        "from openpyxl import Workbook, styles\n",
        "from openpyxl.styles import PatternFill, Font\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "from openpyxl.formatting.rule import Rule\n",
        "\n",
        "def csv_files(path):\n",
        "    \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "    import csv\n",
        "    f = open(path, encoding='windows-1252', errors='ignore')\n",
        "    data = []\n",
        "    for row in csv.reader(f, delimiter=','):\n",
        "        data.append(row)\n",
        "    col = [*data[0]]\n",
        "    data.pop(0)\n",
        "    df = pd.DataFrame(data, columns=col)\n",
        "    return df, col\n",
        "\n",
        "def read_files(path, sheetname, n_skiprows, n_skip_columns, site_index):\n",
        "    \"\"\"\n",
        "    Params:\\n\n",
        "    path: parth of file in the computer.\\n\n",
        "    n_skiprows: Number of rows to delete in the original file,.\\n\n",
        "    columns_to_convert: Columns to convert the data general type. \\n\n",
        "    n_skipcolumn: Columns to skip in the original file. \\n\n",
        "    endrow = pass 0 to read everything, 1 to count entire\n",
        "    columns_order: List of columns names in specific order to pass in the engine.\\n\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(path, sheet_name = sheetname, skiprows = n_skiprows)\n",
        "\n",
        "    df = df.iloc[:,n_skip_columns:]\n",
        "\n",
        "    # define the entiry columns which doesn't have NaN \n",
        "    df = df.dropna(subset=[site_index], axis=0)\n",
        "    \"\"\"cont = 0\n",
        "    for i in df.iloc[:,site_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[:cont, :]\n",
        "    #df.columns = columns_order\"\"\"\n",
        "    \n",
        "    # convert intery columns to integer \n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].fillna(0)\n",
        "    #df[columns_integer_convert] = df[columns_integer_convert].astype('int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "def lower_str(columns):\n",
        "    newlist = list(map(lambda x: x.lower(), columns))\n",
        "    return newlist\n",
        "\n",
        "def check_df(df, error_msg):\n",
        "    if df == None or df.empty:\n",
        "        return f'{error_msg}'\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def count_duplicates(lista):\n",
        "    count_dict = {}\n",
        "    for entry in lista:\n",
        "        if entry in count_dict.keys():\n",
        "            count_dict[entry] += 1\n",
        "        else:\n",
        "            count_dict[entry] = 1\n",
        "    \n",
        "    duplicates = {}\n",
        "    for k, v in count_dict.items():\n",
        "        if v > 1:\n",
        "            duplicates[k] = v\n",
        "    return pd.DataFrame.from_dict(duplicates, orient='index', columns=['# of Duplicates'])\n",
        "\n",
        "def defining_df(df, column_range, number_col):\n",
        "    cont = 0\n",
        "    for i in df.iloc[:,number_col]:\n",
        "        if pd.isnull(i):\n",
        "            break # acertar o break\n",
        "        else:\n",
        "            cont +=1\n",
        "    df = df.iloc[0:cont, :]\n",
        "    return df\n",
        "\n",
        "#old version\n",
        "def check_columns(table, output_columns):\n",
        "    \"\"\"\n",
        "    Check the total of number of missing columns and the missing columns in passed table.\\n\n",
        "\n",
        "    Params:\\n\n",
        "    table: contain the columns to be check.\\n\n",
        "    output_columns: columns structure at the final file. \n",
        "\n",
        "    Returns:\\n\n",
        "    Number of missing columns and a list that contains the name os missing columns.\n",
        "    \"\"\"\n",
        "    total_received = len(table.columns)\n",
        "    number_missing_columns = 0\n",
        "    missing_columns = []\n",
        "    #Counting of missing columns       \n",
        "    #if country in contries:      \n",
        "    for columns in output_columns:\n",
        "        if columns.lower() not in [labels.lower() for labels in table]:\n",
        "            number_missing_columns +=1\n",
        "            missing_columns.append(columns)\n",
        "    \n",
        "    return total_received, number_missing_columns, missing_columns\n",
        "\n",
        "def check_columns_received(df, bill_cols):\n",
        "    twdb_col = lower_str(list(df.columns))\n",
        "    col_miss = [i for i in bill_cols if i not in twdb_col]\n",
        "    \"\"\"\n",
        "    for i in bill_cols:\n",
        "        if i not in twdb_col:\n",
        "            col_miss.append(i)\"\"\"\n",
        "    df_col_missing = pd.DataFrame(col_miss, columns=['Column(s) Missing'], index=range(len(col_miss)))\n",
        "    return df_col_missing\n",
        "\n",
        "def replace_values(df, columns, value=\"\"):\n",
        "    \"\"\"\n",
        "    Está voltando para float\n",
        "    \"\"\"\n",
        "    invalid_values = ['N/A', 'n/a',\"0\", 0, '-', '_', np.nan,'nan']\n",
        "    #lista = []\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "    #df[column] = df[column].astype('int64')\n",
        "\n",
        "    for column in columns:\n",
        "        lista = []\n",
        "        for index in df[column]:\n",
        "            #print(f\"{column} -> {index}\")\n",
        "            if index in invalid_values:\n",
        "                lista.append(value)\n",
        "            else:\n",
        "                lista.append(index)\n",
        "        df[column] = lista\n",
        "\n",
        "    return df\n",
        "      \n",
        "def date_parser(df, columns, format=1, type_dates='normal'):\n",
        "    t_col = type_dates.lower()\n",
        "    if format == 1:\n",
        "        type_date = \"%d/%m/%Y\"\n",
        "    else:\n",
        "        type_date = \"%d-%m-%Y\"\n",
        "    for column in columns:\n",
        "        if t_col == 'mixed':\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) \\\n",
        "                        and not isinstance(date_obj, str) else date_obj for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "        else:\n",
        "            df[column] = [date_obj.strftime(type_date) if not pd.isnull(date_obj) else '' for date_obj in df[column]]\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "# Refactorar esse codigo para receber todas as colunas num dic\n",
        "# Sendo as keys=columns e values= picklist for each column\n",
        "\n",
        "def check_amounts(df_check, df_index, status_col, columns, pattern=','):\n",
        "    \"\"\"\n",
        "    Paramns:\n",
        "    pattern: general (. to decimal), other (, to decimal)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_check[columns]\n",
        "    df['sites'] = df[df_index]\n",
        "    df = df.set_index('sites')\n",
        "\n",
        "    df_new = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df.drop_duplicates(subset=[df_index], inplace=True)\n",
        "    \n",
        "    filtered = df_check[[df_index, status_col]]\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        for column in columns[1:]:\n",
        "            if not str(df.loc[site,column]).__contains__(pattern):\n",
        "                #print(df.loc[site,column])\n",
        "                if df.loc[site,df_index] not in df_new.index:\n",
        "                    df_new.loc[site,df_index] = df.loc[site,df_index]\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "                else:\n",
        "                    df_new.loc[site,column] = 'Incorret Format to separate decimal values'\n",
        "\n",
        "    df_new = df_new.dropna(how='all', axis=1).fillna('Ok')       \n",
        "    if not df_new.empty:\n",
        "        df_new = pd.merge(df_new, filtered, how='left', left_on='sites', right_on=tw_index)\n",
        "        df_new = df_new[['sites', status_col]]\n",
        "        return df_new\n",
        "    else: \n",
        "        print('No one columns with incorrect Amount format!')\n",
        "        \n",
        "def check_picklist(df,df_index,df_status, df_cols, picklist_dict):\n",
        "\n",
        "    df_picklist = df[df_cols]\n",
        "    df_picklist['sites'] = df[df_index]\n",
        "    df_picklist =  df_picklist.set_index('sites')\n",
        "    \n",
        "    #df_picklist = replace_values(df_picklist, df_cols, 0)\n",
        "\n",
        "    df_errors = pd.DataFrame(columns=df_cols)\n",
        "\n",
        "    for site in df[df_index]:\n",
        "        columns = [i.lower() for i in picklist_dict.keys()]\n",
        "        for column in set(columns): \n",
        "            value = str(df_picklist.loc[site,column])\n",
        "            #print(value)\n",
        "            if not value in set(picklist_dict[column]) or pd.isnull(value):\n",
        "                #print(set(picklist_dict[column]))\n",
        "\n",
        "                if not df_picklist.loc[site,df_index] in df_errors.index:\n",
        "                    df_errors.loc[site,df_index] = df_picklist.loc[site,df_index]\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "                else:\n",
        "                    \n",
        "                    if pd.isnull(value) or pd.isna(value) or value == 'nan' or value == '':\n",
        "                        df_errors.loc[site,column] = 'Blank Value'\n",
        "                    else:\n",
        "                        df_errors.loc[site,column] = f'Incorret picklist value: {value}'\n",
        "\n",
        "    #df = df_errors.dropna()   \n",
        "    df_errors = df_errors.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    if len(df_errors)>0:\n",
        "        df = df[[df_index, df_status]]\n",
        "        df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "        df_errors = df_errors.reset_index()\n",
        "    else:\n",
        "        print('\\nNo one Picklist Error Founded!\\n')\n",
        "    return df_errors\n",
        "\n",
        "def check_picklist_3(df,df_index,df_status,picklist_dict):\n",
        "    import json\n",
        "    log = {}\n",
        "\n",
        "    #picklist_dict={'Categorization by Transmission Sys':['Long-Term Mobile Sites','Macro','Non-Enterprise DAS','Repeater Sites','Public DAS Sites','Transmission']}\n",
        "    for column in picklist_dict:\n",
        "        df_aux=df.copy()\n",
        "        #print(type(picklist_dict[column]))\n",
        "        new=df_aux[column].isin(picklist_dict[column])\n",
        "        #print(new)\n",
        "        indexes=df.index[new == False].tolist()\n",
        "        if column not in log:log[column]=[]\n",
        "        log[column]=log[column]+indexes\n",
        "    newDict = {}\n",
        "    #print(log)\n",
        "    #print(json.dumps(picklist_dict,indent=2))\n",
        "    for key,value in log.items():\n",
        "      for val in value:\n",
        "          ID=df.iloc[val][df_index]\n",
        "          if ID in newDict:\n",
        "              v=f'Incorret picklist value: {df.iloc[val][key]}'\n",
        "              if df.iloc[val][key]!=df.iloc[val][key] or pd.isnull(df.iloc[val][key]) or  pd.isna(df.iloc[val][key]) or df.iloc[val][key]=='' or df.iloc[val][key]=='nan':\n",
        "                   v='Blank Value'\n",
        "              newDict[ID][key]=v\n",
        "          else:\n",
        "              v=f'Incorret picklist value: {df.iloc[val][key]}'\n",
        "              if df.iloc[val][key]!=df.iloc[val][key] or pd.isnull(df.iloc[val][key]) or  pd.isna(df.iloc[val][key]) or df.iloc[val][key]=='' or df.iloc[val][key]=='nan':\n",
        "                   v='Blank Value'\n",
        "              newDict[ID] = {'status': df.iloc[val][df_status],key:v}\n",
        "    #print(newDict)\n",
        "    logs = pd.DataFrame.from_dict(newDict, orient='index')\n",
        "    logs.index.name='Site'\n",
        "    logs = logs.dropna(how='all', axis=1).fillna('Ok!')\n",
        "    return logs\n",
        "\n",
        "def check_new_sites(df_towerdb, tw_index, bts_col, bill_col, status_col, msa_list, towerdb_list, uip_list):\n",
        "    \n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    \n",
        "    filtered = df_towerdb[[tw_index, status_col]]\n",
        "\n",
        "    #Finding for ALL NEW SITES\n",
        "    new_site = [i for i in towerdb_list if i not in msa_list]\n",
        "    new_sites = pd.DataFrame(new_site, columns=['New_Sites'])\n",
        "    new_sites = pd.merge(new_sites, filtered, how='left', left_on=['New_Sites'], right_on=tw_index)\n",
        "    new_sites = new_sites[['New_Sites', status_col]]\n",
        "\n",
        "    \n",
        "    #list of new sites out of UIP sheet\n",
        "    bts_out_uis = [i for i in df_towerdb[df_towerdb[bts_col]=='Yes'][tw_index] if str(i) not in uip_list]\n",
        "    bts_out_uis = pd.DataFrame(bts_out_uis, columns=['Bts_Sites_Out_UIS_File'])\n",
        "    bts_out_uis = pd.merge(bts_out_uis, filtered, how='left', left_on=['Bts_Sites_Out_UIS_File'], right_on=tw_index)\n",
        "    bts_out_uis = bts_out_uis[['Bts_Sites_Out_UIS_File', status_col]]\n",
        "\n",
        "    #create a copy to make index modifications\n",
        "    df = df_towerdb.copy()\n",
        "\n",
        "    #New columns with Site Codes\n",
        "    df['Sites'] = df[tw_index]\n",
        "    #defining site code to index\n",
        "    df.set_index('Sites', inplace=True)\n",
        "\n",
        "    #filtering by new sites\n",
        "    df = df[df[tw_index].isin(new_site)]\n",
        "\n",
        "    # Save information os sites with demerged date more than current date\n",
        "    df[bill_col] = df[bill_col].astype('datetime64[s]')\n",
        "    df_site_bts = df[(df[bts_col]=='Yes') | (df[bill_col] > current_date)]\n",
        "    \n",
        "    if not (new_sites.empty or bts_out_uis.empty or df_site_bts.empty):\n",
        "        return new_sites, bts_out_uis, df_site_bts[[tw_index,status_col, bts_col, bill_col]]\n",
        "        \n",
        "\n",
        "def check_tw_doer_planned(df_tw, tw_index, doer_col,bill_col, status_col, dt_format):\n",
        "    \"\"\"Only GR until now\"\"\"\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date, format=dt_format)\n",
        "    if not df_tw[doer_col].empty:\n",
        "        df_tw[bill_col] = pd.to_datetime(df_tw[bill_col],errors='coerce', format=dt_format)\n",
        "        filtered = df_tw[(df_tw[status_col]=='Planned')&(not df_tw[doer_col].astype('datetime64[ns]').empty)&\\\n",
        "                         (df_tw[bill_col].astype('datetime64[ns]') < current_date)]\n",
        "        return filtered[[tw_index, status_col, bill_col, doer_col]]  \n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "                                                              \n",
        "\n",
        "\n",
        "def check_lc_ta_dates(df,tw_index,status_col, start_date,end_date):\n",
        "    filtered = df[df[start_date] > df[end_date]]\n",
        "    return filtered[[tw_index,status_col, start_date,end_date]]\n",
        "\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, decom_col, tw_bts_col, tw_critical_col, df_uip, uip_sites, country):\n",
        "    t1 = ['pt', 'de', 'cz', 'ie', 'es', 'ro', 'hu']\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    if country.lower() in t1:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIS File!'])\n",
        "        in_service_uip_sites = pd.merge(in_service_uip_sites, filtered, how='left', left_on='Site In service out of UIS File!',\\\n",
        "                                        right_on=tw_index)\n",
        "        in_service_uip_sites = in_service_uip_sites[['Site In service out of UIS File!', status_tw_col]]\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        if decom_col != \"\":\n",
        "            tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index] if i in uip_sites]\n",
        "            decomiss_sites_in_uip = pd.DataFrame(tw_decomiss, columns=['Decomissioned Site in UIS File'])\n",
        "            decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "        \n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "            bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIPS File', status_tw_col]]\n",
        "            if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "                return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "        else:\n",
        "            #Check BTS sites\n",
        "            bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "            bts_sites_out_uip = []\n",
        "            if not set(bts_sites).intersection(uip_sites):\n",
        "                bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "            bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "            bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "            #  Check for UIP critical sites \n",
        "            uip_critical = [i for i in df_uip[(df_uip['Commercials for sites beyond 10% cap of critical sites (Annual)']!='')&\\\n",
        "                                        df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]\n",
        "            bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Beyond 10%'][tw_index]\n",
        "            critical = []\n",
        "            if len(uip_critical) > 0:\n",
        "                if set(uip_critical).intersection(bts_tw_critical):\n",
        "                    critical = [i for i in bts_tw_critical if i not in uip_critical]\n",
        "            critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% in out UIS File'])\n",
        "            critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "            critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "            if not (in_service_uip_sites.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "                return in_service_uip_sites, bts_sites_out_uip, critical\n",
        "    else:\n",
        "        #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "        count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "        # check number of sites that are in uip file and doesn't have in df_tw\n",
        "        in_service_uip_sites = []\n",
        "        if not set(count_tw_sites).intersection(uip_sites):\n",
        "            in_service_uip_sites = [i for i in uip_sites if i not in count_tw_sites]\n",
        "        in_service_uip_sites = pd.DataFrame(in_service_uip_sites,columns=['Site In service out of UIS File!'])\n",
        "        in_service_uip_sites = pd.merge(in_service_uip_sites, filtered, how='left', left_on='Site In service out of UIS File!',\\\n",
        "                                        right_on=tw_index)\n",
        "        in_service_uip_sites = in_service_uip_sites[['Site In service out of UIS File!', status_tw_col]]\n",
        "        \n",
        "        #check for decomissioned site not in uip files\n",
        "        tw_decomiss = [i for i in df_tw[df_tw[decom_col]=='Yes'][tw_index]]\n",
        "        decomiss_sites_in_uip = []\n",
        "        if set(tw_decomiss).intersection(uip_sites):\n",
        "            decomiss_sites_in_uip = [i for i in tw_decomiss if i in uip_sites]\n",
        "        decomiss_sites_in_uip = pd.DataFrame(decomiss_sites_in_uip, columns=['Decomissioned Site in UIS File'])\n",
        "        decomiss_sites_in_uip = pd.merge(decomiss_sites_in_uip, filtered, how='left', left_on='Decomissioned Site in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        decomiss_sites_in_uip = decomiss_sites_in_uip[['Decomissioned Site in UIS File', status_tw_col]]\n",
        "        \n",
        "        #Check BTS sites\n",
        "        bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "        bts_sites_out_uip = []\n",
        "        if not set(bts_sites).intersection(uip_sites):\n",
        "            bts_sites_out_uip = [i for i in bts_sites if i not in uip_sites]\n",
        "        bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['BTS Site not in UIS File'])\n",
        "        bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='BTS Site not in UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        bts_sites_out_uip = bts_sites_out_uip[['BTS Site not in UIS File', status_tw_col]]\n",
        "\n",
        "        #  Check for UIP critical sites \n",
        "        uip = [i for i in df_uip['Site_ID']]\n",
        "        bts_tw_critical = df_tw[df_tw[tw_critical_col]=='Yes'][tw_index]\n",
        "        critical = []\n",
        "        if set(uip).intersection(bts_tw_critical):\n",
        "            critical = [i for i in bts_tw_critical if i not in uip]\n",
        "        critical = pd.DataFrame(critical, columns=['Sites with critical level beyond 10% out in UIS File'])\n",
        "        critical = pd.merge(critical, filtered, how='left', left_on='Sites with critical level beyond 10% in out UIS File',\\\n",
        "                                         right_on=tw_index)\n",
        "        critical = critical[['Sites with critical level beyond 10% in out UIS File', status_tw_col]]\n",
        "        if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "            return in_service_uip_sites, decomiss_sites_in_uip, bts_sites_out_uip, critical\n",
        "\n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "        #lista = []\n",
        "\n",
        "        df.fillna(0, inplace=True)\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    \n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names).fillna('')\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names ).fillna('')\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols).fillna('')\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    if not df_commercial_diffs.empty:\n",
        "        return df_commercial_diffs\n",
        "    else:\n",
        "        print('\\nNo errors founded!')\n",
        "\n",
        "\n",
        "\n",
        "def general_log_erros(df_list, sheet_list, path):\n",
        "    writer = pd.ExcelWriter(path,engine='openpyxl')   \n",
        "    for dataframe, sheet in zip(df_list, sheet_list):\n",
        "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   \n",
        "    writer.save() \n",
        "\n",
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, \\\n",
        "                             status_col, path_save, type_file='mix',kind='tw', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, bill_cols):\n",
        "        fit_cols = lower_str(list(df.columns))\n",
        "        df.columns = fit_cols\n",
        "        df = df[bill_cols]\n",
        "        df = df.dropna(subset=[index_col], axis=0)\n",
        "        df = df.drop_duplicates(subset=[index_col], keep='last')\n",
        "        df[index_col] = df[index_col].astype(str)\n",
        "        df['sites'] = df[index_col]\n",
        "        # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "        df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin').fillna('')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin').fillna('')\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "\n",
        "    elif type_file == 'mix':\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD = fit_df(df_OLD,bill_cols)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,bill_cols)\n",
        "    else: \n",
        "        df_OLD = fit_df(path_OLD,bill_cols)\n",
        "        df_NEW = fit_df(path_NEW,bill_cols)\n",
        "\n",
        "\n",
        "    # Perform Diff\n",
        "    new_copy = df_NEW.copy()\n",
        "    droppedRows = []\n",
        "    newRows = []\n",
        "\n",
        "    cols_OLD = df_OLD.columns\n",
        "    cols_NEW = df_NEW.columns\n",
        "    sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "\n",
        "    for row in new_copy.index:\n",
        "        if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "            for col in sharedCols:\n",
        "                value_OLD = df_OLD.loc[row,col]\n",
        "                value_NEW = df_NEW.loc[row,col]\n",
        "               #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                if value_OLD == value_NEW:\n",
        "                    new_copy.loc[row,col] = np.nan\n",
        "                else:\n",
        "                    new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "        else:\n",
        "            newRows.append(row)\n",
        "\n",
        "    new_copy = new_copy.dropna(axis=0, how='all')\n",
        "    new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "    for row in df_OLD.index:\n",
        "        if row not in df_NEW.index:\n",
        "            droppedRows.append(row)\n",
        "            new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "    \n",
        "    new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "    \n",
        "    new_copy = new_copy.reset_index()\n",
        "    if kind=='tw':\n",
        "        sites = [i for i in new_copy['sites']] \n",
        "        old = df_OLD[[status_col]].reset_index()\n",
        "        old = old.loc[old['sites'].isin(sites)]\n",
        "        new = df_NEW[[status_col]].reset_index()\n",
        "        new = new.loc[new['sites'].isin(sites)]\n",
        "        df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "        new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save}_old_file vs new_file.xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    new_copy.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ1000', highlight)\n",
        "    \n",
        "     \n",
        "    #print(len(newRows))\n",
        "    for site in new_copy['sites']:\n",
        "        if site in newRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = new_copy.index[new_copy[index_col]==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "    \n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeqK9FbGaUw2"
      },
      "source": [
        "Deixar para testar depois"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb3PlAut-BKy"
      },
      "source": [
        "bill_cols = [\"Code\",\\\n",
        "            \"Categorization by Transmission Sys\",\\\n",
        "            \"Categorization by Transmission Sys (sub-cluster)\",\\\n",
        "            \"Categorization by Site Type\",\"Categorization by Indoor / Outdoor Site (Antennas)\",\\\n",
        "            \"Categorization by Strategic Site\",\\\n",
        "            \"Categorization by Critical Site\",\\\n",
        "            \"Site Cluster\",\\\n",
        "            \"Strategic Site Bucket\",\\\n",
        "            \"Bts_sites\",\\\n",
        "            \"CriticalSite_Beyond_10\",\\\n",
        "            \"Sites As Metered Estimated\",\\\n",
        "            \"Active Sharing Arrangement\",\\\n",
        "            \"Indoor_Site_Any_Climate_Control\",\\\n",
        "            \"Outdoor_Site_With_Dc_Power\",\\\n",
        "            \"Rfai_Date\",\\\n",
        "            \"decommissioned site\",\\\n",
        "            \"Subsequent_Active_Sharing_Arrangement\",\\\n",
        "            \"First_Active_Sharing_Deployment_Type\",\\\n",
        "            \"First_Active_Sharing_Arrangement_Start_Date\",\\\n",
        "            \"First_Active_Sharing_Arrangement_End_Date\",\\\n",
        "            'Site Status (TIMS)']\n",
        "pathnew = '/content/TowerDB_Germany_20210731.xlsx'\n",
        "pathold = '/content/TowerDB_Germany_20210630.csv'\n",
        "to_save = '/content/DE_TW'\n",
        "find_diffs_between_files(pathold, pathnew, 'Code', bill_cols,'Site Status (TIMS)', to_save,'mix','tw', 'Sheet1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxdOa6NFIYru"
      },
      "source": [
        "Check TowerDB Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_hqckMPb96D"
      },
      "source": [
        "tw_index = 'code'\n",
        "tw_doer = 'date_of_equipment_removal'\n",
        "tw_status = 'site status (tims)'\n",
        "tw_bts = 'bts_sites'\n",
        "tw_bill = 'rfai ( ready for active installation ) '\n",
        "tw_wip = 'wip_sites'\n",
        "tw_decom = 'decommissioned site'\n",
        "tw_critical = 'critical site'\n",
        "\n",
        "msa_index = 'code'\n",
        "msa_doer = 'date_of_equipment_removal'\n",
        "msa_status = 'site status (tims)'\n",
        "msa_bts = 'bts_sites'\n",
        "msa_bill = 'rfai ( ready for active installation ) '\n",
        "msa_wip = 'wip_sites'\n",
        "msa_decom = 'decommissioned site'\n",
        "msa_critical = 'critical site'\n",
        "bill_col = [\"Code\",\"Categorization by Transmission Sys\",\"Categorization by Transmission Sys (sub-cluster)\",\\\n",
        "             \"Categorization by Site Type\",\"Categorization by Indoor / Outdoor Site (Antennas)\",\\\n",
        "             \"Categorization by Strategic Site\",\"Categorization by Critical Site\",\"Site Cluster\",\\\n",
        "             \"Strategic Site Bucket\",\"Bts_sites\",\"CriticalSite_Beyond_10\",\"Sites As Metered Estimated\",\"Active Sharing Arrangement\",\\\n",
        "             \"Indoor_Site_Any_Climate_Control\",\"Outdoor_Site_With_Dc_Power\",\"Rfai_Date\",\"decommissioned site\",\\\n",
        "             \"Subsequent_Active_Sharing_Arrangement\",\"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Arrangement_Start_Date\",\\\n",
        "             \"First_Active_Sharing_Arrangement_End_Date\"]\n",
        "\n",
        "bill = [\"Code\",\"Categorization by Transmission Sys\",\"Categorization by Transmission Sys (sub-cluster)\",\\\n",
        "             \"Categorization by Site Type\",\"Categorization by Indoor / Outdoor Site (Antennas)\",\\\n",
        "             \"Categorization by Strategic Site\",\"Categorization by Critical Site\",\"Site Cluster\",\\\n",
        "             \"Strategic Site Bucket\",\"Bts_sites\",\"CriticalSite_Beyond_10\",\"Sites As Metered Estimated\",\"Active Sharing Arrangement\",\\\n",
        "             \"Indoor_Site_Any_Climate_Control\",\"Outdoor_Site_With_Dc_Power\",\"Rfai_Date\",\"decommissioned site\",\\\n",
        "             \"Subsequent_Active_Sharing_Arrangement\",\"First_Active_Sharing_Deployment_Type\",\"First_Active_Sharing_Arrangement_Start_Date\",\\\n",
        "             \"First_Active_Sharing_Arrangement_End_Date\"]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szr7BJa9rbpc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "cd00e80e-9428-4edf-e5b2-d629843b3e3f"
      },
      "source": [
        "path_towerdb = '/content/TowerDB_Germany_20210731.csv'\n",
        "#teste = '/content/TowerDB_Germany_20210731 - Copia.csv'\n",
        "towerdb = pd.read_csv(path_towerdb,encoding='latin')\n",
        "towerdb.columns = lower_str(list(towerdb.columns))\n",
        "\n",
        "dates_tw = ['first_active_sharing_arrangement_start_date','first_active_sharing_arrangement_end_date', 'date_of_equipment_removal']\n",
        "for i in dates_tw:\n",
        "    towerdb[i] = [date_obj.strftime(\"%d/%m/%Y\") if not pd.isnull(date_obj) else '' for date_obj in towerdb[i]]\n",
        "\n",
        "ints = ['security class', 'postal code']\n",
        "for i in ints:\n",
        "    towerdb[i] = towerdb[i].fillna(0)\n",
        "    towerdb[i] = list(map(int, towerdb[i]))\n",
        "\n",
        "towerdb = towerdb.replace(['N/A', 'n/a',\"0\", '-', '_',' ', np.nan,'nan'], '')\n",
        "towerdb = towerdb.fillna('')\n",
        "\n",
        "\n",
        "\"\"\"Read MSA File\"\"\"\n",
        "path_old = '/content/TowerDB_Germany_20210630.csv'\n",
        "msa = pd.read_csv(path_old, encoding='latin1')\n",
        "msa_cols = lower_str(list(msa.columns))\n",
        "msa.columns = msa_cols\n",
        "\n",
        "towerdb = towerdb[msa_cols]\n",
        "\n",
        "towerdb.head(3)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>site name</th>\n",
              "      <th>macro region</th>\n",
              "      <th>region</th>\n",
              "      <th>province</th>\n",
              "      <th>municipality</th>\n",
              "      <th>inhabitants</th>\n",
              "      <th>address</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>categorization by transmission sys</th>\n",
              "      <th>categorization by transmission sys (sub-cluster)</th>\n",
              "      <th>categorization by site type</th>\n",
              "      <th>categorization by indoor / outdoor site (antennas)</th>\n",
              "      <th>additional site type information</th>\n",
              "      <th>categorization by inhabitants</th>\n",
              "      <th>rural/ suburban/ urban</th>\n",
              "      <th>categorization by connectivity</th>\n",
              "      <th>technology vod</th>\n",
              "      <th>pod id</th>\n",
              "      <th>energy consumption current fy</th>\n",
              "      <th>actual energy cost current fy</th>\n",
              "      <th>infrastructure ready (existing)/ to be ready (new)</th>\n",
              "      <th>infrastructure to be shared by</th>\n",
              "      <th>counterpart</th>\n",
              "      <th># of lease contracts</th>\n",
              "      <th>current annual lease fees</th>\n",
              "      <th>current energy lease fees</th>\n",
              "      <th>current annual other fees</th>\n",
              "      <th>total annual lease</th>\n",
              "      <th>(average) residual duration</th>\n",
              "      <th>maturity cluster</th>\n",
              "      <th>exco rep. avg annual lease costs</th>\n",
              "      <th>total energy cost currrent fy (energy provider + ll)</th>\n",
              "      <th>vod (y/n)</th>\n",
              "      <th>deutsche telekom</th>\n",
              "      <th>annual fee per tenant mno1</th>\n",
              "      <th>annual energy fee mno1</th>\n",
              "      <th>annual maintenance fee mno1</th>\n",
              "      <th>other services fee mno1</th>\n",
              "      <th>...</th>\n",
              "      <th>categorization by shared usage (contractual)</th>\n",
              "      <th>categorization by shared usage (pop count)</th>\n",
              "      <th>security class</th>\n",
              "      <th>categorization by sites with single/multiple tower(s)</th>\n",
              "      <th>categorization by special site/plc</th>\n",
              "      <th>categorization by chimney-location</th>\n",
              "      <th>categorization by antenna constraint</th>\n",
              "      <th>categorization by 5g roll-out site</th>\n",
              "      <th>categorization by authority site</th>\n",
              "      <th>categorization by transport-sammler</th>\n",
              "      <th>categorization by strategic site</th>\n",
              "      <th>categorization by dual use site</th>\n",
              "      <th>categorization by sublease right</th>\n",
              "      <th>gbt reserve space</th>\n",
              "      <th>state</th>\n",
              "      <th>postal code</th>\n",
              "      <th>tower height</th>\n",
              "      <th>floor space</th>\n",
              "      <th>ground register .1</th>\n",
              "      <th>ground register .2</th>\n",
              "      <th>ground register .3</th>\n",
              "      <th>categorization by critical site</th>\n",
              "      <th>site cluster</th>\n",
              "      <th>infrastructure to be dismantled by</th>\n",
              "      <th>decommissioned site</th>\n",
              "      <th>strategic site bucket</th>\n",
              "      <th>bts_sites</th>\n",
              "      <th>criticalsite_beyond_10</th>\n",
              "      <th>sites as metered estimated</th>\n",
              "      <th>active sharing arrangement</th>\n",
              "      <th>subsequent_active_sharing_arrangement</th>\n",
              "      <th>first_active_sharing_deployment_type</th>\n",
              "      <th>first_active_sharing_arrangement_start_date</th>\n",
              "      <th>first_active_sharing_arrangement_end_date</th>\n",
              "      <th>indoor_site_any_climate_control</th>\n",
              "      <th>outdoor_site_with_dc_power</th>\n",
              "      <th>rfai_date</th>\n",
              "      <th>cluster for non-enterprise das sites</th>\n",
              "      <th>site status (tims)</th>\n",
              "      <th>date_of_equipment_removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001 B</td>\n",
              "      <td>Neustadt</td>\n",
              "      <td>East</td>\n",
              "      <td>B</td>\n",
              "      <td></td>\n",
              "      <td>Berlin</td>\n",
              "      <td>3550948</td>\n",
              "      <td>Kandeler Weg 1</td>\n",
              "      <td>52,5419278</td>\n",
              "      <td>13,1852444</td>\n",
              "      <td>Macro</td>\n",
              "      <td>Standard</td>\n",
              "      <td>RTT</td>\n",
              "      <td>Outdoor</td>\n",
              "      <td></td>\n",
              "      <td>Urban</td>\n",
              "      <td>URBAN</td>\n",
              "      <td>Microwave</td>\n",
              "      <td>2G</td>\n",
              "      <td>0001 B</td>\n",
              "      <td>6820.729000</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8980.7825</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>9.583333</td>\n",
              "      <td>&lt;= 10 years</td>\n",
              "      <td>8980.7825</td>\n",
              "      <td>1516.903597</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>Stand-alone</td>\n",
              "      <td>Co-located</td>\n",
              "      <td>1</td>\n",
              "      <td>Single Tower</td>\n",
              "      <td>Standard Site</td>\n",
              "      <td>Other</td>\n",
              "      <td></td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>No</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td></td>\n",
              "      <td>Berlin</td>\n",
              "      <td>13583</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Spandau</td>\n",
              "      <td>68/4</td>\n",
              "      <td>5/6</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Non Strategic</td>\n",
              "      <td>No</td>\n",
              "      <td>Non Critical</td>\n",
              "      <td>Estimated Model</td>\n",
              "      <td>No Active Sharing</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>AC</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>In Service</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0001 D</td>\n",
              "      <td>D-Innenstadt</td>\n",
              "      <td>West</td>\n",
              "      <td>D</td>\n",
              "      <td></td>\n",
              "      <td>Düsseldorf</td>\n",
              "      <td>611356</td>\n",
              "      <td>Oststr. 86</td>\n",
              "      <td>51,22286</td>\n",
              "      <td>6,78708</td>\n",
              "      <td>Macro</td>\n",
              "      <td>Standard</td>\n",
              "      <td>RTT</td>\n",
              "      <td>Outdoor</td>\n",
              "      <td></td>\n",
              "      <td>Urban</td>\n",
              "      <td>URBAN</td>\n",
              "      <td>Microwave</td>\n",
              "      <td>2G</td>\n",
              "      <td>0001 D</td>\n",
              "      <td>6800.803807</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5510.0000</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>5.583333</td>\n",
              "      <td>&lt;= 10 years</td>\n",
              "      <td>5510.0000</td>\n",
              "      <td>1784.930952</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>Stand-alone</td>\n",
              "      <td>Stand-alone</td>\n",
              "      <td>1</td>\n",
              "      <td>Single Tower</td>\n",
              "      <td>Standard Site</td>\n",
              "      <td>Other</td>\n",
              "      <td></td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>Yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td></td>\n",
              "      <td>North Rhine-Westphalia</td>\n",
              "      <td>40210</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Düsseldorf</td>\n",
              "      <td>357,358</td>\n",
              "      <td>4</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Yes - 0-5%</td>\n",
              "      <td>No</td>\n",
              "      <td>Non Critical</td>\n",
              "      <td>Estimated Model</td>\n",
              "      <td>No Active Sharing</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>AC</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>In Service</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0001 M</td>\n",
              "      <td>M-Innenstadt</td>\n",
              "      <td>South</td>\n",
              "      <td>M</td>\n",
              "      <td></td>\n",
              "      <td>München</td>\n",
              "      <td>1452824</td>\n",
              "      <td>Arnulfstr. 2</td>\n",
              "      <td>48,14165278</td>\n",
              "      <td>11,56010278</td>\n",
              "      <td>Transmission</td>\n",
              "      <td>Transmission</td>\n",
              "      <td>RTT</td>\n",
              "      <td>Indoor</td>\n",
              "      <td></td>\n",
              "      <td>Urban</td>\n",
              "      <td>URBAN</td>\n",
              "      <td>Fibre</td>\n",
              "      <td>Transmission only</td>\n",
              "      <td>0001 M</td>\n",
              "      <td>110204.480000</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>85000.0000</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>7.166667</td>\n",
              "      <td>&lt;= 10 years</td>\n",
              "      <td>85000.0000</td>\n",
              "      <td>16377.251800</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>Shared</td>\n",
              "      <td>Co-located</td>\n",
              "      <td>4</td>\n",
              "      <td>Single Tower</td>\n",
              "      <td>Standard Site</td>\n",
              "      <td>Other</td>\n",
              "      <td></td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>No</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td></td>\n",
              "      <td>Bavaria</td>\n",
              "      <td>80335</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Max-Vorstadt</td>\n",
              "      <td>6781</td>\n",
              "      <td></td>\n",
              "      <td>Yes</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Non Strategic</td>\n",
              "      <td>No</td>\n",
              "      <td>Within 10%</td>\n",
              "      <td>Estimated Model</td>\n",
              "      <td>No Active Sharing</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Yes; Indoor Air Conditioning</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>In Service</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 126 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     code     site name  ... site status (tims) date_of_equipment_removal\n",
              "0  0001 B      Neustadt  ...         In Service                          \n",
              "1  0001 D  D-Innenstadt  ...         In Service                          \n",
              "2  0001 M  M-Innenstadt  ...         In Service                          \n",
              "\n",
              "[3 rows x 126 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bCowhuYNcQ5"
      },
      "source": [
        "\"\"\"First Step Check All columns received\"\"\"\n",
        "#check billing columns\n",
        "df_col_missing = check_columns_received(towerdb, msa_cols)\n",
        "df_col_missing\n",
        "#tem error Falta Columns de Billing trigger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "5p5ORYnwFbC2",
        "outputId": "2771d040-8197-46bc-d83f-fbf6059cc5c1"
      },
      "source": [
        "towerdb.head(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>site name</th>\n",
              "      <th>macro region</th>\n",
              "      <th>region</th>\n",
              "      <th>province</th>\n",
              "      <th>municipality</th>\n",
              "      <th>inhabitants</th>\n",
              "      <th>address</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>categorization by transmission sys</th>\n",
              "      <th>categorization by transmission sys (sub-cluster)</th>\n",
              "      <th>categorization by site type</th>\n",
              "      <th>categorization by indoor / outdoor site (antennas)</th>\n",
              "      <th>additional site type information</th>\n",
              "      <th>categorization by inhabitants</th>\n",
              "      <th>rural/ suburban/ urban</th>\n",
              "      <th>categorization by connectivity</th>\n",
              "      <th>technology vod</th>\n",
              "      <th>pod id</th>\n",
              "      <th>energy consumption current fy</th>\n",
              "      <th>actual energy cost current fy</th>\n",
              "      <th>infrastructure ready (existing)/ to be ready (new)</th>\n",
              "      <th>infrastructure to be shared by</th>\n",
              "      <th>counterpart</th>\n",
              "      <th># of lease contracts</th>\n",
              "      <th>current annual lease fees</th>\n",
              "      <th>current energy lease fees</th>\n",
              "      <th>current annual other fees</th>\n",
              "      <th>total annual lease</th>\n",
              "      <th>(average) residual duration</th>\n",
              "      <th>maturity cluster</th>\n",
              "      <th>exco rep. avg annual lease costs</th>\n",
              "      <th>total energy cost currrent fy (energy provider + ll)</th>\n",
              "      <th>vod (y/n)</th>\n",
              "      <th>deutsche telekom</th>\n",
              "      <th>annual fee per tenant mno1</th>\n",
              "      <th>annual energy fee mno1</th>\n",
              "      <th>annual maintenance fee mno1</th>\n",
              "      <th>other services fee mno1</th>\n",
              "      <th>...</th>\n",
              "      <th>categorization by shared usage (contractual)</th>\n",
              "      <th>categorization by shared usage (pop count)</th>\n",
              "      <th>security class</th>\n",
              "      <th>categorization by sites with single/multiple tower(s)</th>\n",
              "      <th>categorization by special site/plc</th>\n",
              "      <th>categorization by chimney-location</th>\n",
              "      <th>categorization by antenna constraint</th>\n",
              "      <th>categorization by 5g roll-out site</th>\n",
              "      <th>categorization by authority site</th>\n",
              "      <th>categorization by transport-sammler</th>\n",
              "      <th>categorization by strategic site</th>\n",
              "      <th>categorization by dual use site</th>\n",
              "      <th>categorization by sublease right</th>\n",
              "      <th>gbt reserve space</th>\n",
              "      <th>state</th>\n",
              "      <th>postal code</th>\n",
              "      <th>tower height</th>\n",
              "      <th>floor space</th>\n",
              "      <th>ground register .1</th>\n",
              "      <th>ground register .2</th>\n",
              "      <th>ground register .3</th>\n",
              "      <th>categorization by critical site</th>\n",
              "      <th>site cluster</th>\n",
              "      <th>infrastructure to be dismantled by</th>\n",
              "      <th>decommissioned site</th>\n",
              "      <th>strategic site bucket</th>\n",
              "      <th>bts_sites</th>\n",
              "      <th>criticalsite_beyond_10</th>\n",
              "      <th>sites as metered estimated</th>\n",
              "      <th>active sharing arrangement</th>\n",
              "      <th>subsequent_active_sharing_arrangement</th>\n",
              "      <th>first_active_sharing_deployment_type</th>\n",
              "      <th>first_active_sharing_arrangement_start_date</th>\n",
              "      <th>first_active_sharing_arrangement_end_date</th>\n",
              "      <th>indoor_site_any_climate_control</th>\n",
              "      <th>outdoor_site_with_dc_power</th>\n",
              "      <th>rfai_date</th>\n",
              "      <th>cluster for non-enterprise das sites</th>\n",
              "      <th>site status (tims)</th>\n",
              "      <th>date_of_equipment_removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001 B</td>\n",
              "      <td>Neustadt</td>\n",
              "      <td>East</td>\n",
              "      <td>B</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>3550948</td>\n",
              "      <td>Kandeler Weg 1</td>\n",
              "      <td>52,5419278</td>\n",
              "      <td>13,1852444</td>\n",
              "      <td>Macro</td>\n",
              "      <td>Standard</td>\n",
              "      <td>RTT</td>\n",
              "      <td>Outdoor</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Urban</td>\n",
              "      <td>URBAN</td>\n",
              "      <td>Microwave</td>\n",
              "      <td>2G</td>\n",
              "      <td>0001 B</td>\n",
              "      <td>6820.729000</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8980.7825</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9.583333</td>\n",
              "      <td>&lt;= 10 years</td>\n",
              "      <td>8980.7825</td>\n",
              "      <td>1516.903597</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Stand-alone</td>\n",
              "      <td>Co-located</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Single Tower</td>\n",
              "      <td>Standard Site</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>No</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>13583.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Spandau</td>\n",
              "      <td>68/4</td>\n",
              "      <td>5/6</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Non Strategic</td>\n",
              "      <td>No</td>\n",
              "      <td>Non Critical</td>\n",
              "      <td>Estimated Model</td>\n",
              "      <td>No Active Sharing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In Service</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0001 D</td>\n",
              "      <td>D-Innenstadt</td>\n",
              "      <td>West</td>\n",
              "      <td>D</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Düsseldorf</td>\n",
              "      <td>611356</td>\n",
              "      <td>Oststr. 86</td>\n",
              "      <td>51,22286</td>\n",
              "      <td>6,78708</td>\n",
              "      <td>Macro</td>\n",
              "      <td>Standard</td>\n",
              "      <td>RTT</td>\n",
              "      <td>Outdoor</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Urban</td>\n",
              "      <td>URBAN</td>\n",
              "      <td>Microwave</td>\n",
              "      <td>2G</td>\n",
              "      <td>0001 D</td>\n",
              "      <td>6800.803807</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5510.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5.583333</td>\n",
              "      <td>&lt;= 10 years</td>\n",
              "      <td>5510.0000</td>\n",
              "      <td>1784.930952</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Stand-alone</td>\n",
              "      <td>Stand-alone</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Single Tower</td>\n",
              "      <td>Standard Site</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>Yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>North Rhine-Westphalia</td>\n",
              "      <td>40210.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Düsseldorf</td>\n",
              "      <td>357,358</td>\n",
              "      <td>4</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes - 0-5%</td>\n",
              "      <td>No</td>\n",
              "      <td>Non Critical</td>\n",
              "      <td>Estimated Model</td>\n",
              "      <td>No Active Sharing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In Service</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0001 M</td>\n",
              "      <td>M-Innenstadt</td>\n",
              "      <td>South</td>\n",
              "      <td>M</td>\n",
              "      <td>NaN</td>\n",
              "      <td>München</td>\n",
              "      <td>1452824</td>\n",
              "      <td>Arnulfstr. 2</td>\n",
              "      <td>48,14165278</td>\n",
              "      <td>11,56010278</td>\n",
              "      <td>Transmission</td>\n",
              "      <td>Transmission</td>\n",
              "      <td>RTT</td>\n",
              "      <td>Indoor</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Urban</td>\n",
              "      <td>URBAN</td>\n",
              "      <td>Fibre</td>\n",
              "      <td>Transmission only</td>\n",
              "      <td>0001 M</td>\n",
              "      <td>110204.480000</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>85000.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>7.166667</td>\n",
              "      <td>&lt;= 10 years</td>\n",
              "      <td>85000.0000</td>\n",
              "      <td>16377.251800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Shared</td>\n",
              "      <td>Co-located</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Single Tower</td>\n",
              "      <td>Standard Site</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>No</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bavaria</td>\n",
              "      <td>80335.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Max-Vorstadt</td>\n",
              "      <td>6781</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Non Strategic</td>\n",
              "      <td>No</td>\n",
              "      <td>Within 10%</td>\n",
              "      <td>Estimated Model</td>\n",
              "      <td>No Active Sharing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes; Indoor Air Conditioning</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In Service</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 126 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     code     site name  ... site status (tims) date_of_equipment_removal\n",
              "0  0001 B      Neustadt  ...         In Service                       NaN\n",
              "1  0001 D  D-Innenstadt  ...         In Service                       NaN\n",
              "2  0001 M  M-Innenstadt  ...         In Service                       NaN\n",
              "\n",
              "[3 rows x 126 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTPDPq5iN_Sf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl8AHhvsE5mX"
      },
      "source": [
        "\n",
        "sites = [i for i in towerdb[towerdb['climate control (yes/no)']=='Yes; Indoor Air Conditioning and Free Air cooling / Free cooling units']['site code']]\n",
        "towerdb.loc[towerdb['site code'].isin(sites), 'climate control (yes/no)'] = 'Yes; Indoor Air Conditioning'\n",
        "\n",
        "sites = [i for i in towerdb[towerdb['climate control (yes/no)']=='Yes; Indoor Free Air cooling / Free cooling units']['site code']]\n",
        "towerdb.loc[towerdb['site code'].isin(sites), 'climate control (yes/no)'] = 'Yes; Indoor Air Conditioning'\n",
        "\n",
        "towerdb = towerdb.fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OScOV9p_9lPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7fb658-b802-4422-f636-b553f773a437"
      },
      "source": [
        "\"\"\"Second Check Date Format dd-mm-YYYY\"\"\"\n",
        "def check_date_columns(df, df_index,status_col,columns, format):\n",
        "    \"\"\"\n",
        "    Paramns \\n\n",
        "        Dates needs to be in string format not in datetime, otherwise raise an error.\\n\n",
        "        Convert entire column to string before.\n",
        "    \"\"\"\n",
        "    df_dates = df[columns]\n",
        "    df_dates['sites'] = df_dates[df_index]\n",
        "    df_dates = df_dates.set_index('sites')\n",
        "\n",
        "    df_de = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    df_duplicates = count_duplicates(df_dates[df_index])\n",
        "    if not df_duplicates.empty:\n",
        "        df_dates.drop_duplicates(subset=[df_index], inplace=True)\n",
        "\n",
        "    filtered = df[[df_index, status_col]]\n",
        "\n",
        "    if format == 1:\n",
        "        date_format = re.compile(r'\\d{2}\\-\\d{2}\\-\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                #print(type(df_dates.loc[de_site,de_column]))\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "    else:\n",
        "        date_format = re.compile(r'\\d{2}\\/\\d{2}\\/\\d{4}')\n",
        "        for de_site in df_dates[df_index]:\n",
        "            for de_column in columns[1:]:\n",
        "                #match = re.search(r'\\d{2}\\/\\d{2}\\/\\d{4}', df_dates.loc[ta_site,ta_column])\n",
        "                value = df_dates.loc[de_site,de_column]\n",
        "                if date_format.match(value) == None:\n",
        "                    if df_dates.loc[de_site,df_index] not in df_de.index:\n",
        "                        df_de.loc[de_site,df_index] = df_dates.loc[de_site,df_index]\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                    else:\n",
        "                        if pd.isnull(value) or pd.isna(value) or value == 'nan' or value=='':\n",
        "                            df_de.loc[de_site,de_column] = 'Blank Value'\n",
        "                        else:\n",
        "                            df_de.loc[de_site,de_column] = f'Incorret picklist value: {value}'\n",
        "                            \n",
        "        df_de = df_de.dropna(how='all', axis=1).fillna('Ok')       \n",
        "        if not df_de.empty:\n",
        "            df_de.reset_index()\n",
        "            df_de = pd.merge(df_de, filtered, how='left', on=df_index)\n",
        "            df_de = df_de[[df_index, status_col, *columns[1:]]]\n",
        "            df_de = df_de[~(df_de[status_col]=='In Service')]\n",
        "            return df_de\n",
        "        else: \n",
        "            print('\\nNo one columns with incorrect date format!\\n')\n",
        "\n",
        "dates_cols = ['Code', 'Date_Of_Equipment_Removal']\n",
        "dates_cols = lower_str(dates_cols)\n",
        "#Check dates of column 'Date_Of_Equipment_Removal' Pg 7\n",
        "#check_date_columns(df, df_index,status_col,columns, format)\n",
        "dftw_dates_errors = check_date_columns(towerdb, tw_index, tw_status, dates_cols, 2) \n",
        "# Errors msg"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "9inxuWiWPw9r",
        "outputId": "1696d28e-ed98-4297-b699-538e179dfa74"
      },
      "source": [
        "dftw_dates_errors"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>site status (tims)</th>\n",
              "      <th>date_of_equipment_removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [code, site status (tims), date_of_equipment_removal]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi5spCPz9iq2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "49840efd-5809-47ea-a217-1662becc170d"
      },
      "source": [
        "\"\"\"Check all picklist Columns\"\"\"\n",
        "def check_picklist_v1(df,df_index,df_status, df_cols, picklist_dict):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    df_picklist = df[df_cols].fillna('')\n",
        "    df_picklist = df_picklist.to_dict(orient='list')\n",
        "\n",
        "    new_dic = defaultdict(list)\n",
        "    \"\"\"for id in df_picklist[df_index]:\n",
        "        if id not in new_dic.keys():\n",
        "            new_dic[df_index].append(id)\"\"\" \n",
        "            \n",
        "    for column in set(picklist_dict.keys()):\n",
        "        for value in df_picklist[column]:\n",
        "            if value not in set(picklist_dict[column]): \n",
        "                if pd.isnull(value) or pd.isna(value) or value=='' or value=='nan':\n",
        "                    new_dic[column].append('Blank Value')\n",
        "                else:\n",
        "                    new_dic[column].append(f'Incorrect value: {value}')\n",
        "            else:\n",
        "                new_dic[column].append('Ok!')\n",
        "\n",
        "    df_errors = pd.DataFrame(new_dic)\n",
        "    df_errors = df_errors.replace('Ok!', np.nan)\n",
        "    df_errors = df_errors.dropna(how='all', axis=1)\n",
        "    \n",
        "    if not df_errors.empty:\n",
        "        #df = df[[df_index, df_status]]\n",
        "        #df_errors = pd.merge(df_errors,df, how='left', on=[df_index])\n",
        "        df_errors[df_index] = df_picklist[df_index]\n",
        "        df_errors[df_status] = df_picklist[df_status]\n",
        "        df_errors = df_errors.set_index(df_index)\n",
        "        df_errors = df_errors[[df_status]+ df_errors.columns[:-1].tolist()]\n",
        "        df_errors = df_errors.reset_index()\n",
        "        return df_errors\n",
        "    else:\n",
        "        print('\\nNo one Picklist Error Founded!\\n')\n",
        "\n",
        "picklist_col = [\"Code\",'site status (tims)','Categorization by Transmission Sys','Categorization by Transmission Sys (sub-cluster)',\\\n",
        "            'Categorization by Site Type', 'Categorization by Indoor / Outdoor Site (Antennas)',\\\n",
        "            'Categorization by Strategic Site','Categorization by Critical Site', 'Strategic Site Bucket', 'Bts_sites',\\\n",
        "            'CriticalSite_Beyond_10','decommissioned site', 'Sites As Metered Estimated',\\\n",
        "            'Active Sharing Arrangement', 'Indoor_Site_Any_Climate_Control', 'Outdoor_Site_With_Dc_Power']\n",
        "picklist_col = lower_str(picklist_col)\n",
        "picklist = {\n",
        "    'categorization by transmission sys': ['Macro','Public DAS Sites','Repeater Sites','Transmission', 'Long-Term Mobile Sites', 'Non-Enterprise DAS'],\n",
        "    'categorization by transmission sys (sub-cluster)': ['Core', 'DAS', 'Long-term Mobile', 'Non-Enterprise DAS', 'Repeater', 'Standard', 'Transmission'],\n",
        "    'categorization by site type':['DAS','GBT','RTT', 'Non-Enterprise DAS'],\n",
        "    'categorization by indoor / outdoor site (antennas)': ['Outdoor', 'Indoor'],\n",
        "    'categorization by strategic site':['Yes','No'],\n",
        "    'categorization by critical site':['Yes','No'],\n",
        "    'strategic site bucket': ['Yes - 0-5%','Non Strategic'],\n",
        "    'bts_sites': ['Yes','No'],\n",
        "    'criticalsite_beyond_10': ['Beyond 10%','Within 10%','Non Critical'],\n",
        "    'decommissioned site': ['Yes', 'No'],\n",
        "    'sites as metered estimated': ['Estimated Model','Metered Model'],\n",
        "    'active sharing arrangement': ['No Active Sharing'],\n",
        "    'indoor_site_any_climate_control': ['No','Yes; Indoor Air Conditioning','Yes; Indoor Free Air cooling / Free cooling units'],\n",
        "    'outdoor_site_with_dc_power':['AC','DC','AC & DC','No Power']}\n",
        "actives = towerdb[towerdb[tw_status]=='In Service']\n",
        "df_picklist_errors = check_picklist_v1(actives,'code','site status (tims)',picklist_col, picklist)\n",
        "df_picklist_errors"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>site status (tims)</th>\n",
              "      <th>indoor_site_any_climate_control</th>\n",
              "      <th>outdoor_site_with_dc_power</th>\n",
              "      <th>decommissioned site</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001 B</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0001 D</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0001 M</td>\n",
              "      <td>In Service</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001 O</td>\n",
              "      <td>In Service</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001 S</td>\n",
              "      <td>In Service</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19747</th>\n",
              "      <td>Y155 M</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19748</th>\n",
              "      <td>Y156 M</td>\n",
              "      <td>In Service</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19749</th>\n",
              "      <td>Y157 M</td>\n",
              "      <td>In Service</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19750</th>\n",
              "      <td>Y158 M</td>\n",
              "      <td>In Service</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19751</th>\n",
              "      <td>Y159 M</td>\n",
              "      <td>In Service</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Blank Value</td>\n",
              "      <td>Blank Value</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19752 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         code site status (tims)  ... outdoor_site_with_dc_power decommissioned site\n",
              "0      0001 B         In Service  ...                        NaN         Blank Value\n",
              "1      0001 D         In Service  ...                        NaN         Blank Value\n",
              "2      0001 M         In Service  ...                Blank Value         Blank Value\n",
              "3      0001 O         In Service  ...                Blank Value         Blank Value\n",
              "4      0001 S         In Service  ...                Blank Value         Blank Value\n",
              "...       ...                ...  ...                        ...                 ...\n",
              "19747  Y155 M         In Service  ...                        NaN         Blank Value\n",
              "19748  Y156 M         In Service  ...                Blank Value         Blank Value\n",
              "19749  Y157 M         In Service  ...                Blank Value         Blank Value\n",
              "19750  Y158 M         In Service  ...                Blank Value         Blank Value\n",
              "19751  Y159 M         In Service  ...                Blank Value         Blank Value\n",
              "\n",
              "[19752 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PQGxAyyozaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c68e2e-9442-46d3-d8c0-da2a0a82ba77"
      },
      "source": [
        "actives['decommissioned site'].value_counts()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    19752\n",
              "Name: decommissioned site, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxmlKKCF9VE-"
      },
      "source": [
        "#Read UIP File\n",
        "uip_names = ['Site_ID', 'BTS site applicable charge (Annual)', 'Commercials for sites beyond 10% cap of critical sites (Annual)']\n",
        "uip = pd.read_excel('/content/UserInput_Germany_20210731_InMonth.xlsx',sheet_name='SiteLevel',usecols=[0,1,2],skiprows=2)\n",
        "uip.columns = uip_names\n",
        "\n",
        "msa_sites = [str(i) for i in msa[msa[msa_status]=='In Service']['code']]\n",
        "tw_sites = [str(i) for i in towerdb[towerdb[tw_status]=='In Service']['code']]\n",
        "uip_sites = [str(i) for i in uip['Site_ID']]\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5N27RTRXtrO",
        "outputId": "7ed14e41-7199-40f6-bd79-a3d84199b67d"
      },
      "source": [
        "print([i for i in tw_sites if i not in uip_sites])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rftVRqlT9TDW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "cbe1197a-986d-4232-abc8-01b9cb9987fa"
      },
      "source": [
        "\"\"\"Eigth Check for BTS sites\"\"\"\n",
        "def check_bts(df_tw, bts_tw_columns, tw_index, status_col, df_msa, bts_msa_column, msa_index):\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_bts_check(bts_msa, tw_bts_sites):\n",
        "        bts_out_tw=[]\n",
        "        if sorted(bts_msa) != sorted(tw_bts_sites):\n",
        "            for i in tw_bts_sites:\n",
        "                if i not in bts_msa:\n",
        "                    bts_out_tw.append(i)\n",
        "\n",
        "        return bts_out_tw\n",
        "\n",
        "    bts_msa = msa[msa[bts_msa_column]=='Yes']\n",
        "    bts_msa = [str(i) for i in bts_msa[msa_index]]\n",
        "\n",
        "    tw_bts_sites = df_tw[df_tw[bts_tw_columns]=='Yes']\n",
        "    tw_bts_sites = [str(i) for i in tw_bts_sites[tw_index]]\n",
        "\n",
        "    #return of datas\n",
        "    filtered = df_tw[[tw_index, status_col, bts_tw_columns]]\n",
        "    bts_out_tw = cond_bts_check(bts_msa, tw_bts_sites)\n",
        "    df = pd.DataFrame(bts_out_tw, columns=['New Sites'])\n",
        "    if not df.empty:\n",
        "        df = pd.merge(df, filtered, how='left', left_on=['New Sites'], right_on=tw_index)\n",
        "        df = df[['New Sites', status_col, bts_tw_columns]]\n",
        "        return df\n",
        "    else: \n",
        "        print('\\nNo errors founded!\\n')\n",
        "\n",
        "# Check for BTS sites\n",
        "df_bts_out_tw = check_bts(towerdb,tw_bts,tw_index,tw_status,msa,msa_bts,msa_index)\n",
        "df_bts_out_tw"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>New Sites</th>\n",
              "      <th>site status (tims)</th>\n",
              "      <th>bts_sites</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0861 W</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3207 O</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5331 B</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5866 O</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6214 S</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6282 O</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7963 H</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>B194 M</td>\n",
              "      <td>In Service</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  New Sites site status (tims) bts_sites\n",
              "0    0861 W         In Service       Yes\n",
              "1    3207 O         In Service       Yes\n",
              "2    5331 B         In Service       Yes\n",
              "3    5866 O         In Service       Yes\n",
              "4    6214 S         In Service       Yes\n",
              "5    6282 O         In Service       Yes\n",
              "6    7963 H         In Service       Yes\n",
              "7    B194 M         In Service       Yes"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA7zKEfBY2fS"
      },
      "source": [
        "msa.columns.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID604E24XA4J"
      },
      "source": [
        "\"\"\"Falta column billing trigger(Rfai_Date) p fazer o check\"\"\"\n",
        "def check_wip(df_tw,tw_index,tw_status, tw_wip, tw_bts, df_msa, msa_index, wip_msa_col):\n",
        "\n",
        "    #Nested Function to make conditional validations\n",
        "    def cond_wip_check(wip_msa, tw_wip_sites):\n",
        "        count_wip = 0\n",
        "        wip_out_tw=[]\n",
        "        if sorted(wip_msa) != sorted(tw_wip_sites):\n",
        "            for i in tw_wip_sites:\n",
        "                if i not in wip_msa:\n",
        "                    count_wip += 1\n",
        "                    wip_out_tw.append(i)\n",
        "\n",
        "        return wip_out_tw\n",
        "\n",
        "    wip_msa = df_msa[df_msa[wip_msa_col]=='Yes']\n",
        "    wip_msa = [str(i) for i in wip_msa[msa_index]]\n",
        "\n",
        "    tw_wip_sites = df_tw[df_tw[tw_wip]=='Yes']\n",
        "    tw_wip_sites = [str(i) for i in tw_wip_sites[tw_index]]\n",
        "\n",
        "    tw_wip_site_bts_flagged = df_tw[(df_tw[tw_wip]=='Yes')&(df_tw[tw_bts]=='Yes')]\n",
        "    tw_wip_site_bts_flagged = tw_wip_site_bts_flagged[[tw_index,tw_status,tw_wip, tw_bts]]\n",
        "\n",
        "    wip_out_tw_list = cond_wip_check(wip_msa, tw_wip_sites)\n",
        "    if not(len(wip_out_tw_list)==0 or tw_wip_site_bts_flagged.empty):\n",
        "        return wip_out_tw_list, tw_wip_site_bts_flagged\n",
        "\n",
        "wip_out_tw_list, df_wip_and_bts_flagged = check_wip(towerdb,tw_index, tw_status, tw_wip,tw_bts, msa, msa_index, msa_wip)\n",
        "# No Errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-AVS1fSZuu1",
        "outputId": "1e8a1718-8abc-4462-c7ca-0c658b65c6b9"
      },
      "source": [
        "sites = [i for i in towerdb[towerdb['decommissioned site']=='Yes'][tw_index]]\n",
        "if len(sites)>0:   \n",
        "    towerdb[towerdb[tw_index].isin(sites)&(towerdb[tw_doer]==\"\")]\n",
        "else:\n",
        "    print('vazio')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vazio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZDpKiiU9Ryi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221c4e20-eb0d-49f1-a693-976af16ace41"
      },
      "source": [
        "\"\"\"Nineth - Check fopr decomissioned sites\"\"\"\n",
        "def check_decommissioned(df,df_index,status_col, decom_col, doer_col):\n",
        "    #c = country.lower()\n",
        "    sites = [i for i in df[df[decom_col]=='Yes'][df_index]]\n",
        "    if len(sites)>0:\n",
        "        filtered = df[(df[df_index].isin(sites))&(df[doer_col]==\"\")]\n",
        "        return filtered[[df_index,status_col, decom_col, doer_col]]\n",
        "    else:\n",
        "        print('\\nNo errors Founded!\\n')\n",
        "#Validate Decomissioned Sites\n",
        "df_decom_errors = check_decommissioned(towerdb,tw_index, tw_status, 'decommissioned site', tw_doer)\n",
        "df_decom_errors\n",
        "#None errors"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Satoh09PTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2109cae9-6547-4a8c-a1f0-e20b074f4e1a"
      },
      "source": [
        "\"\"\"Tenth - Check Date Of Equipement Removed date for in service sites\"\"\"\n",
        "def check_tw_bill_doer(df_tw, tw_index, date_col, status_col, status, type_col):\n",
        "    \n",
        "    t = type_col.lower()\n",
        "    # capture current date as string\n",
        "    current_date = pd.to_datetime('now').date()\n",
        "    # convert current to timestamp\n",
        "    current_date = pd.to_datetime(current_date)\n",
        "    df_tw = df_tw[df_tw[status_col]==status][[tw_index, status_col, date_col]] \n",
        "    dates = [i for i in df_tw[date_col] if i!= '']\n",
        "\n",
        "    if len(dates)>0:\n",
        "        if t == 'doer':\n",
        "            filtered = df_tw[df_tw[date_col].astype('datetime64[ns]') < current_date]\n",
        "            return filtered\n",
        "        else:\n",
        "            #bill_dates = pd.to_datetime(df_tw[date_col], errors==coerrce)\n",
        "            filtered = df_tw[df_tw[date_col]=='']\n",
        "            return filtered\n",
        "    else:\n",
        "        print('\\nNo errors founded!\\n')\n",
        "\n",
        "df_doer_errors = check_tw_bill_doer(towerdb, tw_index, tw_doer , tw_status,'In Service', 'doer')\n",
        "df_doer_errors\n",
        "# None Errors"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "\n",
            "No errors founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbCsf3dk9OOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33867e9d-7f78-4dd8-baf4-f7258011b9aa"
      },
      "source": [
        "\"\"\"Eleventh - Check Month-on-Month towerdb sites\"\"\"\n",
        "def check_mom_bts(df_tw, tw_index,status_col,df_bts, df_msa, msa_index, msa_col):\n",
        "\n",
        "    #c = country   \n",
        "    msa_bts = df_msa[df_msa[msa_col]=='Yes']\n",
        "    msa_bts_sites = [i for i in msa_bts[msa_index]]\n",
        "\n",
        "    tw_bts = df_tw[df_tw[df_bts]=='Yes']\n",
        "    tw_bts_sites = [i for i in tw_bts[tw_index]]\n",
        "\n",
        "    out_tower_bts = [i for i in msa_bts_sites if i not in tw_bts_sites]\n",
        "    filtered = tw_bts[tw_bts[tw_index].isin(out_tower_bts)]\n",
        "    if not filtered.empty:\n",
        "        return filtered[[tw_index,status_col, df_bts]]    \n",
        "    else:\n",
        "        print('\\nNo Errors Founded!\\n')\n",
        "df_mom_errors = check_mom_bts(towerdb,tw_index, tw_status, tw_bts, msa,msa_index, msa_bts)\n",
        "# None Errors"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No Errors Founded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXqih0e49NDk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "outputId": "90b6f101-8e83-44a3-e636-598ba8ab9c95"
      },
      "source": [
        "\"\"\"Twelveth - Match UIS sites with Towerdb Sites \"\"\"\n",
        "def check_uip_tw(df_tw,tw_index, status_tw_col, tw_bts_col, tw_critical_col, df_uip, uip_sites):\n",
        "\n",
        "    filtered = df_tw[[tw_index, status_tw_col]]\n",
        "    #tw_active = df_tw[df_tw['Site Status']=='In Service']\n",
        "    count_tw_sites = [i for i in df_tw[df_tw[status_tw_col] =='In Service'][tw_index]]\n",
        "\n",
        "    # check number of sites that are in uip file and doesn't have in df_tw\n",
        "    #if not set(count_tw_sites).intersection(uip_sites):\n",
        "    uis_sites_not_in_towerdb = [i for i in uip_sites if i not in count_tw_sites]\n",
        "\n",
        "    uis_sites_not_in_towerdb = pd.DataFrame(uis_sites_not_in_towerdb,columns=['UIS In Month not active in TowerDB!'])\n",
        "    uis_sites_not_in_towerdb = pd.merge(uis_sites_not_in_towerdb, filtered, how='left', left_on='UIS In Month not active in TowerDB!',\\\n",
        "                                    right_on=tw_index)\n",
        "    uis_sites_not_in_towerdb = uis_sites_not_in_towerdb[['UIS In Month not active in TowerDB!', status_tw_col]]\n",
        "    \n",
        "    in_service_not_in_uis = [i for i in count_tw_sites if i not in uip_sites]\n",
        "    in_service_not_in_uis = pd.DataFrame(in_service_not_in_uis,columns=['TowerDB Sites out of UIS In Month!'])\n",
        "    in_service_not_in_uis = pd.merge(in_service_not_in_uis, filtered, how='left', left_on='TowerDB Sites out of UIS In Month!',\\\n",
        "                                    right_on=tw_index)\n",
        "    in_service_not_in_uis = in_service_not_in_uis[['TowerDB Sites out of UIS In Month!', status_tw_col]]\n",
        "    #check for decomissioned site not in uip files\n",
        "\n",
        "    #Check BTS sites\n",
        "        #Check BTS sites\n",
        "    bts_sites = [i for i in df_tw[df_tw[tw_bts_col]=='Yes'][tw_index]]\n",
        "    uip_bts = [i for i in df_uip[df_uip['BTS site applicable charge (Annual)']!=\"\"]['Site_ID']]                            \n",
        "    #if not set(bts_sites).intersection(uip_sites):\n",
        "    bts_sites_out_uip = [i for i in uip_bts if i not in bts_sites]\n",
        "    \n",
        "    bts_sites_out_uip = pd.DataFrame(bts_sites_out_uip, columns=['UIS BTS not in TowerDB(BTS)'])\n",
        "    bts_sites_out_uip = pd.merge(bts_sites_out_uip, filtered, how='left', left_on='UIS BTS not in TowerDB(BTS)',\\\n",
        "                                    right_on=tw_index)\n",
        "    bts_sites_out_uip = bts_sites_out_uip[['UIS BTS not in TowerDB(BTS)', status_tw_col]]\n",
        "\n",
        "    #if not (in_service_uip_sites.empty or decomiss_sites_in_uip.empty or bts_sites_out_uip.empty or critical.empty):\n",
        "    return uis_sites_not_in_towerdb, in_service_not_in_uis,  bts_sites_out_uip\n",
        "\n",
        "\n",
        "uis_sites_not_in_towerdb, in_service_not_in_uis,  bts_sites_out_uip = check_uip_tw(towerdb, tw_index, tw_status, tw_bts, \\\n",
        "                                                                                             tw_critical, uip, uip_sites)\n",
        "uis_sites_not_in_towerdb\n",
        "#None Errors"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UIS In Month not active in TowerDB!</th>\n",
              "      <th>site status (tims)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [UIS In Month not active in TowerDB!, site status (tims)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvYu4xZ9JAI"
      },
      "source": [
        "Commercial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpAIShm69IWX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "f4b719de-33c8-4c5b-8588-43c570c66186"
      },
      "source": [
        "act = '/content/UserInput_Germany_20210731_InMonth.xlsx'\n",
        "old = '/content/UserInput_Germany_20210630.xlsx'\n",
        "cols_ordered = ['Charge_Type','Sub_charge_Type', 'Param1','Param2', 'Data_Type','Input_Value_actual', 'Input_Value_before',\\\n",
        "                'Equal Values','Description/Instruction', 'Frequency of Update']\n",
        "merge_cols = ['Charge_Type','Sub_charge_Type', 'Param1','Param2', 'Data_Type', 'Description/Instruction', 'Frequency of Update']\n",
        "name = ['Charge_Type','Sub_charge_Type', 'Param1','Param2', 'Data_Type', 'Input_Value', 'Description/Instruction', 'Frequency of Update'] \n",
        "col = ['Input_Value']               \n",
        "def check_commercial(path_current, path_before, col_replace, col_names, merge_cols, col_order):\n",
        "    \n",
        "    def replace_values(df, columns, value=\"\"):\n",
        "        \"\"\"\n",
        "        Está voltando para float\n",
        "        \"\"\"\n",
        "        invalid_values = ['N/A', 'n/a',\"0\", '-', '_', np.nan,'nan']\n",
        "        #lista = []\n",
        "\n",
        "        df.fillna(0, inplace=True)\n",
        "        #df[column] = df[column].astype('int64')\n",
        "\n",
        "        for column in columns:\n",
        "            lista = []\n",
        "            for index in df[column]:\n",
        "                #print(f\"{column} -> {index}\")\n",
        "                if index in invalid_values:\n",
        "                    lista.append(value)\n",
        "                else:\n",
        "                    lista.append(index)\n",
        "            df[column] = lista\n",
        "\n",
        "        return df    \n",
        "    def check_uip_commercial_values(df_actual, df_before, merge_cols):\n",
        "        df_atual = uip_comercial_actual\n",
        "        df_ant = uip_comercial_before\n",
        "        df_cross = pd.merge(df_actual, df_before, on=merge_cols, \\\n",
        "                            how='left', suffixes=('_actual', '_before'), indicator='Exist')\n",
        "        df_cross['Exist'] = np.where(df_cross.Exist == 'both', 'Yes', 'No')\n",
        "        df_cross['Equal Values'] = df_cross['Exist']\n",
        "        \n",
        "        return df_cross\n",
        "    \n",
        "    # Check for commercial Values into current UIP File and compare with UIP File before\n",
        "    uip_comercial_actual = pd.read_excel(path_current,sheet_name='Commercial', names=col_names).fillna('')\n",
        "    #uip_comercial_actual = uip_comercial_actual[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_actual = replace_values(uip_comercial_actual, col_replace, value=0)\n",
        "\n",
        "    uip_comercial_before = pd.read_excel(path_before,sheet_name='Commercial', names=col_names ).fillna('')\n",
        "    #uip_comercial_before = uip_comercial_before[['Charge_Type', 'Data_Type', 'Input_Value']]\n",
        "    uip_comercial_before = replace_values(uip_comercial_before, col_replace, value=0)\n",
        "\n",
        "    df_commercial =  check_uip_commercial_values(uip_comercial_actual, uip_comercial_before, merge_cols).fillna('')\n",
        "\n",
        "    df_commercial = df_commercial.reindex(columns=col_order)\n",
        "    df_commercial_diffs = df_commercial[df_commercial['Equal Values']=='No']\n",
        "    if not df_commercial_diffs.empty:\n",
        "        return df_commercial_diffs\n",
        "    else:\n",
        "        print('\\nNo errors founded!')\n",
        "\n",
        "df_comm = check_commercial(act, old, col, name, merge_cols, cols_ordered)\n",
        "df_comm"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Charge_Type</th>\n",
              "      <th>Sub_charge_Type</th>\n",
              "      <th>Param1</th>\n",
              "      <th>Param2</th>\n",
              "      <th>Data_Type</th>\n",
              "      <th>Input_Value_actual</th>\n",
              "      <th>Input_Value_before</th>\n",
              "      <th>Equal Values</th>\n",
              "      <th>Description/Instruction</th>\n",
              "      <th>Frequency of Update</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>BASE SERVICE CHARGE</td>\n",
              "      <td>LEGACY NON-ENTERPRISE DAS SITES</td>\n",
              "      <td>CAT 1</td>\n",
              "      <td></td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>2020</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>- Annual value as per the MSA\\n- To be edited ...</td>\n",
              "      <td>Only in case of change in MSA/Side letter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>BASE SERVICE CHARGE</td>\n",
              "      <td>LEGACY NON-ENTERPRISE DAS SITES</td>\n",
              "      <td>CAT 2</td>\n",
              "      <td></td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>8080</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>- Annual value as per the MSA\\n- To be edited ...</td>\n",
              "      <td>Only in case of change in MSA/Side letter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>BASE SERVICE CHARGE</td>\n",
              "      <td>LEGACY NON-ENTERPRISE DAS SITES</td>\n",
              "      <td>CAT 3</td>\n",
              "      <td></td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>30300</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>- Annual value as per the MSA\\n- To be edited ...</td>\n",
              "      <td>Only in case of change in MSA/Side letter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>BASE SERVICE CHARGE</td>\n",
              "      <td>LEGACY NON-ENTERPRISE DAS SITES</td>\n",
              "      <td>CAT 4</td>\n",
              "      <td></td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>80800</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>- Annual value as per the MSA\\n- To be edited ...</td>\n",
              "      <td>Only in case of change in MSA/Side letter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>LEGACY NON-ENTERPRISE DAS SITE UPGRADE CHARGIN...</td>\n",
              "      <td>APPLICABLE PROJECT SERVICE CHARGE FOR SITE UPG...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>PERCENTAGE(NUMBER ONLY)</td>\n",
              "      <td>18</td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td>- Annual value as per the MSA\\n- To be edited ...</td>\n",
              "      <td>Only in case of change in MSA/Side letter</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Charge_Type  ...                        Frequency of Update\n",
              "41                                BASE SERVICE CHARGE  ...  Only in case of change in MSA/Side letter\n",
              "42                                BASE SERVICE CHARGE  ...  Only in case of change in MSA/Side letter\n",
              "43                                BASE SERVICE CHARGE  ...  Only in case of change in MSA/Side letter\n",
              "44                                BASE SERVICE CHARGE  ...  Only in case of change in MSA/Side letter\n",
              "45  LEGACY NON-ENTERPRISE DAS SITE UPGRADE CHARGIN...  ...  Only in case of change in MSA/Side letter\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-j1sf25x31H"
      },
      "source": [
        "Generate xlsx Log with invalid or incorrect values founded in TowerDB File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZXadj6YnkKf"
      },
      "source": [
        "df_list = [ df2, df_picklist_errors, df_on_air_sites_errors, df_commercial_diffs]\n",
        "sheetnames = ['Dates Invalid Format', 'Picklist Invalid Value', 'On air sites with Blank Values', \"Commercial Different values\"]\n",
        "\n",
        "path = '/content/towerdb_errors.xlsx'\n",
        "general_log_erros(df_list, sheetnames, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8a5zTVJZdHb"
      },
      "source": [
        "TA Input File Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pl2j5hEBBxW"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQgaEwLAnhO3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "c7b4494f-47f2-4144-9548-c1bfb2d3c9cf"
      },
      "source": [
        "import pandas as pd\n",
        "#Read TA Input File\n",
        "path_ta_input = '/content/TA_Input_Germany_20210731.csv'\n",
        "\n",
        "ta_col = ['Code','Tenant Agreement','Tenant','Tenant Cluster','MNO Cluster','Tenant (ID)','Starting date','Expiring date',\\\n",
        "          ' Annual Fee per Tenant ','Overlapping days in FY20',' Current Annual Fee per Tenant ',\\\n",
        "          'Annual Energy Fee from 3rd Party Tenants',' OTP Amount ',' Renewal option ',' Residual duration ',\\\n",
        "          'Maturity Cluster','Contract Scope','Site Scope','Contract Migration Scope 14.05.2020','Comment']\n",
        "\n",
        "ta = pd.read_csv(path_ta_input, encoding='latin')\n",
        "\n",
        "ta = ta[ta_col]\n",
        "ta = ta.replace(['N/A', 'n/a',\"0\", '-', '_',' ', np.nan,'nan'], '')\n",
        "ta.head(2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Code</th>\n",
              "      <th>Tenant Agreement</th>\n",
              "      <th>Tenant</th>\n",
              "      <th>Tenant Cluster</th>\n",
              "      <th>MNO Cluster</th>\n",
              "      <th>Tenant (ID)</th>\n",
              "      <th>Starting date</th>\n",
              "      <th>Expiring date</th>\n",
              "      <th>Annual Fee per Tenant</th>\n",
              "      <th>Overlapping days in FY20</th>\n",
              "      <th>Current Annual Fee per Tenant</th>\n",
              "      <th>Annual Energy Fee from 3rd Party Tenants</th>\n",
              "      <th>OTP Amount</th>\n",
              "      <th>Renewal option</th>\n",
              "      <th>Residual duration</th>\n",
              "      <th>Maturity Cluster</th>\n",
              "      <th>Contract Scope</th>\n",
              "      <th>Site Scope</th>\n",
              "      <th>Contract Migration Scope 14.05.2020</th>\n",
              "      <th>Comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001 M</td>\n",
              "      <td>2400004762/2500003831</td>\n",
              "      <td>Tangens GmbH</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>386</td>\n",
              "      <td>14/10/2002</td>\n",
              "      <td>31/12/2050</td>\n",
              "      <td>0.000</td>\n",
              "      <td>365</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>No contractual end date</td>\n",
              "      <td>0.0</td>\n",
              "      <td>exclude</td>\n",
              "      <td>In scope</td>\n",
              "      <td>in scope</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0002 H</td>\n",
              "      <td>2400004299/2500006639</td>\n",
              "      <td>Telefónica Germany GmbH &amp; Co. OHG</td>\n",
              "      <td>TEF</td>\n",
              "      <td>MNO</td>\n",
              "      <td>400000577</td>\n",
              "      <td>01/01/2017</td>\n",
              "      <td>31/12/2050</td>\n",
              "      <td>1461.672</td>\n",
              "      <td>365</td>\n",
              "      <td>1461.672</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>No contractual end date</td>\n",
              "      <td>0.0</td>\n",
              "      <td>exclude</td>\n",
              "      <td>In scope</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In scope</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Code       Tenant Agreement  ... Contract Migration Scope 14.05.2020 Comment\n",
              "0  0001 M  2400004762/2500003831  ...                                            \n",
              "1  0002 H  2400004299/2500006639  ...                            In scope        \n",
              "\n",
              "[2 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAeUPluoBlVM"
      },
      "source": [
        "ta['Expiring date'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiWCoNFiC-dW"
      },
      "source": [
        "\n",
        "#Check date format(dd/mm/YYYY)\n",
        "cols_date = ['Code','Starting date', 'Expiring date']\n",
        "#df_dates_erros = check_date_columns(ta, cols_date, 'DE', 'TA')\n",
        "\n",
        "#Check Amount values\n",
        "cols = ['Code',' Annual Fee per Tenant ', ' Current Annual Fee per Tenant ']                \n",
        "df = check_amounts(ta, 'Code',cols,'.')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn8s2zm9Am6l"
      },
      "source": [
        "ta_bills = ['Code','Tenant','Starting date', 'Expiring date']\n",
        "path_ta_new = '/content/TA_Input_Germany_20210731.csv'\n",
        "path_ta_old = '/content/TA_Input_Germany_20210630.csv'\n",
        "ta_save = '/content/TA_DE'\n",
        "find_diffs_between_files(path_ta_old, path_ta_new, 'Code', ta_bills,'',ta_save,'csv','ta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHVNEIgbRrwD",
        "outputId": "ce6acd25-f3bc-4008-a5d3-604a42791e8b"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col] + df[kind_col]\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        \"\"\"if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\"\"\"\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "        \"\"\"if kind=='ta':\n",
        "            #new_sites['site_code'] = [i.endswith('MEO') or i.endswith('NOS')]\n",
        "            new_copy['sites_code'] = [re.sub('MEO$|NOS$', '', str(x)) for x in new_copy['sites']]\n",
        "            new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\"\"\"\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "ta_bills = ['Code','Tenant','Starting date', 'Expiring date']\n",
        "path_ta_new = '/content/TA_Input_Germany_20210731.csv'\n",
        "path_ta_old = '/content/TA_Input_Germany_20210630.csv'\n",
        "ta_save = '/content/TA_DE'\n",
        "old = '20210630.csv'\n",
        "new = '20210731.csv'\n",
        "\"\"\"(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0)\"\"\"\n",
        "find_diffs_between_files(path_ta_old, path_ta_new, 'Tenant Agreement', ta_bills, ta_save, old, new,type_file='csv', status_col='',\\\n",
        "                         kind='ta', kind_col='Tenant')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     []\n",
            "Dropped Rows: ['/2500006069Telefónica Germany GmbH & Co. OHG', '2400003988/2500006284Landratsamt Niederschlesischer', '/2500006914EWE NETZ GmbH', '/2400000811Deutsche Funkturm GmbH', '/2400000810Deutsche Funkturm GmbH', '/2400000812Telekom Deutschland']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX3W3VkdyK_u"
      },
      "source": [
        "LC Input File Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ4pbk9US_yM"
      },
      "source": [
        "#Read TA Input File\n",
        "lc_cols = ['Lease Contract ID','Code','Counterpart','Counterpart Cluster','Counterpart (ID)','Starting date','Expiring date',\\\n",
        "           ' Annual lease fees ','Overlapping days in FY20',' Current annual lease fees ',' OTP Amount ',\\\n",
        "           'Renewal option','(Average) residual duration','Maturity Cluster','Site Scope','Contract scope','MNO Cluster','reason for scope',\\\n",
        "           'Adjustment','+/-','Contract Migration Scope 14.05.2020']\n",
        "\n",
        "path_lc_input = '/content/LC_Input_Germany_20210731.csv'\n",
        "lc = pd.read_csv(path_lc_input, encoding='latin')\n",
        "\n",
        "lc = lc[lc_cols]\n",
        "lc = lc.replace(['N/A', 'n/a',\"0\", '-', '_',' ', np.nan,'nan'], '')\n",
        "lc.head(1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKAl12onDOyt"
      },
      "source": [
        "date_cols=['Code','Starting date', 'Expiring date']\n",
        "#(df, df_index, columns, format)\n",
        "df_dates_erros = check_date_columns(lc, 'Code', date_cols, 2)\n",
        "\n",
        "lc_cols = ['Code', ' Annual lease fees ']\n",
        "df = check_amounts(lc, 'Code',lc_cols, '.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_TiM16MDnN0"
      },
      "source": [
        "lc_bills = ['Code',' Annual lease fees ','Starting date', 'Expiring date']\n",
        "path_lc_new = '/content/LC_Input_Germany_20210731.csv'\n",
        "path_lc_old = '/content/LC_Input_Germany_20210630.csv'\n",
        "lc_save = '/content/LC_DE'\n",
        "find_diffs_between_files(path_lc_old, path_lc_new, 'Code', lc_bills,'',lc_save,'csv','lc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gabkd2DvE6xy",
        "outputId": "6eafea99-4431-4c3f-c810-15b8cc63828c"
      },
      "source": [
        "def find_diffs_between_files(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0):\n",
        "    \n",
        "    def lower_str(columns):\n",
        "        newlist = list(map(lambda x: x.lower(), columns))\n",
        "        return newlist\n",
        "\n",
        "    def fit_df(df, index_col, kind, kind_col):\n",
        "        kind_col = kind_col.lower()\n",
        "        if kind == 'ta':\n",
        "            df['sites'] = df[index_col] + df[kind_col]\n",
        "            #fit_cols = lower_str(list(df.columns))\n",
        "            #df.columns = fit_cols\n",
        "            df = df.dropna(subset=['sites'], axis=0)\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        else:\n",
        "            df = df.dropna(subset=[index_col], axis=0)\n",
        "            df[index_col] = df[index_col].astype(str)\n",
        "            df['sites'] = df[index_col]\n",
        "            # Aqui ajusta os nan e nat dos DFs, quando não forem arquivos não lidos\n",
        "            df = df.set_index('sites').fillna('')\n",
        "        return df\n",
        "        \n",
        "    def change_format(index, worksheet, format):\n",
        "        for cell in worksheet[f\"{str(index)}:{str(index)}\"]:\n",
        "            cell.font = format\n",
        "\n",
        "    def csv_header(path):\n",
        "        \"\"\"Função usada para ler os ficheiros que tem o simbolo do Euro\"\"\"\n",
        "        import csv\n",
        "        f = open(path, encoding='windows-1252', errors='ignore')\n",
        "        data = []\n",
        "        for row in csv.reader(f, delimiter=','):\n",
        "            data.append(row)\n",
        "        col = lower_str([*data[0]])\n",
        "        #data.pop(0)\n",
        "        #df = pd.DataFrame(data, columns=col)\n",
        "        return  col\n",
        "\n",
        "    def comparison(df_old, df_new,status):\n",
        "        # Perform Diff\n",
        "        new_copy = df_NEW.copy()\n",
        "        droppedRows = []\n",
        "        newRows = []\n",
        "\n",
        "        cols_OLD = list(df_OLD.columns)\n",
        "        cols_NEW = list(df_NEW.columns)\n",
        "        sharedCols = list(set(cols_OLD).intersection(cols_NEW))\n",
        "        #print(sharedCols)\n",
        "        for row in new_copy.index:\n",
        "            if (row in df_OLD.index) and (row in df_NEW.index):\n",
        "                for col in sharedCols:\n",
        "                    value_OLD = str(df_OLD.loc[row,col])\n",
        "                    value_NEW = str(df_NEW.loc[row,col])\n",
        "                #Error de a.empty() são sites duplcados e nã poodem estar no index\n",
        "                    if value_OLD == value_NEW:\n",
        "                        new_copy.loc[row,col] = np.nan\n",
        "                    else:\n",
        "                        new_copy.loc[row,col] = f'{value_OLD} > {value_NEW}'\n",
        "            else:\n",
        "                newRows.append(row)\n",
        "\n",
        "        new_copy = new_copy.dropna(axis=0, how='all')\n",
        "        new_copy = new_copy.dropna(axis=1, how='all')\n",
        "\n",
        "        for row in df_OLD.index:\n",
        "            if row not in df_NEW.index:\n",
        "                droppedRows.append(row)\n",
        "                new_copy = new_copy.append(df_OLD.loc[row,:])\n",
        "        \n",
        "        new_copy = new_copy.sort_index(key=lambda x: x.str.lower()).fillna('')\n",
        "        \n",
        "        new_copy = new_copy.reset_index()\n",
        "\n",
        "        \"\"\"if kind=='tw':\n",
        "            sites = [i for i in new_copy['sites']] \n",
        "            old = df_OLD[[status]].reset_index()\n",
        "            old = old.loc[old['sites'].isin(sites)]\n",
        "            new = df_NEW[[status]].reset_index()\n",
        "            new = new.loc[new['sites'].isin(sites)]\n",
        "            df_cross = pd.merge(new, old, on=['sites'], how='inner', suffixes=('_current', '_before'))\n",
        "            new_copy = pd.merge(new_copy, df_cross, on=['sites'], how='left')\n",
        "            status_1 = f'{status}_current'\n",
        "            status_2 = f'{status}_before'\n",
        "            new_copy = new_copy.set_index('sites')\n",
        "            new_copy = new_copy[[status_1, status_2]+ new_copy.columns[:-2].tolist()]\n",
        "            new_copy = new_copy.reset_index()\"\"\"\n",
        "\n",
        "        return newRows, droppedRows, new_copy\n",
        "        \"\"\"if kind=='ta':\n",
        "            #new_sites['site_code'] = [i.endswith('MEO') or i.endswith('NOS')]\n",
        "            new_copy['sites_code'] = [re.sub('MEO$|NOS$', '', str(x)) for x in new_copy['sites']]\n",
        "            new_copy = new_copy[['sites_code']+ new_copy.columns[:-1].tolist()]\"\"\"\n",
        "\n",
        "    bill_cols = lower_str(bill_cols)\n",
        "    index_col = index_col.lower()\n",
        "    type_file = type_file.lower()\n",
        "    status_col = status_col.lower()\n",
        "    if type_file == 'excel':\n",
        "        df_OLD = pd.read_excel(path_OLD,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns)) \n",
        "        df_OLD = df_OLD.iloc[:,skipc:]\n",
        "        df_OLD = fit_df(df_OLD,index_col,kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr).fillna('')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW,index_col,kind, kind_col)\n",
        "\n",
        "    elif type_file == 'csv':\n",
        "        #cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD, encoding='latin')\n",
        "        df_OLD.columns = lower_str(list(df_OLD.columns))\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        #cols_new = csv_header(path_NEW)\n",
        "        #Se for o arquivo gerado a partir do xlsx não prcisa de encoding\n",
        "        df_NEW = pd.read_csv(path_NEW, encoding='latin')\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns))\n",
        "        df_NEW = fit_df(df_NEW, index_col,kind, kind_col)\n",
        "\n",
        "    else:\n",
        "        cols_old = csv_header(path_OLD)\n",
        "        df_OLD = pd.read_csv(path_OLD,header=0, names=cols_old, engine='python',encoding='latin1').fillna('')\n",
        "        df_OLD = fit_df(df_OLD, index_col, kind, kind_col)\n",
        "\n",
        "        df_NEW = pd.read_excel(path_NEW,sheet_name = sheetname, skiprows = skipr)\n",
        "        df_NEW.columns = lower_str(list(df_NEW.columns)) \n",
        "        df_NEW = df_NEW.iloc[:,skipc:]\n",
        "        df_NEW = fit_df(df_NEW, index_col, kind, kind_col)\n",
        "\n",
        "    newRows, droppedRows, df_all = comparison(df_OLD, df_NEW, status_col)\n",
        "\n",
        "    print(f'\\nNew Rows:     {newRows}')\n",
        "    print(f'Dropped Rows: {droppedRows}')\n",
        "\n",
        "    # Save output and format\n",
        "    fname = f'{path_save} - ({old_name}) vs ({new_name}).xlsx'\n",
        "    file = pd.ExcelWriter(fname, engine='openpyxl')\n",
        "    \n",
        "    df_all.to_excel(file, sheet_name='diffs_founded', index=False)\n",
        "    df_NEW.to_excel(file, sheet_name='new_file', index=False)\n",
        "    df_OLD.to_excel(file, sheet_name='old_file', index=False)\n",
        "\n",
        "    # get openpyxl objects\n",
        "    wb  = file.book\n",
        "    ws = file.sheets['diffs_founded']\n",
        "    \n",
        "    red_fill = PatternFill(start_color='95A7B3', \\\n",
        "                                end_color='95A7B3', fill_type='solid')\n",
        "    red_header = PatternFill(start_color='00FF0000', \\\n",
        "                                end_color='00FF0000', fill_type='solid')\n",
        "    red_font = Font(color='00FF0000', italic=True)\n",
        "    new_fmt = Font(color='3F976D',bold=True, italic=True)\n",
        "    removed = Font(color='95A7B3',bold=True, italic=True)\n",
        "\n",
        "    dxf = DifferentialStyle(font=red_font, fill=red_fill)\n",
        "    highlight = Rule(type=\"containsText\", operator=\"containsText\", text=\">\", dxf=dxf)\n",
        "    highlight.formula = ['SEARCH(\">\", A1)']\n",
        "    ws.conditional_formatting.add('A1:ZZ10000', highlight)\n",
        "    \n",
        "    for column in bill_cols:\n",
        "        dx = DifferentialStyle(font=Font(color='FFFFFF', bold=True), fill=red_header)\n",
        "        header_style = Rule(type=\"containsText\", operator=\"containsText\", text=column, dxf=dx)\n",
        "        header_style.formula = [f'SEARCH(\"{column}\", A1)']\n",
        "        ws.conditional_formatting.add('A1:ZZ10000', header_style)\n",
        "    \n",
        "    #print(len(newRows))\n",
        "    for site in df_all['sites']:\n",
        "        if site in newRows:\n",
        "            idx = df_all.index[df_all['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,new_fmt)\n",
        "        if site in droppedRows:\n",
        "            idx = df_all.index[df_all['sites']==site].to_list()\n",
        "            idx = list(map(lambda x: x+2, idx))\n",
        "            change_format(*idx,ws,removed)\n",
        "\n",
        "    wb.save(fname)\n",
        "    print('\\nDone.\\n')\n",
        "\n",
        "lc_bills = ['Lease Contract ID',' Annual lease fees ','Starting date', 'Expiring date']\n",
        "path_lc_new = '/content/LC_Input_Germany_20210731.csv'\n",
        "path_lc_old = '/content/LC_Input_Germany_20210630.csv'\n",
        "lc_save = '/content/LC_DE'\n",
        "old = '20210630.csv'\n",
        "new = '20210731.csv'\n",
        "\"\"\"(path_OLD, path_NEW, index_col, bill_cols, path_save, old_name,new_name, type_file='mix', status_col='',\n",
        "                             kind='tw',kind_col='', sheetname='', skipr=0, skipc=0)\"\"\"\n",
        "find_diffs_between_files(path_lc_old, path_lc_new, 'Lease Contract ID', lc_bills, lc_save, old, new,type_file='csv', status_col='',\\\n",
        "                         kind='ta', kind_col='Counterpart')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New Rows:     ['1100013647/1100023345Marien-Krankenhaus', '1100004626/1100021970BAVARIA Hausverwaltung GmbH', '1500000342/1500000376Telefónica Germany GmbH & Co. OHG', '1100024422/GSW Immobilien AG', '1100011133/1100033881Hörmann Holding GmbH&Co.KG', '1100024327/Ludwigsburg Eduard Spranger', '1100024202/Wohnungsbau- u. Siedlungsges.', '1100024420/GSW Immobilien AG', '1100023533/1100006177Harz Energie GmbH&Co.KG', '1600000638/1600000014BVVG Bodenverwertungs-u.verwaltungs', '1100022907/1100041166e.discom Telekommunikations GmbH', '1500000337/1500000387Telefónica Germany GmbH & Co. OHG', '1100024417/NOVEC GmbH', '1100009895/1100025070R+V Lebensversichrung AG', '1100024398/Herbert Baltes', '1100009450/1100034666Löwenstein Invest Vier GmbH', '1100024305/Kaufhof Düsseldorf Carschhaus GmbH', '1100024059/Guenter Schneider', '1100024389/Magdeburger Getreide GmbH', '1600000823/DFMG Deutsche Funkturm GmbH', '1100024419/BauBeCon Wohnwert GmbH', '1100021320/Aenne Pitz', '1100002683/1100011790Ilona Hilberg', '1100024404/Ilona Hilberg', '1100024391/Magdeburger Getreide GmbH', '1100024367/BraWo Invest Magni Eins GmbH', '1100024359/Rhein Main Wohnen GmbH', '1100016739/1100035691Citcor Franconia Berlin VI S.a.r.l.', '1100003253/1100016578Vattenfall Europe Business Services GmbH', '1100013699/1100027143Monika und Konrad Saier', '1100001973/1100016515Land Baden-Württemberg', '1100003679/1100031895Catella Real Estate AG, Sondervermögen', '1100024426/CBC AcquiCo1 S.a.r.l', '1100024415/Georg Bentele', '1100024416/GKN Driveline Deutschland GmbH', '1100022347/1100020361Netze BW GmbH', '1500000385/Telefónica Germany GmbH & Co. OHG', '1100002618/1100020939Grundstücksgemeinschaft', '/1100042875Jürgen Habigsberg', '1100017809/1100038470Grund + Haus ImmobilienVertriebs- und', '1100024392/Zanker & Co Verw. Verm. KG', '1100006565/1100032066Daniel Hug', '1100014763/1100037681GEWOBAG Wohnungsbau-AG Berlin', '1100013155/1100015191Vattenfall Europe Business Services GmbH', '1100024373/Kath. Kirchengem.St.Johannes', '1100019156/1100030624Robert Brenzinger', '1100024322/Markt Dinkelscherben', '1100016619/1100002761GSW Immobilien AG', '1500000384/Telefónica Germany GmbH & Co. OHG', '1100021303/GSW Immobilien AG', '1100022013/1100042682GSW Immobilien AG', '1100024053/Heckmair Josef jun.', '1100024395/Landratsamt Ostalbkreis', '1100005798/1100031086Vattenfall Europe Business Services GmbH', '1100024411/Wohnungsbau-Genossenschafaft', '1100005758/1100031085Vattenfall Europe Business Services GmbH', '1100020195/1100041015Robert Pleis', '1100024348/Elisabeth Schmidt und Kathrin Schwan', '1100014655/1100012360Land Rheinland-Pfalz', '1100024170/Novec GmbH', '1100024377/GSW Immobilien AG', '1100011950/1100020323EDEKA Handelsges.Südbayern mbH', '1100024263/WEG Lichtenwalder Str. 8-14', '1100008770/1100023066GEBAG Duisb.Gemn.Bauges. AG', '1100006399/1100008858Admiral Immobilien GmbH', '1100024295/Stadt Grebenstein', '/1100040124Evamaria Hartwig', '1100018839/1100003343GSW Immobilien AG', '/1100003086Studentenwerk Potsdam', '1100024325/J + R Immobilien Konstanz GmbH', '1100013625/1100040238Thomas Jung', '1100024385/Evangelische Kirchenverwaltung', '1100014916/1100029340Franz-Xaver&Emma-Seiler Stiftg', '1100005817/1100002642GSW Immobilien AG', '1100024154/Evangelische Stiftungen Osnabrück', '1100024226/Grundstücksverw.Lescow GmbH', '1100000080/1100003129Bersaringplatz GmbH c/o DIM Deutsche', '1100021310/Sergiy Grenz und Jaroslav Logojda', '1100004828/1100039979RdB Commercial GmbH', '1100022438/1100021991Predigtstuhlbahn GmbH', '1100015217/1100010398Evangelische Brüder-Unität', '1100018545/1100040409Markt Grassau', '1500000382/Telekom Deutschland GmbH', '1100014471/1100009340Kerstin Schröder', '2100000523/Karl-Heinz Friedrich Pfäffle', '1100018652/1100038498Rath Sales GmbH & Co. KG', '1100018039/1100015372Land Baden-Württemberg', '1100021367/Abfallwirtsch.& Stadtreinigung GmbH', '1100016566/1100024431N191 GmbH', '1100001375/1100018658Suck & Möller GmbH f.Frank Scheffle', '1100024429/Deutsche Wohnen Asset Immobilien GmbH', '1500000345/1500000384Telefónica Germany GmbH & Co. OHG', '1500000383/Telefónica Germany GmbH & Co. OHG', '1100002822/1100036631MPU fix GmbH', '1100012868/1100002592Hansestadt Stralsund v.d.Zentrales', '1100024381/Landesbetrieb Straßenbau NRW', '1100007124/1100028895CIH 4502 GmbH & Co. KG', '1100022859/1100025983Telefónica Germany GmbH & Co. OHG', '1100001760/1100008591MVL Dresden GmbH', '1100020362/1100041412Vonovia SE', '1100006293/1100002144Mitteldeutsche Capital Management', '1100024368/Kath. Kirchengem. St. Georg', '1100012584/1100022352Christian und Karin Knott', '1100024430/Karo 4 S.a.r.l', '1100024164/Joachim Hagensieker', '1100024434/Sozialdienst kath. Maenner & Frauen', '/1100040374Gerd-Heinrich Speer', '1100024199/Gewobag PB', '1100023035/1100026055Telefónica Germany GmbH & Co. OHG', '1600000816/Elke Foline Korries', '1100024403/Verwaltungsges. Mayer GbR', '1100024421/Adient Real Estate Holding', '1100000695/1100018620Grienitz und Veith GbR', '1100024376/Boulevard Lu GmbH & Co. KG', '/1100041897Margot Fricke', '1100007802/1100013943Evangelishe Kirchengemeinde', '1600000822/Stadt Zittau', '1100021386/Nezir Aric', '1100003457/1100035958Peter Kuester', '1100024234/Josef Eismann', '1100024406/Kloster Seeon Kultur-u.Bildungsz.', '1100011725/1100029128Benntrans Vermietungs UG', '1100023859/ABS Hydraulik-Sevice GmbH', '1100004340/1100036677ETG Liegnitzer Strasse 15', '1100010575/1100025107Stefanie Fischenich', '1100017659/1100022595Amicitia Hebertshausen GmbH & Co. KG', '1100011024/1100022255Jörg Bertrams und Marie Simon GbR', '1100024211/Saint-Gobain Rigips GmbH', '1100014834/1100035610Diana Reck', '1100011340/1100027016Stadt Karlsruhe', '1100024379/Gemeinde Eurasburg', '1100012170/1100012237Raiffeisen Lagerhaus GmbH', '1100012812/1100017097Yasar Savucu', '1100024227/Ulla Wilhelmine Wambold', '1100008017/1100036160Woveg & Jegg GmbH', '1100024436/Josef Michels', '1100018660/1100039137Lämmler Wohnungsunternehmen GmbH', '/1100042219Stadt Rottenburg a. d. Laaber', '1100020347/1100041382ZBVV Zentral Boden Vermietung', '1100024414/Hans Georg Susmann', '1100004019/1100025719Martinrea Bergneustadt GmbH', '/1100033418Norbert Brünemann', '1100024352/Peter Westermair']\n",
            "Dropped Rows: ['1100022113/1100042735GAGFAH Group', '1100004626/1100021970WEG Bahnhofstr. 13. c/o IIMV', '1100017091/1100002188GSW AG', '1100011133/1100033881AIC-HÖRMANN GmbH & Co. KG', '1100010180/1100026948Consell GmbH', '1100016177/1100002522GSW Immobilien AG', '1100023533/1100006177Harz Energie Netz GmbH', '1600000638/1600000014BVVG Bodenverwertgs- und', '1100023774/1100037143NOVEC GmbH', '1100003094/1100022831Herbert Baltes', '1100009450/1100034666Manthor 13 S.á.r.l.', '1100019074/1100034994BauBeCon Wohnwert GmbH', '1100002683/1100011790Helmut Hilberg', '1100004050/1100012767Rhein Main Wohnen GmbH', '1100016739/1100035691Deutsche Annington Immobilien SE', '1100003253/1100016578Stromnetz Hamburg GmbH', '1100001973/1100016515Bundeskasse Halle', '1100003679/1100031895OC 6. Taufkirchen Immobilien GmbH', '1100014520/1100016159WHOK Studentenwohnheim GmbH', '1100007423/1100022080Georg Bentele', '1100009922/1100014038GKN Driveline Deutschland GmbH', '1100022347/1100020361NetCom BW GmbH', '1100017809/1100038470Pensionskasse für die Deutsche', '1100007343/1100022074Zanker & Co Verw. Verm. KG', '1100006565/1100032066Michael Hanspeter Bietenholz und', '1100014763/1100037681Galim 3 Grundstücks GmbH', '1100013155/1100015191Stromnetz Hamburg GmbH', '1100001356/1100005640Kath. Kirchengem.St.Johannes', '1100019156/1100030624Susanne und Tobias Merx', '1100016619/1100002761GSW AG', '1600000761/1600001012Phoenix Service GmbH & Co. KG', '1100021303/GSW AG', '1100022013/1100042682GSW AG', '1100000705/1100026430Landratsamt Ostalbkreis', '1100005798/1100031086Stromnetz Hamburg GmbH', '1100012243/1100008095Wohnungsbaugen. Loessnig e.G.', '1100005758/1100031085Stromnetz Hamburg GmbH', '1100022769/1100036561Ulrich Albert', '1100001879/1100001480GSW AG', '1100009270/1100006869Alfred Domma', '1100010073/1100010170Kassenverwaltung Bautzen', '1100006399/1100008858Wertbau Baugesellschaft mbH f.', '1100024052/GSW Immobilien AG', '1100012701/1100039575Konstanz Invest GmbH', '1100013625/1100040238Walter Jung', '1100002917/1100007560JO Vermietungs GmbH', '1500000199/1500000259DFMG Deutsche Funkturm GmbH', '1100005817/1100002642GSW AG', '1100014508/1100005353RWE Power Aktiengesellschaft', '2100000376/2100000115Inquam BMR S.àr.l', '1100000080/1100003129WBM Immobilien-Service GmbH', '1100004828/1100039979Ehmer & Pilz GbR vertr.d. Wohn- u.', '1100022438/1100021991Josef & Marga Posch GmbH & Co. KG', '1100015217/1100010398Ev.Brueder-Unitaet.Herrnhuter', '1100018545/1100040409Katek GmbH', '1100014471/1100009340Kurt Schröder', '1100018652/1100038498Rath GmbH', '1100018039/1100015372Bundeskasse Halle', '1500000289/1500000395DFMG Deutsche Funkturm GmbH', '1100016566/1100024431EB8 Aachen A.p.S.v.d.GPT Halverton GmbH', '1100010400/1100009108R & M Hausverwaltung GmbH', '1100002822/1100036631Indigo Park Deutschland GmbH', '1100019934/1100023553Landesbetrieb Straßenbau NRW', '1100021650/1100040987Peter Helmle', '1100007124/1100028895Nürnberger Lebensversicherung AG', '1100022859/1100025983E-Plus Mobilfunk GmbH', '1100001760/1100008591CVL Dresden GmbH', '1100020362/1100041412die Deutsche Annington Beteiligungs-', '1100006293/1100002144Nicola u. Douglas Smith c/o Heine', '1100024128/Stadt Dortmund', '1100020163/1100040967Kath. Kirchengem. St. Georg', '1100012584/1100022352Raiffeisenbank Geiselhoering', '1100000815/1100018627Akelius GmbH', '1100019978/1100023640Sozialdienst kath. Maenner & Frauen', '1100023035/1100026055E-Plus Mobilfunk GmbH', '1500000181/1500000231DFMG Deutsche Funkturm GmbH', '1100000695/1100018620Ingrid Grienitz', '1100007802/1100013943Kreiskirchenamt Kirchberg', '1100003457/1100035958AP Wireless Deutschland II GmbH', '1100011725/1100029128Karl Heinz Kloeser', '1100004340/1100036677Grundstücksgesellschaft Witten GbR', '1100017659/1100022595Schuster Grundbesitz GmbH', '1100011024/1100022255Dr.Frank Steinicke', '/1100040814Deutsche Bahn', '1100014834/1100035610H.J. und Brigitte Proehl', '1100011340/1100027016RaumFabrik Verm.Gesellsch.& Co KG Durlac', '1100005848/1100017845WEGW. Raudszus u. H.-J. Gebhardt', '1100014942/1100031535BMW AG', '1100023058/1100030259Vermögen und Bau Baden Württemberg', '1100012812/1100017097Thomas Freers', '1100008017/1100036160Eheleute Maier', '1100018660/1100039137Lämmer Wohnungsunternehmen GmbH', '1100020347/1100041382ZBI Fondsmanagement AG', '1100009717/1100005990Hans Georg Susmann', '1100004019/1100025719Metalsa Automotive GmbH', '/1100033418Stadt Nordhorn']\n",
            "\n",
            "Done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "kV9xAgQAM8zF",
        "outputId": "ca0dfe13-8c0d-408e-d89f-7f0d061bd5b3"
      },
      "source": [
        "d = pd.read_csv(path_lc_old, encoding='latin')\n",
        "d"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lease Contract ID</th>\n",
              "      <th>Code</th>\n",
              "      <th>Counterpart</th>\n",
              "      <th>Counterpart Cluster</th>\n",
              "      <th>Counterpart (ID)</th>\n",
              "      <th>Starting date</th>\n",
              "      <th>Expiring date</th>\n",
              "      <th>Annual lease fees</th>\n",
              "      <th>Overlapping days in FY20</th>\n",
              "      <th>Current annual lease fees</th>\n",
              "      <th>OTP Amount</th>\n",
              "      <th>Renewal option</th>\n",
              "      <th>(Average) residual duration</th>\n",
              "      <th>Maturity Cluster</th>\n",
              "      <th>Site Scope</th>\n",
              "      <th>Contract scope</th>\n",
              "      <th>MNO Cluster</th>\n",
              "      <th>reason for scope</th>\n",
              "      <th>Adjustment</th>\n",
              "      <th>+/-</th>\n",
              "      <th>Contract Migration Scope 14.05.2020</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1100002245/1100000642</td>\n",
              "      <td>0001 B</td>\n",
              "      <td>Gemeinn.Wohnungsbau AG Berlin</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>800001944.0</td>\n",
              "      <td>05/12/1990</td>\n",
              "      <td>31/12/2030</td>\n",
              "      <td>8980.782500</td>\n",
              "      <td>365</td>\n",
              "      <td>8980.782500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>On-going</td>\n",
              "      <td>9.666667</td>\n",
              "      <td>&lt;= 10 years</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In Scope</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>condition, or indicator for one-time payment o...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In scope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1100006016/1100024912</td>\n",
              "      <td>0001 D</td>\n",
              "      <td>Kilrush S.à.r.l. v.d. Tectareal Property</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>800020546.0</td>\n",
              "      <td>01/01/2007</td>\n",
              "      <td>31/12/2026</td>\n",
              "      <td>5510.000000</td>\n",
              "      <td>365</td>\n",
              "      <td>5510.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Active until termination date</td>\n",
              "      <td>5.666667</td>\n",
              "      <td>&lt;= 10 years</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In Scope</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1100006591/1100020007</td>\n",
              "      <td>0001 M</td>\n",
              "      <td>Inka Arnulfstraße GmbH &amp; Co. KG</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>800046411.0</td>\n",
              "      <td>01/10/1990</td>\n",
              "      <td>31/07/2028</td>\n",
              "      <td>85000.000000</td>\n",
              "      <td>365</td>\n",
              "      <td>85000.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>On-going</td>\n",
              "      <td>7.250000</td>\n",
              "      <td>&lt;= 10 years</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In Scope</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In scope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1600000237/1600001056</td>\n",
              "      <td>0001 O</td>\n",
              "      <td>Bundesanstalt für Immobilienaufgaben</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>800019339.0</td>\n",
              "      <td>28/06/2013</td>\n",
              "      <td>28/06/2043</td>\n",
              "      <td>329.752066</td>\n",
              "      <td>365</td>\n",
              "      <td>329.752066</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No contractual end date</td>\n",
              "      <td>22.083333</td>\n",
              "      <td>&lt;= 25 years</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In Scope</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1100003041/1100028651</td>\n",
              "      <td>0001 S</td>\n",
              "      <td>Ver.Freunde der Uni Stuttgart</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>800001917.0</td>\n",
              "      <td>23/10/1990</td>\n",
              "      <td>31/10/2022</td>\n",
              "      <td>600.000000</td>\n",
              "      <td>365</td>\n",
              "      <td>600.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Renewed</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>&lt;= 5 years</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In Scope</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22016</th>\n",
              "      <td>1100022312/1100022480</td>\n",
              "      <td>Y157 M</td>\n",
              "      <td>Bundesanstalt für Immobilienaufgaben</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>800004417.0</td>\n",
              "      <td>01/01/2006</td>\n",
              "      <td>01/01/2036</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No contractual end date</td>\n",
              "      <td>14.666667</td>\n",
              "      <td>&lt;= 15 years</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In Scope</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22017</th>\n",
              "      <td>1100022313/1100019807</td>\n",
              "      <td>Y158 M</td>\n",
              "      <td>WorldCell Inc.</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>800018907.0</td>\n",
              "      <td>01/12/2005</td>\n",
              "      <td>31/05/2022</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No contractual end date</td>\n",
              "      <td>1.083333</td>\n",
              "      <td>&lt;= 5 years</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In Scope</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In scope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22018</th>\n",
              "      <td>1100022314/1100022483</td>\n",
              "      <td>Y158 M</td>\n",
              "      <td>Bundesanstalt für Immobilienaufgaben</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>800004417.0</td>\n",
              "      <td>01/01/2006</td>\n",
              "      <td>01/01/2036</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No contractual end date</td>\n",
              "      <td>14.666667</td>\n",
              "      <td>&lt;= 15 years</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In Scope</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In scope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22019</th>\n",
              "      <td>1100022315/1100019811</td>\n",
              "      <td>Y159 M</td>\n",
              "      <td>WorldCell Inc.</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>800018907.0</td>\n",
              "      <td>01/12/2005</td>\n",
              "      <td>31/05/2022</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No contractual end date</td>\n",
              "      <td>1.083333</td>\n",
              "      <td>&lt;= 5 years</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In Scope</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22020</th>\n",
              "      <td>1100022316/1100022488</td>\n",
              "      <td>Y159 M</td>\n",
              "      <td>Bundesanstalt für Immobilienaufgaben</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>800004417.0</td>\n",
              "      <td>01/01/2006</td>\n",
              "      <td>01/01/2036</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No contractual end date</td>\n",
              "      <td>14.666667</td>\n",
              "      <td>&lt;= 15 years</td>\n",
              "      <td>in scope</td>\n",
              "      <td>In Scope</td>\n",
              "      <td>OTMO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In scope</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22021 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Lease Contract ID    Code  ...  +/- Contract Migration Scope 14.05.2020\n",
              "0      1100002245/1100000642  0001 B  ...  NaN                            In scope\n",
              "1      1100006016/1100024912  0001 D  ...  NaN                                 NaN\n",
              "2      1100006591/1100020007  0001 M  ...  NaN                            In scope\n",
              "3      1600000237/1600001056  0001 O  ...  NaN                                 NaN\n",
              "4      1100003041/1100028651  0001 S  ...  NaN                                 NaN\n",
              "...                      ...     ...  ...  ...                                 ...\n",
              "22016  1100022312/1100022480  Y157 M  ...  NaN                                 NaN\n",
              "22017  1100022313/1100019807  Y158 M  ...  NaN                            In scope\n",
              "22018  1100022314/1100022483  Y158 M  ...  NaN                            In scope\n",
              "22019  1100022315/1100019811  Y159 M  ...  NaN                                 NaN\n",
              "22020  1100022316/1100022488  Y159 M  ...  NaN                            In scope\n",
              "\n",
              "[22021 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P9lolIfMGJ3",
        "outputId": "02fd0c0a-2758-41a3-f426-4f8ca97b4921"
      },
      "source": [
        "lc[['Counterpart']].info()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 22055 entries, 0 to 22054\n",
            "Data columns (total 1 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Counterpart  22055 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 172.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}